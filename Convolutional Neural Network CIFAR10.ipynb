{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Neural Networks Assignment.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "QiuuPVphWTnX",
        "tgnnDx1uffKa",
        "nOmpspUIZfsQ",
        "ZHGiA812Yskw",
        "1KRfLzwC_KY-",
        "nKFH5cs1tXma",
        "T_Zyq8YVtZ4i",
        "34vAr34A_Sh9",
        "ceNYv23_vAym",
        "dbJ_oHEnvYfj",
        "qhlFzjU5_Whk",
        "8CUSG_zyvlxE",
        "dfUcXbYavpby",
        "iUKvtk6lYzHL",
        "_f5pVoSg6sgW",
        "RYbtbA0G6u6H",
        "S978HaJf6xKO",
        "W08AK6Qx6FSA",
        "jQig4pO66Xby",
        "L7qGOt1A6RBW",
        "-EVb-2bj7NBs",
        "eddrp84z7ojv",
        "k-sZD7xB7s5O",
        "rmX3n2ukYzW1",
        "hF9OyP7PYVu1",
        "Q7Vj56e3ZJu9",
        "tNW8w6rlZNtV",
        "OSyAOHjeZexM",
        "J1RJ93xNZs5N",
        "eH-YkkCJZzCi",
        "n8UTt4CeZ7Ut",
        "EiuXhVYrZ9Xf",
        "xcHCA4VWnzth",
        "M9YQFRM_n5v3",
        "8L7ijjBen-EC",
        "E9aDAnooY-An",
        "VQ4pPns7rC4H",
        "rSrVVG4XrMUo",
        "3CEfIV-GrPzu",
        "Jrma4IZ4rTKm",
        "A_HlrPfq0y8l",
        "KsPu9xLc09oC",
        "vx-7cVTv0_lQ",
        "ssDSn2Aq1Qji",
        "UDcbHWdA1Wy5",
        "C5VaZ2PDo1aN",
        "VPhowvFnpDLF",
        "U3N9a3Chpjb3",
        "oia341F6taf2",
        "_6_Jc0TzuuYZ",
        "rexTXJlJp_DY",
        "XOKO0j_hqETp",
        "9n3QVvmXpxkv",
        "kHYG2yhctsCP",
        "NUU27JJ3tvX9",
        "3LRr6ljop4Zv",
        "EOaXR3G0tyU3",
        "x55QI_fAt4Mg",
        "n5-2HmfA6KVP",
        "9cnCfMkv6deH",
        "7kdDARCr6hTy",
        "_AZBvlQM6jHI",
        "Ixjwc_sY6k7D",
        "zm0BZ2y66nl4",
        "L2f-PVtW6qzb",
        "lhWE9BFj6seB",
        "zGJe-XzJ6ulj",
        "39A-JNC7FdNx",
        "_Je8Y0OR6N9R",
        "xmPASDyeZgom",
        "RxuMMAG6YYXp",
        "runblX19Ycxu",
        "jY1UWE2wYgWL",
        "RYLVBj_8YjSG"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Import packages & Setting up tensorboard"
      ],
      "metadata": {
        "id": "BhiwI5YssWpH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "RPlzIw_nsSzP"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "from keras.datasets import cifar10  #Dataset\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.utils import to_categorical #One hot encoding\n",
        "\n",
        "from sklearn.model_selection import KFold #Cross validation\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "import datetime\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#To visualise tensorboard, please type in    \" tensorboard --logdir=logs/fit --host localhost --port 8088 \"   into the command line, and then visit localhost:8088 on your web browser\n",
        "\n",
        "os.environ['TENSORBOARD_BINARY'] = 'C:/Users/benat/anaconda3/Lib/site-packages/tensorboard' #Change path accordingly\n",
        "%load_ext tensorboard"
      ],
      "metadata": {
        "id": "_qmdsXrTfV8Y"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load & Visualise data"
      ],
      "metadata": {
        "id": "eFrLHxy5sebF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load dataset\n",
        "(trainX, trainy), (testX, testy) = cifar10.load_data()\n",
        "# summarize loaded dataset\n",
        "print('Train: X=%s, y=%s' % (trainX.shape, trainy.shape))\n",
        "print('Test: X=%s, y=%s' % (testX.shape, testy.shape))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jmre_lnosldl",
        "outputId": "66f93706-287a-4f06-8023-902817c2caa0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: X=(50000, 32, 32, 3), y=(50000, 1)\n",
            "Test: X=(10000, 32, 32, 3), y=(10000, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Label images with appropaite classes\n",
        "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
        "               'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "\n",
        "plt.figure(figsize=(10,10))\n",
        "for i in range(25):\n",
        "    plt.subplot(5,5,i+1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.grid(False)\n",
        "    plt.imshow(trainX[i])\n",
        "    plt.xlabel(class_names[trainy[i][0]])\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 589
        },
        "id": "zu5uJbnVwaeq",
        "outputId": "5ddd8ba2-d790-496f-ea4f-d444ee6166c9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x720 with 25 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pre processing the data"
      ],
      "metadata": {
        "id": "BRFm_7xDyBMa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Normalising pixel values"
      ],
      "metadata": {
        "id": "JYX0ntLFyEis"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Normalise pixel values to have a value between 0-1\n",
        "trainX, testX = trainX.astype('float32'), testX.astype('float32')\n",
        "trainX, testX = trainX/255.0, testX/255.0\n",
        "\n",
        "#One hot encoding for labels\n",
        "trainy = to_categorical(trainy, num_classes=10)\n",
        "testy = to_categorical(testy, num_classes=10)"
      ],
      "metadata": {
        "id": "tXssjVd3yK3S"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating cutout"
      ],
      "metadata": {
        "id": "QiuuPVphWTnX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Add a black square of size 12x12 in a random location on the image\n",
        "def apply_mask(image, size=12, n_squares=1):\n",
        "    h, w, channels = image.shape\n",
        "    new_image = image\n",
        "    for _ in range(n_squares):\n",
        "        y = np.random.randint(h)\n",
        "        x = np.random.randint(w)\n",
        "        y1 = np.clip(y - size // 2, 0, h)\n",
        "        y2 = np.clip(y + size // 2, 0, h)\n",
        "        x1 = np.clip(x - size // 2, 0, w)\n",
        "        x2 = np.clip(x + size // 2, 0, w)\n",
        "        new_image[y1:y2,x1:x2,:] = 0\n",
        "    return new_image"
      ],
      "metadata": {
        "id": "K1oTlnlOXMkw"
      },
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Visualise the image with cutout\n",
        "print(\"Original images:\")\n",
        "for i in range(2):\n",
        "    plt.subplot(330 + 1 + i)\n",
        "    plt.imshow(trainX[i])\n",
        "plt.show()\n",
        "print(\"Images with cutout:\")\n",
        "for i in range(2):\n",
        "    plt.subplot(330 + 1 + i)\n",
        "    img = np.copy(trainX[i])\n",
        "    plt.imshow(apply_mask(img))\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "id": "sYnan4tj4viW",
        "outputId": "eda0d7a7-4a6a-4e75-b5fe-0debf58cc0ec"
      },
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original images:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Images with cutout:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Batch generator for images with cutout\n",
        "def batch_generator(x, y, epochs, m, batch_size, augment=None):\n",
        "    for _ in range(epochs):\n",
        "        n = x.shape[0]\n",
        "        reorder = np.random.permutation(n)\n",
        "        cursor = 0\n",
        "        while cursor + batch_size < x.shape[0]:\n",
        "            x_batch = x[reorder[cursor:cursor+batch_size]]\n",
        "            y_batch = y[reorder[cursor:cursor+batch_size]]\n",
        "            if augment != None:\n",
        "                yield np.array([augment(xx) for xx in x_batch for rep in range(m)]), np.array([yy for yy in y_batch for rep in range(m)])\n",
        "            else:\n",
        "                yield x_batch, y_batch\n",
        "            cursor += batch_size"
      ],
      "metadata": {
        "id": "82nSWQOqWTA_"
      },
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CutMix"
      ],
      "metadata": {
        "id": "tgnnDx1uffKa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#For some reason, we cannot visualise the reconstructed image with CutMix, if we perform normalisation\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# load dataset\n",
        "(trainX, trainy), (testX, testy) = cifar10.load_data()\n",
        "# summarize loaded dataset\n",
        "print('Train: X=%s, y=%s' % (trainX.shape, trainy.shape))\n",
        "print('Test: X=%s, y=%s' % (testX.shape, testy.shape))\n",
        "\n",
        "#One hot encoding for labels\n",
        "trainy = to_categorical(trainy, num_classes=10)\n",
        "testy = to_categorical(testy, num_classes=10)\n",
        "\n",
        "AUTO = tf.data.AUTOTUNE\n",
        "BATCH_SIZE = 32\n",
        "IMG_SIZE = 32\n",
        "\n",
        "def preprocess_image(image, label): #Convert image to tensorflow object\n",
        "    image = tf.image.resize(image, (IMG_SIZE, IMG_SIZE))\n",
        "    image = tf.image.convert_image_dtype(image, tf.float32) / 255.0\n",
        "    return image, label"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s9sZBogAhT4H",
        "outputId": "7afddebc-cb19-486e-af07-ee93de1d63a5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: X=(50000, 32, 32, 3), y=(50000, 1)\n",
            "Test: X=(10000, 32, 32, 3), y=(10000, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#We get an error if we run this on GPU\n",
        "with tf.device('/device:CPU:0'):\n",
        "  train_ds_one = (\n",
        "    tf.data.Dataset.from_tensor_slices((trainX, trainy))\n",
        "    .shuffle(1024)\n",
        "    .map(preprocess_image, num_parallel_calls=AUTO)\n",
        "  )\n",
        "  train_ds_two = (\n",
        "    tf.data.Dataset.from_tensor_slices((trainX, trainy))\n",
        "    .shuffle(1024)\n",
        "    .map(preprocess_image, num_parallel_calls=AUTO)\n",
        "  )\n",
        "\n",
        "\n",
        "  test_ds = tf.data.Dataset.from_tensor_slices((testX, testy))\n",
        "\n",
        "\n",
        "# Combine two shuffled datasets from the same training data.\n",
        "  train_ds = tf.data.Dataset.zip((train_ds_one, train_ds_two))\n",
        "\n",
        "  test_ds = (\n",
        "    test_ds.map(preprocess_image, num_parallel_calls=AUTO)\n",
        "    .batch(BATCH_SIZE)\n",
        "    .prefetch(AUTO)\n",
        "  )"
      ],
      "metadata": {
        "id": "knuziLKPfe1v"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Get a particular size of the cutout image to apply to the original image\n",
        "def sample_beta_distribution(size, concentration_0=0.2, concentration_1=0.2):\n",
        "    gamma_1_sample = tf.random.gamma(shape=[size], alpha=concentration_1)\n",
        "    gamma_2_sample = tf.random.gamma(shape=[size], alpha=concentration_0)\n",
        "    return gamma_1_sample / (gamma_1_sample + gamma_2_sample)\n",
        "\n",
        "#Cut a fragment of the image to apply to the orignal image\n",
        "@tf.function\n",
        "def get_box(lambda_value):\n",
        "    cut_rat = tf.math.sqrt(1.0 - lambda_value)\n",
        "\n",
        "    cut_w = IMG_SIZE * cut_rat  \n",
        "    cut_w = tf.cast(cut_w, tf.int32)\n",
        "\n",
        "    cut_h = IMG_SIZE * cut_rat  \n",
        "    cut_h = tf.cast(cut_h, tf.int32)\n",
        "\n",
        "    cut_x = tf.random.uniform((1,), minval=0, maxval=IMG_SIZE, dtype=tf.int32)  \n",
        "    cut_y = tf.random.uniform((1,), minval=0, maxval=IMG_SIZE, dtype=tf.int32)  \n",
        "\n",
        "    boundaryx1 = tf.clip_by_value(cut_x[0] - cut_w // 2, 0, IMG_SIZE)\n",
        "    boundaryy1 = tf.clip_by_value(cut_y[0] - cut_h // 2, 0, IMG_SIZE)\n",
        "    bbx2 = tf.clip_by_value(cut_x[0] + cut_w // 2, 0, IMG_SIZE)\n",
        "    bby2 = tf.clip_by_value(cut_y[0] + cut_h // 2, 0, IMG_SIZE)\n",
        "\n",
        "    target_h = bby2 - boundaryy1\n",
        "    if target_h == 0:\n",
        "        target_h += 1\n",
        "\n",
        "    target_w = bbx2 - boundaryx1\n",
        "    if target_w == 0:\n",
        "        target_w += 1\n",
        "\n",
        "    return boundaryx1, boundaryy1, target_h, target_w\n",
        "\n",
        "\n",
        "@tf.function\n",
        "def cutmix(train_ds_one, train_ds_two):\n",
        "    (image1, label1), (image2, label2) = train_ds_one, train_ds_two\n",
        "\n",
        "    alpha = [0.25]\n",
        "    beta = [0.25]\n",
        "\n",
        "    # Get a sample from the Beta distribution\n",
        "    lambda_value = sample_beta_distribution(1, alpha, beta)\n",
        "\n",
        "    # Define Lambda\n",
        "    lambda_value = lambda_value[0][0]\n",
        "\n",
        "    # Get the bounding box offsets, heights and widths\n",
        "    boundaryx1, boundaryy1, target_h, target_w = get_box(lambda_value)\n",
        "\n",
        "    # Get a patch from the second image (`image2`)\n",
        "    crop2 = tf.image.crop_to_bounding_box(\n",
        "        image2, boundaryy1, boundaryx1, target_h, target_w\n",
        "    )\n",
        "    # Pad the `image2` patch (`crop2`) with the same offset\n",
        "    image2 = tf.image.pad_to_bounding_box(\n",
        "        crop2, boundaryy1, boundaryx1, IMG_SIZE, IMG_SIZE\n",
        "    )\n",
        "    # Get a patch from the first image (`image1`)\n",
        "    crop1 = tf.image.crop_to_bounding_box(\n",
        "        image1, boundaryy1, boundaryx1, target_h, target_w\n",
        "    )\n",
        "    # Pad the `image1` patch (`crop1`) with the same offset\n",
        "    img1 = tf.image.pad_to_bounding_box(\n",
        "        crop1, boundaryy1, boundaryx1, IMG_SIZE, IMG_SIZE\n",
        "    )\n",
        "\n",
        "    # Modify the first image by subtracting the patch from `image1`\n",
        "    # (before applying the `image2` patch)\n",
        "    image1 = image1 - img1\n",
        "    # Add the modified `image1` and `image2`  together to get the CutMix image\n",
        "    image = image1 + image2\n",
        "\n",
        "    # Adjust Lambda in accordance to the pixel ration\n",
        "    lambda_value = 1 - (target_w * target_h) / (IMG_SIZE * IMG_SIZE)\n",
        "    lambda_value = tf.cast(lambda_value, tf.float32)\n",
        "\n",
        "    # Combine the labels of both images\n",
        "    label = lambda_value * label1 + (1 - lambda_value) * label2\n",
        "    return image, label\n"
      ],
      "metadata": {
        "id": "twa1qHLxfrCv"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the new dataset using our `cutmix` utility\n",
        "train_ds_cmu = (\n",
        "    train_ds.shuffle(1024)\n",
        "    .map(cutmix, num_parallel_calls=AUTO)\n",
        "    .batch(BATCH_SIZE)\n",
        "    .prefetch(AUTO)\n",
        ")\n",
        "\n",
        "# Let's preview 9 samples from the dataset\n",
        "image_batch, label_batch = next(iter(train_ds_cmu))\n",
        "plt.figure(figsize=(10, 10))\n",
        "for i in range(9):\n",
        "    ax = plt.subplot(3, 3, i + 1)\n",
        "    plt.title(class_names[np.argmax(label_batch[i])])\n",
        "    plt.imshow(image_batch[i])\n",
        "    plt.axis(\"off\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 591
        },
        "id": "Ay5prPuDf5Wg",
        "outputId": "c3ba7559-8fea-4d5c-858d-9ff5d043348e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x720 with 9 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### K-fold"
      ],
      "metadata": {
        "id": "ZJbGwRhK9dYS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define per-fold score containers\n",
        "acc_per_fold = []\n",
        "loss_per_fold = []\n",
        "\n",
        "\n",
        "# Define the K-fold Cross Validator\n",
        "kfold = KFold(n_splits=5, shuffle=True)"
      ],
      "metadata": {
        "id": "5-Q_Sozy9hWp"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Defining the model"
      ],
      "metadata": {
        "id": "701TEBB007hE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN_Model(tf.keras.Model):\n",
        "  def __init__(self, batch_size, depth, hidden_neurons_1, hidden_neurons_2, hidden_neurons_3, kernel_size_1, kernel_size_2, kernel_size_3,\n",
        "               dropout_rate, batch_norm, pooling, layer_activation_1, layer_activation_2, layer_activation_3,\n",
        "               kernel_initialiser, bias_initialiser, optimiser, weight_decay, learning_rate, output_activation, SGD_momentum, use_cutout=False):\n",
        "    super(CNN_Model,self).__init__()\n",
        "    \n",
        "    ############ Batch Size ###############\n",
        "    self.batch_size = batch_size        \n",
        "    #######################################\n",
        "\n",
        "\n",
        "    ############ Conv2D hyperparameters ###############\n",
        "    self.hidden_neurons_1=hidden_neurons_1\n",
        "    self.kernel_size_1=kernel_size_1\n",
        "\n",
        "    self.hidden_neurons_2=hidden_neurons_2\n",
        "    self.kernel_size_2=kernel_size_2\n",
        "\n",
        "    self.hidden_neurons_3=hidden_neurons_3\n",
        "    self.kernel_size_3=kernel_size_3\n",
        "    ####################################################\n",
        "\n",
        "\n",
        "    ############ Other layers in the model #####################\n",
        "    if pooling=='max':\n",
        "      self.pooling = tf.keras.layers.MaxPooling2D(pool_size=(2, 2), padding='same')\n",
        "    elif pooling=='average':\n",
        "      self.pooling = tf.keras.layers.AveragePooling2D(pool_size=(2,2), padding='same')\n",
        "    ############################################################\n",
        "    self.batch_norm=batch_norm\n",
        "    ############################################################\n",
        "    self.dropout_rate=dropout_rate\n",
        "    ############################################################\n",
        "\n",
        "\n",
        "    ############ Activation functions #########################\n",
        "    self.layer_activation_1 = layer_activation_1\n",
        "    self.layer_activation_2 = layer_activation_2\n",
        "    self.layer_activation_3 = layer_activation_3\n",
        "\n",
        "    self.output_activation = output_activation\n",
        "    ############################################################\n",
        "\n",
        "\n",
        "    ####################### Learning rate #################################\n",
        "    self.learning_rate = learning_rate\n",
        "    ###############################################################################\n",
        "    \n",
        "\n",
        "    ################################### Optimiser #################################\n",
        "    self.momentum = SGD_momentum\n",
        "    if optimiser=='Adam':\n",
        "      self.optimizer=tf.keras.optimizers.Adam(learning_rate=self.learning_rate)\n",
        "    elif optimiser=='SGD':\n",
        "      self.optimizer=tf.keras.optimizers.SGD(learning_rate=self.learning_rate, momentum=self.momentum)\n",
        "    elif optimiser=='RMSprop':\n",
        "      self.optimizer=tf.keras.optimizers.RMSprop(learning_rate=self.learning_rate)\n",
        "    ###############################################################################\n",
        "\n",
        "\n",
        "    ####################### Weight initialisation #################################\n",
        "    self.kernel_initialiser=kernel_initialiser\n",
        "    self.bias_initialiser=bias_initialiser\n",
        "    ###############################################################################\n",
        "\n",
        "\n",
        "    ####################### Weight decay #################################\n",
        "    self.weight_decay = weight_decay\n",
        "    ###############################################################################\n",
        "\n",
        "    ####################### Cutout #################################\n",
        "    self.cutout = use_cutout\n",
        "    ###############################################################################\n",
        "\n",
        "\n",
        "################################################################################################# The Model ####################################################################################################################\n",
        "#################################################################################################################################################################################################################################\n",
        "\n",
        "\n",
        "\n",
        "    self.model = tf.keras.Sequential()\n",
        "\n",
        "    #Input layer\n",
        "    self.model.add(tf.keras.layers.InputLayer(input_shape=(32, 32, 3)))\n",
        "\n",
        "    #1st hidden layer\n",
        "    self.model.add(tf.keras.layers.Conv2D(filters=self.hidden_neurons_1, kernel_size=self.kernel_size_1, activation=self.layer_activation_1,\n",
        "                                          kernel_regularizer=self.weight_decay, kernel_initializer=self.kernel_initialiser, bias_initializer=self.bias_initialiser, padding='same'))\n",
        "    if self.batch_norm:\n",
        "      self.model.add(tf.keras.layers.BatchNormalization())\n",
        "    if pooling!='None':\n",
        "      self.model.add(self.pooling)\n",
        "\n",
        "    #2nd hidden layer\n",
        "    if depth >=2:\n",
        "      self.model.add(tf.keras.layers.Conv2D(filters=self.hidden_neurons_2, kernel_size=self.kernel_size_2, activation=self.layer_activation_2,\n",
        "                                            kernel_regularizer=self.weight_decay, kernel_initializer=self.kernel_initialiser, bias_initializer=self.bias_initialiser, padding='same'))\n",
        "      if self.batch_norm:\n",
        "        self.model.add(tf.keras.layers.BatchNormalization())\n",
        "      if pooling!='None':\n",
        "        self.model.add(self.pooling)\n",
        "      self.model.add(tf.keras.layers.Dropout(self.dropout_rate))\n",
        "\n",
        "    #3rd hidden layer\n",
        "    if depth>=3:\n",
        "      self.model.add(tf.keras.layers.Conv2D(filters=self.hidden_neurons_3, kernel_size=self.kernel_size_3, activation=self.layer_activation_3,\n",
        "                                            kernel_regularizer=self.weight_decay, kernel_initializer=self.kernel_initialiser, bias_initializer=self.bias_initialiser, padding='same'))\n",
        "      if self.batch_norm:\n",
        "        self.model.add(tf.keras.layers.BatchNormalization())\n",
        "      if pooling!='None':\n",
        "        self.model.add(self.pooling)\n",
        "\n",
        "    #4th hidden layer (ONLY FOR COMPARISON!)\n",
        "    if depth>=4:\n",
        "      self.model.add(tf.keras.layers.Conv2D(filters=256, kernel_size=3, activation='relu',\n",
        "                                            kernel_regularizer=self.weight_decay, kernel_initializer=self.kernel_initialiser, bias_initializer=self.bias_initialiser))\n",
        "      if pooling!='None':\n",
        "        self.model.add(self.pooling)\n",
        "\n",
        "    #Output layer\n",
        "    self.model.add(tf.keras.layers.Flatten())\n",
        "    self.model.add(tf.keras.layers.Dense(units=10, activation=self.output_activation))\n",
        "\n",
        "    self.model.summary()\n",
        "\n",
        "\n",
        "################################################################################################# Methods #######################################################################################################################\n",
        "#################################################################################################################################################################################################################################\n",
        "\n",
        "  #Exponential learning scheduler\n",
        "  def lr_schedulerExp(self, epoch, lr):\n",
        "    if epoch < 15:\n",
        "      return lr \n",
        "    else:\n",
        "      return lr * tf.math.exp(-0.1)\n",
        "\n",
        "  #Step learning scheduler\n",
        "  def lr_schedulerStep(self, epoch, lr):\n",
        "    drop = 0.5\n",
        "    epochs_drop = 10.0\n",
        "    if epoch==10 or epoch==20:\n",
        "      lr = lr * math.pow(0.5,  \n",
        "           math.floor((1+epoch)/10.0))\n",
        "      return lr\n",
        "    else:\n",
        "      return lr\n",
        "    \n",
        "  #Train on data augmenatation or cutout\n",
        "  def train(self, train_gen, train_x, train_y, test_x, test_y, lr_rate=True, lr_rate_type='exponential'):\n",
        "    if lr_rate:\n",
        "      if lr_rate_type=='exponential':\n",
        "        callback = tf.keras.callbacks.LearningRateScheduler(self.lr_schedulerExp)\n",
        "      else:\n",
        "        callback = tf.keras.callbacks.LearningRateScheduler(self.lr_schedulerStep)\n",
        "\n",
        "    log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "\n",
        "    self.model.compile(optimizer=self.optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    steps = int(train_x.shape[0]/self.batch_size) \n",
        "    if lr_rate:\n",
        "      if self.cutout:\n",
        "        return self.model.fit(train_gen, epochs=30, validation_data=(test_x, test_y), steps_per_epoch=steps, callbacks=[tensorboard_callback,callback], verbose=1)\n",
        "      else:\n",
        "        return self.model.fit(train_gen.flow(train_x, train_y, batch_size=self.batch_size), validation_data=(test_x, test_y) ,steps_per_epoch=steps, epochs=30, callbacks=[tensorboard_callback,callback], verbose=1)  #  self.model.fit(train_x, train_y, validation_data=(test_x, test_y) , epochs=25, verbose=1)\n",
        "    else:\n",
        "      return self.model.fit(train_gen.flow(train_x, train_y, batch_size=self.batch_size), validation_data=(test_x, test_y) ,steps_per_epoch=steps, epochs=30, callbacks=[tensorboard_callback], verbose=1)\n",
        "    \n",
        "  #Train on CutMix\n",
        "  def train_mix(self,train_ds, val_ds, lr_rate=True, lr_rate_type='exponential'):\n",
        "    if lr_rate:\n",
        "      if lr_rate_type=='exponential':\n",
        "        callback = tf.keras.callbacks.LearningRateScheduler(self.lr_schedulerExp)\n",
        "      else:\n",
        "        callback = tf.keras.callbacks.LearningRateScheduler(self.lr_schedulerStep)\n",
        "\n",
        "    log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "\n",
        "    self.model.compile(optimizer=self.optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    if lr_rate:\n",
        "      return self.model.fit(train_ds, validation_data=val_ds, epochs=30, callbacks=[tensorboard_callback, callback], verbose=1)\n",
        "    else:\n",
        "      return self.model.fit(train_ds, validation_data=val_ds, epochs=30, callbacks=[tensorboard_callback], verbose=1)\n",
        "     \n",
        "  def evaluate(self, test_x, test_y):\n",
        "    return self.model.evaluate(test_x, test_y, verbose=0)\n",
        "\n",
        "  def evaluate_mix(self, test_ds):\n",
        "    return self.model.evaluate(test_ds)"
      ],
      "metadata": {
        "id": "1-stUvKv0_48"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training the model"
      ],
      "metadata": {
        "id": "iNkKHiJO098s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting GPU"
      ],
      "metadata": {
        "id": "BMcwanPMuVre"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "metadata": {
        "id": "4V_QKxC2uYGF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09e9ed09-f140-4cfa-a7df-69909217f451"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CNN model"
      ],
      "metadata": {
        "id": "bYGRIJgV_itV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Learning rate"
      ],
      "metadata": {
        "id": "ZHGiA812Yskw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Learning_rate=0.1 & Exponential decay"
      ],
      "metadata": {
        "id": "1KRfLzwC_KY-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# K-fold Cross Validation model evaluation\n",
        "acc_per_fold = []\n",
        "loss_per_fold = []\n",
        "with tf.device('/device:GPU:0'):\n",
        "  fold_no = 1\n",
        "  for train, valid in kfold.split(trainX, trainy):\n",
        "\n",
        "    model = CNN_Model(batch_size=32, depth=3, hidden_neurons_1=32, hidden_neurons_2=64, hidden_neurons_3=128, kernel_size_1=3, kernel_size_2=3, kernel_size_3=3, \n",
        "              dropout_rate=0.2, batch_norm=False, pooling='max', layer_activation_1='relu', layer_activation_2='relu', layer_activation_3='relu',\n",
        "               kernel_initialiser='glorot_uniform', bias_initialiser='zeros', optimiser='Adam', weight_decay=None, learning_rate=0.1, output_activation='softmax', SGD_momentum=0)\n",
        "  \n",
        "    #Data augmentation\n",
        "    train_gen = tf.keras.preprocessing.image.ImageDataGenerator(width_shift_range=0.1,\n",
        "      height_shift_range=0.1,\n",
        "      horizontal_flip=True)\n",
        "    train_gen.fit(trainX[train])\n",
        "  \n",
        "    print('------------------------------------------------------------------------')\n",
        "    print(f'Training for fold {fold_no} ...')\n",
        "\n",
        "    history = model.train(\n",
        "              train_gen,\n",
        "                trainX[train],\n",
        "                  trainy[train],\n",
        "                trainX[valid],\n",
        "                  trainy[valid])\n",
        "\n",
        "    scores = model.evaluate(trainX[valid], trainy[valid])\n",
        "\n",
        "    print(f'Score for fold {fold_no}: loss of {scores[0]}; accuracy of {scores[1]*100}%')\n",
        "    acc_per_fold.append(scores[1] * 100)\n",
        "    loss_per_fold.append(scores[0])\n",
        "\n",
        "    fold_no+=1\n",
        "\n",
        "# == Provide average scores ==\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Score per fold')\n",
        "for i in range(0, len(acc_per_fold)):\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Average scores for all folds:')\n",
        "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
        "print(f'> Loss: {np.mean(loss_per_fold)}')"
      ],
      "metadata": {
        "id": "4ycMo-Y51AJ7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e34dddec-ef3e-4239-9d8f-6b1079285c83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 32, 32, 32)        896       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  multiple                 0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 16, 16, 64)        18496     \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 8, 8, 64)          0         \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 8, 8, 128)         73856     \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 2048)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 10)                20490     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 113,738\n",
            "Trainable params: 113,738\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/30\n",
            "1250/1250 [==============================] - 27s 14ms/step - loss: 12.1284 - accuracy: 0.1007 - val_loss: 2.3104 - val_accuracy: 0.0964 - lr: 0.1000\n",
            "Epoch 2/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 2.3147 - accuracy: 0.0996 - val_loss: 2.3122 - val_accuracy: 0.1031 - lr: 0.1000\n",
            "Epoch 3/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 2.3149 - accuracy: 0.1026 - val_loss: 2.3137 - val_accuracy: 0.1011 - lr: 0.1000\n",
            "Epoch 4/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 2.3155 - accuracy: 0.0992 - val_loss: 2.3125 - val_accuracy: 0.1009 - lr: 0.1000\n",
            "Epoch 5/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 2.3148 - accuracy: 0.1014 - val_loss: 2.3182 - val_accuracy: 0.1031 - lr: 0.1000\n",
            "Epoch 6/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 2.3157 - accuracy: 0.1011 - val_loss: 2.3068 - val_accuracy: 0.0964 - lr: 0.1000\n",
            "Epoch 7/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 2.3146 - accuracy: 0.1010 - val_loss: 2.3116 - val_accuracy: 0.0996 - lr: 0.1000\n",
            "Epoch 8/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 2.3155 - accuracy: 0.1009 - val_loss: 2.3096 - val_accuracy: 0.1031 - lr: 0.1000\n",
            "Epoch 9/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 2.3150 - accuracy: 0.1001 - val_loss: 2.3256 - val_accuracy: 0.1001 - lr: 0.1000\n",
            "Epoch 10/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 2.3156 - accuracy: 0.0984 - val_loss: 2.3168 - val_accuracy: 0.1031 - lr: 0.1000\n",
            "Epoch 11/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 2.3156 - accuracy: 0.0980 - val_loss: 2.3080 - val_accuracy: 0.1031 - lr: 0.1000\n",
            "Epoch 12/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 2.3150 - accuracy: 0.0985 - val_loss: 2.3189 - val_accuracy: 0.1011 - lr: 0.1000\n",
            "Epoch 13/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 2.3150 - accuracy: 0.0987 - val_loss: 2.3072 - val_accuracy: 0.1004 - lr: 0.1000\n",
            "Epoch 14/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 2.3154 - accuracy: 0.1004 - val_loss: 2.3108 - val_accuracy: 0.1001 - lr: 0.1000\n",
            "Epoch 15/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 2.3159 - accuracy: 0.0987 - val_loss: 2.3223 - val_accuracy: 0.1001 - lr: 0.1000\n",
            "Epoch 16/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 2.3132 - accuracy: 0.1008 - val_loss: 2.3072 - val_accuracy: 0.1001 - lr: 0.0905\n",
            "Epoch 17/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 2.3130 - accuracy: 0.0989 - val_loss: 2.3091 - val_accuracy: 0.1031 - lr: 0.0819\n",
            "Epoch 18/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 2.3132 - accuracy: 0.0991 - val_loss: 2.3165 - val_accuracy: 0.1004 - lr: 0.0741\n",
            "Epoch 19/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 2.3122 - accuracy: 0.0986 - val_loss: 2.3136 - val_accuracy: 0.0964 - lr: 0.0670\n",
            "Epoch 20/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 2.3111 - accuracy: 0.0976 - val_loss: 2.3077 - val_accuracy: 0.1004 - lr: 0.0607\n",
            "Epoch 21/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 2.3094 - accuracy: 0.1006 - val_loss: 2.3074 - val_accuracy: 0.1009 - lr: 0.0549\n",
            "Epoch 22/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 2.3095 - accuracy: 0.0993 - val_loss: 2.3091 - val_accuracy: 0.0967 - lr: 0.0497\n",
            "Epoch 23/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 2.3083 - accuracy: 0.0997 - val_loss: 2.3079 - val_accuracy: 0.1001 - lr: 0.0449\n",
            "Epoch 24/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 2.3076 - accuracy: 0.1000 - val_loss: 2.3060 - val_accuracy: 0.1008 - lr: 0.0407\n",
            "Epoch 25/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 2.3072 - accuracy: 0.1020 - val_loss: 2.3106 - val_accuracy: 0.0964 - lr: 0.0368\n",
            "Epoch 26/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 2.3071 - accuracy: 0.0985 - val_loss: 2.3057 - val_accuracy: 0.0996 - lr: 0.0333\n",
            "Epoch 27/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 2.3070 - accuracy: 0.0991 - val_loss: 2.3081 - val_accuracy: 0.1009 - lr: 0.0301\n",
            "Epoch 28/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 2.3063 - accuracy: 0.0998 - val_loss: 2.3037 - val_accuracy: 0.1004 - lr: 0.0273\n",
            "Epoch 29/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 2.3058 - accuracy: 0.0992 - val_loss: 2.3043 - val_accuracy: 0.1009 - lr: 0.0247\n",
            "Epoch 30/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 2.3056 - accuracy: 0.0996 - val_loss: 2.3064 - val_accuracy: 0.0996 - lr: 0.0223\n",
            "Score for fold 1: loss of 2.3063862323760986; accuracy of 9.960000216960907%\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_3 (Conv2D)           (None, 32, 32, 32)        896       \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  multiple                 0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_4 (Conv2D)           (None, 16, 16, 64)        18496     \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 8, 8, 64)          0         \n",
            "                                                                 \n",
            " conv2d_5 (Conv2D)           (None, 8, 8, 128)         73856     \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 2048)              0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 10)                20490     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 113,738\n",
            "Trainable params: 113,738\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/30\n",
            "1250/1250 [==============================] - 19s 14ms/step - loss: 6.9954 - accuracy: 0.0992 - val_loss: 2.3200 - val_accuracy: 0.0984 - lr: 0.1000\n",
            "Epoch 2/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 2.3146 - accuracy: 0.1010 - val_loss: 2.3256 - val_accuracy: 0.0991 - lr: 0.1000\n",
            "Epoch 3/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 2.3148 - accuracy: 0.1024 - val_loss: 2.3122 - val_accuracy: 0.0975 - lr: 0.1000\n",
            "Epoch 4/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 2.3147 - accuracy: 0.0981 - val_loss: 2.3165 - val_accuracy: 0.1023 - lr: 0.1000\n",
            "Epoch 5/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 2.3174 - accuracy: 0.0960 - val_loss: 2.3278 - val_accuracy: 0.0984 - lr: 0.1000\n",
            "Epoch 6/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 2.3153 - accuracy: 0.0993 - val_loss: 2.3204 - val_accuracy: 0.1032 - lr: 0.1000\n",
            "Epoch 7/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 2.3165 - accuracy: 0.0962 - val_loss: 2.3072 - val_accuracy: 0.1023 - lr: 0.1000\n",
            "Epoch 8/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 2.3153 - accuracy: 0.0993 - val_loss: 2.3132 - val_accuracy: 0.1032 - lr: 0.1000\n",
            "Epoch 9/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 2.3159 - accuracy: 0.1026 - val_loss: 2.3118 - val_accuracy: 0.0969 - lr: 0.1000\n",
            "Epoch 10/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 2.3152 - accuracy: 0.1013 - val_loss: 2.3266 - val_accuracy: 0.0991 - lr: 0.1000\n",
            "Epoch 11/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 2.3160 - accuracy: 0.1000 - val_loss: 2.3254 - val_accuracy: 0.1021 - lr: 0.1000\n",
            "Epoch 12/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 2.3159 - accuracy: 0.0997 - val_loss: 2.3174 - val_accuracy: 0.0969 - lr: 0.1000\n",
            "Epoch 13/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 2.3161 - accuracy: 0.1008 - val_loss: 2.3353 - val_accuracy: 0.0991 - lr: 0.1000\n",
            "Epoch 14/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 2.3153 - accuracy: 0.1004 - val_loss: 2.3134 - val_accuracy: 0.0969 - lr: 0.1000\n",
            "Epoch 15/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 2.3160 - accuracy: 0.0972 - val_loss: 2.3155 - val_accuracy: 0.0984 - lr: 0.1000\n",
            "Epoch 16/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 2.3152 - accuracy: 0.1000 - val_loss: 2.3140 - val_accuracy: 0.1014 - lr: 0.0905\n",
            "Epoch 17/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 2.3125 - accuracy: 0.0984 - val_loss: 2.3046 - val_accuracy: 0.1014 - lr: 0.0819\n",
            "Epoch 18/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 2.3122 - accuracy: 0.0990 - val_loss: 2.3113 - val_accuracy: 0.1032 - lr: 0.0741\n",
            "Epoch 19/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 2.3113 - accuracy: 0.0999 - val_loss: 2.3116 - val_accuracy: 0.0984 - lr: 0.0670\n",
            "Epoch 20/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 2.3108 - accuracy: 0.0977 - val_loss: 2.3060 - val_accuracy: 0.1014 - lr: 0.0607\n",
            "Epoch 21/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 2.3098 - accuracy: 0.0973 - val_loss: 2.3062 - val_accuracy: 0.0969 - lr: 0.0549\n",
            "Epoch 22/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 2.3094 - accuracy: 0.1002 - val_loss: 2.3121 - val_accuracy: 0.1032 - lr: 0.0497\n",
            "Epoch 23/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 2.3087 - accuracy: 0.0993 - val_loss: 2.3076 - val_accuracy: 0.1007 - lr: 0.0449\n",
            "Epoch 24/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 2.3078 - accuracy: 0.0988 - val_loss: 2.3123 - val_accuracy: 0.1007 - lr: 0.0407\n",
            "Epoch 25/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 2.3078 - accuracy: 0.0979 - val_loss: 2.3067 - val_accuracy: 0.0991 - lr: 0.0368\n",
            "Epoch 26/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 2.3070 - accuracy: 0.0984 - val_loss: 2.3092 - val_accuracy: 0.1021 - lr: 0.0333\n",
            "Epoch 27/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 2.3067 - accuracy: 0.1028 - val_loss: 2.3120 - val_accuracy: 0.0984 - lr: 0.0301\n",
            "Epoch 28/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 2.3066 - accuracy: 0.0981 - val_loss: 2.3066 - val_accuracy: 0.1014 - lr: 0.0273\n",
            "Epoch 29/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 2.3058 - accuracy: 0.0950 - val_loss: 2.3081 - val_accuracy: 0.0984 - lr: 0.0247\n",
            "Epoch 30/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 2.3059 - accuracy: 0.0975 - val_loss: 2.3057 - val_accuracy: 0.0969 - lr: 0.0223\n",
            "Score for fold 2: loss of 2.3056581020355225; accuracy of 9.690000116825104%\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_6 (Conv2D)           (None, 32, 32, 32)        896       \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPooling  multiple                 0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_7 (Conv2D)           (None, 16, 16, 64)        18496     \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 8, 8, 64)          0         \n",
            "                                                                 \n",
            " conv2d_8 (Conv2D)           (None, 8, 8, 128)         73856     \n",
            "                                                                 \n",
            " flatten_2 (Flatten)         (None, 2048)              0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 10)                20490     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 113,738\n",
            "Trainable params: 113,738\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 18.4229 - accuracy: 0.1017 - val_loss: 2.3165 - val_accuracy: 0.1021 - lr: 0.1000\n",
            "Epoch 2/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 2.3159 - accuracy: 0.1002 - val_loss: 2.3138 - val_accuracy: 0.1021 - lr: 0.1000\n",
            "Epoch 3/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 2.3159 - accuracy: 0.0989 - val_loss: 2.3127 - val_accuracy: 0.1021 - lr: 0.1000\n",
            "Epoch 4/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 2.3160 - accuracy: 0.0995 - val_loss: 2.3316 - val_accuracy: 0.1021 - lr: 0.1000\n",
            "Epoch 5/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 2.3151 - accuracy: 0.0973 - val_loss: 2.3116 - val_accuracy: 0.0985 - lr: 0.1000\n",
            "Epoch 6/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 2.3152 - accuracy: 0.0993 - val_loss: 2.3123 - val_accuracy: 0.1035 - lr: 0.1000\n",
            "Epoch 7/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 2.3151 - accuracy: 0.0979 - val_loss: 2.3271 - val_accuracy: 0.0978 - lr: 0.1000\n",
            "Epoch 8/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 2.3155 - accuracy: 0.1001 - val_loss: 2.3103 - val_accuracy: 0.0973 - lr: 0.1000\n",
            "Epoch 9/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 2.3162 - accuracy: 0.0991 - val_loss: 2.3243 - val_accuracy: 0.0985 - lr: 0.1000\n",
            "Epoch 10/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 2.3162 - accuracy: 0.0975 - val_loss: 2.3230 - val_accuracy: 0.0985 - lr: 0.1000\n",
            "Epoch 11/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 2.3151 - accuracy: 0.1027 - val_loss: 2.3209 - val_accuracy: 0.1024 - lr: 0.1000\n",
            "Epoch 12/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 2.3147 - accuracy: 0.1007 - val_loss: 2.3179 - val_accuracy: 0.0987 - lr: 0.1000\n",
            "Epoch 13/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 2.3151 - accuracy: 0.0990 - val_loss: 2.3094 - val_accuracy: 0.0978 - lr: 0.1000\n",
            "Epoch 14/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 2.3153 - accuracy: 0.1012 - val_loss: 2.3094 - val_accuracy: 0.0998 - lr: 0.1000\n",
            "Epoch 15/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 2.3162 - accuracy: 0.0991 - val_loss: 2.3229 - val_accuracy: 0.1024 - lr: 0.1000\n",
            "Epoch 16/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 2.3139 - accuracy: 0.0979 - val_loss: 2.3138 - val_accuracy: 0.1024 - lr: 0.0905\n",
            "Epoch 17/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 2.3137 - accuracy: 0.0997 - val_loss: 2.3104 - val_accuracy: 0.0987 - lr: 0.0819\n",
            "Epoch 18/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 2.3115 - accuracy: 0.0975 - val_loss: 2.3231 - val_accuracy: 0.0978 - lr: 0.0741\n",
            "Epoch 19/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 2.3113 - accuracy: 0.0990 - val_loss: 2.3072 - val_accuracy: 0.0998 - lr: 0.0670\n",
            "Epoch 20/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 2.3108 - accuracy: 0.0994 - val_loss: 2.3137 - val_accuracy: 0.1014 - lr: 0.0607\n",
            "Epoch 21/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 2.3099 - accuracy: 0.1009 - val_loss: 2.3109 - val_accuracy: 0.0985 - lr: 0.0549\n",
            "Epoch 22/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 2.3090 - accuracy: 0.0978 - val_loss: 2.3095 - val_accuracy: 0.0985 - lr: 0.0497\n",
            "Epoch 23/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 2.3086 - accuracy: 0.0998 - val_loss: 2.3070 - val_accuracy: 0.0985 - lr: 0.0449\n",
            "Epoch 24/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 2.3075 - accuracy: 0.0994 - val_loss: 2.3052 - val_accuracy: 0.0973 - lr: 0.0407\n",
            "Epoch 25/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 2.3077 - accuracy: 0.1001 - val_loss: 2.3082 - val_accuracy: 0.0973 - lr: 0.0368\n",
            "Epoch 26/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 2.3072 - accuracy: 0.1019 - val_loss: 2.3034 - val_accuracy: 0.1024 - lr: 0.0333\n",
            "Epoch 27/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 2.3065 - accuracy: 0.0990 - val_loss: 2.3065 - val_accuracy: 0.1021 - lr: 0.0301\n",
            "Epoch 28/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 2.3066 - accuracy: 0.0978 - val_loss: 2.3058 - val_accuracy: 0.0998 - lr: 0.0273\n",
            "Epoch 29/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 2.3062 - accuracy: 0.0988 - val_loss: 2.3032 - val_accuracy: 0.1035 - lr: 0.0247\n",
            "Epoch 30/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 2.3058 - accuracy: 0.0999 - val_loss: 2.3035 - val_accuracy: 0.1035 - lr: 0.0223\n",
            "Score for fold 3: loss of 2.303461790084839; accuracy of 10.350000113248825%\n",
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_9 (Conv2D)           (None, 32, 32, 32)        896       \n",
            "                                                                 \n",
            " max_pooling2d_3 (MaxPooling  multiple                 0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_10 (Conv2D)          (None, 16, 16, 64)        18496     \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 8, 8, 64)          0         \n",
            "                                                                 \n",
            " conv2d_11 (Conv2D)          (None, 8, 8, 128)         73856     \n",
            "                                                                 \n",
            " flatten_3 (Flatten)         (None, 2048)              0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 10)                20490     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 113,738\n",
            "Trainable params: 113,738\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/30\n",
            "1250/1250 [==============================] - 19s 14ms/step - loss: 17.9240 - accuracy: 0.1023 - val_loss: 2.3206 - val_accuracy: 0.1021 - lr: 0.1000\n",
            "Epoch 2/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 2.3147 - accuracy: 0.1003 - val_loss: 2.3123 - val_accuracy: 0.1002 - lr: 0.1000\n",
            "Epoch 3/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 2.3259 - accuracy: 0.1002 - val_loss: 2.3086 - val_accuracy: 0.0974 - lr: 0.1000\n",
            "Epoch 4/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 2.3141 - accuracy: 0.0991 - val_loss: 2.3139 - val_accuracy: 0.1002 - lr: 0.1000\n",
            "Epoch 5/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 2.3147 - accuracy: 0.0984 - val_loss: 2.3153 - val_accuracy: 0.1021 - lr: 0.1000\n",
            "Epoch 6/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 2.3146 - accuracy: 0.0999 - val_loss: 2.3101 - val_accuracy: 0.0956 - lr: 0.1000\n",
            "Epoch 7/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 2.3150 - accuracy: 0.1002 - val_loss: 2.3228 - val_accuracy: 0.1023 - lr: 0.1000\n",
            "Epoch 8/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 2.3151 - accuracy: 0.0993 - val_loss: 2.3211 - val_accuracy: 0.1006 - lr: 0.1000\n",
            "Epoch 9/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 2.3147 - accuracy: 0.1009 - val_loss: 2.3117 - val_accuracy: 0.0968 - lr: 0.1000\n",
            "Epoch 10/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 2.3162 - accuracy: 0.0992 - val_loss: 2.3110 - val_accuracy: 0.1006 - lr: 0.1000\n",
            "Epoch 11/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 2.3151 - accuracy: 0.0987 - val_loss: 2.3127 - val_accuracy: 0.0956 - lr: 0.1000\n",
            "Epoch 12/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 2.3145 - accuracy: 0.1000 - val_loss: 2.3274 - val_accuracy: 0.0974 - lr: 0.1000\n",
            "Epoch 13/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 2.3162 - accuracy: 0.0985 - val_loss: 2.3184 - val_accuracy: 0.1021 - lr: 0.1000\n",
            "Epoch 14/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 2.3150 - accuracy: 0.0974 - val_loss: 2.3217 - val_accuracy: 0.1002 - lr: 0.1000\n",
            "Epoch 15/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 2.3147 - accuracy: 0.0992 - val_loss: 2.3111 - val_accuracy: 0.1027 - lr: 0.1000\n",
            "Epoch 16/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 2.3153 - accuracy: 0.0996 - val_loss: 2.3161 - val_accuracy: 0.1023 - lr: 0.0905\n",
            "Epoch 17/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 2.3125 - accuracy: 0.1013 - val_loss: 2.3111 - val_accuracy: 0.0956 - lr: 0.0819\n",
            "Epoch 18/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 2.3121 - accuracy: 0.1007 - val_loss: 2.3059 - val_accuracy: 0.1021 - lr: 0.0741\n",
            "Epoch 19/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 2.3118 - accuracy: 0.0969 - val_loss: 2.3127 - val_accuracy: 0.1003 - lr: 0.0670\n",
            "Epoch 20/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 2.3103 - accuracy: 0.0991 - val_loss: 2.3074 - val_accuracy: 0.1003 - lr: 0.0607\n",
            "Epoch 21/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 2.3099 - accuracy: 0.0988 - val_loss: 2.3166 - val_accuracy: 0.0956 - lr: 0.0549\n",
            "Epoch 22/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 2.3088 - accuracy: 0.0987 - val_loss: 2.3069 - val_accuracy: 0.1003 - lr: 0.0497\n",
            "Epoch 23/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 2.3084 - accuracy: 0.0997 - val_loss: 2.3079 - val_accuracy: 0.1023 - lr: 0.0449\n",
            "Epoch 24/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 2.3079 - accuracy: 0.0977 - val_loss: 2.3068 - val_accuracy: 0.1027 - lr: 0.0407\n",
            "Epoch 25/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 2.3070 - accuracy: 0.0986 - val_loss: 2.3066 - val_accuracy: 0.0968 - lr: 0.0368\n",
            "Epoch 26/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 2.3067 - accuracy: 0.1019 - val_loss: 2.3060 - val_accuracy: 0.0974 - lr: 0.0333\n",
            "Epoch 27/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 2.3063 - accuracy: 0.1004 - val_loss: 2.3060 - val_accuracy: 0.1021 - lr: 0.0301\n",
            "Epoch 28/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 2.3065 - accuracy: 0.0990 - val_loss: 2.3039 - val_accuracy: 0.1023 - lr: 0.0273\n",
            "Epoch 29/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 2.3058 - accuracy: 0.1017 - val_loss: 2.3050 - val_accuracy: 0.0968 - lr: 0.0247\n",
            "Epoch 30/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 2.3055 - accuracy: 0.0987 - val_loss: 2.3065 - val_accuracy: 0.1020 - lr: 0.0223\n",
            "Score for fold 4: loss of 2.3065106868743896; accuracy of 10.199999809265137%\n",
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_12 (Conv2D)          (None, 32, 32, 32)        896       \n",
            "                                                                 \n",
            " max_pooling2d_4 (MaxPooling  multiple                 0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_13 (Conv2D)          (None, 16, 16, 64)        18496     \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 8, 8, 64)          0         \n",
            "                                                                 \n",
            " conv2d_14 (Conv2D)          (None, 8, 8, 128)         73856     \n",
            "                                                                 \n",
            " flatten_4 (Flatten)         (None, 2048)              0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 10)                20490     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 113,738\n",
            "Trainable params: 113,738\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/30\n",
            "1250/1250 [==============================] - 19s 14ms/step - loss: 14.7966 - accuracy: 0.0985 - val_loss: 2.3147 - val_accuracy: 0.0980 - lr: 0.1000\n",
            "Epoch 2/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 2.3149 - accuracy: 0.0991 - val_loss: 2.3050 - val_accuracy: 0.0948 - lr: 0.1000\n",
            "Epoch 3/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 2.3163 - accuracy: 0.1005 - val_loss: 2.3131 - val_accuracy: 0.1014 - lr: 0.1000\n",
            "Epoch 4/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 2.3145 - accuracy: 0.1001 - val_loss: 2.3175 - val_accuracy: 0.0979 - lr: 0.1000\n",
            "Epoch 5/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 2.3158 - accuracy: 0.1018 - val_loss: 2.3158 - val_accuracy: 0.1014 - lr: 0.1000\n",
            "Epoch 6/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 2.3157 - accuracy: 0.1020 - val_loss: 2.3206 - val_accuracy: 0.0948 - lr: 0.1000\n",
            "Epoch 7/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 2.3150 - accuracy: 0.0978 - val_loss: 2.3218 - val_accuracy: 0.0979 - lr: 0.1000\n",
            "Epoch 8/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 2.3162 - accuracy: 0.0974 - val_loss: 2.3119 - val_accuracy: 0.1006 - lr: 0.1000\n",
            "Epoch 9/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 2.3158 - accuracy: 0.0984 - val_loss: 2.3152 - val_accuracy: 0.0979 - lr: 0.1000\n",
            "Epoch 10/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 2.3150 - accuracy: 0.1006 - val_loss: 2.3172 - val_accuracy: 0.1014 - lr: 0.1000\n",
            "Epoch 11/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 2.3154 - accuracy: 0.0972 - val_loss: 2.3197 - val_accuracy: 0.0980 - lr: 0.1000\n",
            "Epoch 12/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 2.3153 - accuracy: 0.0995 - val_loss: 2.3146 - val_accuracy: 0.1014 - lr: 0.1000\n",
            "Epoch 13/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 2.3149 - accuracy: 0.0990 - val_loss: 2.3153 - val_accuracy: 0.1014 - lr: 0.1000\n",
            "Epoch 14/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 2.3151 - accuracy: 0.1001 - val_loss: 2.3279 - val_accuracy: 0.1014 - lr: 0.1000\n",
            "Epoch 15/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 2.3156 - accuracy: 0.0985 - val_loss: 2.3193 - val_accuracy: 0.1006 - lr: 0.1000\n",
            "Epoch 16/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 2.3151 - accuracy: 0.0984 - val_loss: 2.3088 - val_accuracy: 0.1030 - lr: 0.0905\n",
            "Epoch 17/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 2.3135 - accuracy: 0.0995 - val_loss: 2.3097 - val_accuracy: 0.1014 - lr: 0.0819\n",
            "Epoch 18/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 2.3119 - accuracy: 0.0987 - val_loss: 2.3201 - val_accuracy: 0.1030 - lr: 0.0741\n",
            "Epoch 19/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 2.3112 - accuracy: 0.1014 - val_loss: 2.3095 - val_accuracy: 0.1025 - lr: 0.0670\n",
            "Epoch 20/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 2.3102 - accuracy: 0.1025 - val_loss: 2.3087 - val_accuracy: 0.1030 - lr: 0.0607\n",
            "Epoch 21/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 2.3101 - accuracy: 0.0991 - val_loss: 2.3175 - val_accuracy: 0.1014 - lr: 0.0549\n",
            "Epoch 22/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 2.3088 - accuracy: 0.1002 - val_loss: 2.3098 - val_accuracy: 0.1014 - lr: 0.0497\n",
            "Epoch 23/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 2.3085 - accuracy: 0.1002 - val_loss: 2.3056 - val_accuracy: 0.0979 - lr: 0.0449\n",
            "Epoch 24/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 2.3076 - accuracy: 0.1003 - val_loss: 2.3091 - val_accuracy: 0.0979 - lr: 0.0407\n",
            "Epoch 25/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 2.3071 - accuracy: 0.1000 - val_loss: 2.3104 - val_accuracy: 0.0980 - lr: 0.0368\n",
            "Epoch 26/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 2.3070 - accuracy: 0.0984 - val_loss: 2.3070 - val_accuracy: 0.1030 - lr: 0.0333\n",
            "Epoch 27/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 2.3070 - accuracy: 0.0995 - val_loss: 2.3061 - val_accuracy: 0.0997 - lr: 0.0301\n",
            "Epoch 28/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 2.3061 - accuracy: 0.0993 - val_loss: 2.3036 - val_accuracy: 0.1007 - lr: 0.0273\n",
            "Epoch 29/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 2.3058 - accuracy: 0.0993 - val_loss: 2.3069 - val_accuracy: 0.0980 - lr: 0.0247\n",
            "Epoch 30/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 2.3054 - accuracy: 0.0998 - val_loss: 2.3069 - val_accuracy: 0.1007 - lr: 0.0223\n",
            "Score for fold 5: loss of 2.3068761825561523; accuracy of 10.06999984383583%\n",
            "------------------------------------------------------------------------\n",
            "Score per fold\n",
            "------------------------------------------------------------------------\n",
            "> Fold 1 - Loss: 2.3063862323760986 - Accuracy: 9.960000216960907%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 2 - Loss: 2.3056581020355225 - Accuracy: 9.690000116825104%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 3 - Loss: 2.303461790084839 - Accuracy: 10.350000113248825%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 4 - Loss: 2.3065106868743896 - Accuracy: 10.199999809265137%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 5 - Loss: 2.3068761825561523 - Accuracy: 10.06999984383583%\n",
            "------------------------------------------------------------------------\n",
            "Average scores for all folds:\n",
            "> Accuracy: 10.05400002002716 (+- 0.22383917463852354)\n",
            "> Loss: 2.3057785987854005\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Learning_rate=0.1 & No decay"
      ],
      "metadata": {
        "id": "nKFH5cs1tXma"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# K-fold Cross Validation model evaluation\n",
        "acc_per_fold = []\n",
        "loss_per_fold = []\n",
        "\n",
        "with tf.device('/device:GPU:0'):\n",
        "  fold_no = 1\n",
        "  for train, valid in kfold.split(trainX, trainy):\n",
        "\n",
        "    model = CNN_Model(batch_size=32, depth=3, hidden_neurons_1=32, hidden_neurons_2=64, hidden_neurons_3=128, kernel_size_1=3, kernel_size_2=3, kernel_size_3=3, \n",
        "               dropout_rate=0.2, batch_norm=False, pooling='max', layer_activation_1='relu', layer_activation_2='relu', layer_activation_3='relu',\n",
        "               kernel_initialiser='glorot_uniform', bias_initialiser='zeros', optimiser='Adam', weight_decay=None, learning_rate=0.1, output_activation='softmax', SGD_momentum=0) \n",
        "  \n",
        "    train_gen = tf.keras.preprocessing.image.ImageDataGenerator(width_shift_range=0.1,\n",
        "      height_shift_range=0.1,\n",
        "      horizontal_flip=True)\n",
        "    train_gen.fit(trainX[train])  #Data Augmentation\n",
        "  \n",
        "    print('------------------------------------------------------------------------')\n",
        "    print(f'Training for fold {fold_no} ...')\n",
        "\n",
        "    history = model.train(\n",
        "              train_gen,\n",
        "                trainX[train],\n",
        "                  trainy[train],\n",
        "                trainX[valid],\n",
        "                  trainy[valid],\n",
        "                  lr_rate=False)  #Don't use learning scheduler\n",
        "\n",
        "    scores = model.evaluate(trainX[valid], trainy[valid])\n",
        "\n",
        "    print(f'Score for fold {fold_no}: loss of {scores[0]}; accuracy of {scores[1]*100}%')\n",
        "    acc_per_fold.append(scores[1] * 100)\n",
        "    loss_per_fold.append(scores[0])\n",
        "\n",
        "    fold_no+=1\n",
        "\n",
        "# == Provide average scores ==\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Score per fold')\n",
        "for i in range(0, len(acc_per_fold)):\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Average scores for all folds:')\n",
        "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
        "print(f'> Loss: {np.mean(loss_per_fold)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "en8lQ5AjYTvv",
        "outputId": "9aba5e3f-e97a-40c6-e3d7-ba96b2d0f85c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_14\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_42 (Conv2D)          (None, 32, 32, 32)        896       \n",
            "                                                                 \n",
            " max_pooling2d_14 (MaxPoolin  multiple                 0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_43 (Conv2D)          (None, 16, 16, 64)        18496     \n",
            "                                                                 \n",
            " dropout_14 (Dropout)        (None, 8, 8, 64)          0         \n",
            "                                                                 \n",
            " conv2d_44 (Conv2D)          (None, 8, 8, 128)         73856     \n",
            "                                                                 \n",
            " flatten_14 (Flatten)        (None, 2048)              0         \n",
            "                                                                 \n",
            " dense_14 (Dense)            (None, 10)                20490     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 113,738\n",
            "Trainable params: 113,738\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/25\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 4.8086 - accuracy: 0.0958 - val_loss: 2.3137 - val_accuracy: 0.0983\n",
            "Epoch 2/25\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 2.3153 - accuracy: 0.0984 - val_loss: 2.3149 - val_accuracy: 0.0983\n",
            "Epoch 3/25\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 2.3163 - accuracy: 0.0984 - val_loss: 2.3127 - val_accuracy: 0.0983\n",
            "Epoch 4/25\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 2.3146 - accuracy: 0.0992 - val_loss: 2.3121 - val_accuracy: 0.1018\n",
            "Epoch 5/25\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 2.3152 - accuracy: 0.0994 - val_loss: 2.3175 - val_accuracy: 0.0983\n",
            "Epoch 6/25\n",
            "1250/1250 [==============================] - 20s 16ms/step - loss: 2.3163 - accuracy: 0.0988 - val_loss: 2.3125 - val_accuracy: 0.0951\n",
            "Epoch 7/25\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 2.3152 - accuracy: 0.1009 - val_loss: 2.3089 - val_accuracy: 0.1050\n",
            "Epoch 8/25\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 2.3149 - accuracy: 0.1003 - val_loss: 2.3239 - val_accuracy: 0.0989\n",
            "Epoch 9/25\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 2.3148 - accuracy: 0.0990 - val_loss: 2.3226 - val_accuracy: 0.0983\n",
            "Epoch 10/25\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 2.3144 - accuracy: 0.1011 - val_loss: 2.3134 - val_accuracy: 0.1068\n",
            "Epoch 11/25\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 2.3164 - accuracy: 0.1001 - val_loss: 2.3137 - val_accuracy: 0.1018\n",
            "Epoch 12/25\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 2.3156 - accuracy: 0.0991 - val_loss: 2.3091 - val_accuracy: 0.1018\n",
            "Epoch 13/25\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 2.3147 - accuracy: 0.1018 - val_loss: 2.3139 - val_accuracy: 0.0961\n",
            "Epoch 14/25\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 2.3154 - accuracy: 0.0990 - val_loss: 2.3255 - val_accuracy: 0.0983\n",
            "Epoch 15/25\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 2.3150 - accuracy: 0.0996 - val_loss: 2.3215 - val_accuracy: 0.0988\n",
            "Epoch 16/25\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 2.3161 - accuracy: 0.0991 - val_loss: 2.3236 - val_accuracy: 0.0961\n",
            "Epoch 17/25\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 2.3166 - accuracy: 0.0983 - val_loss: 2.3107 - val_accuracy: 0.0951\n",
            "Epoch 18/25\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 2.3152 - accuracy: 0.1005 - val_loss: 2.3163 - val_accuracy: 0.0989\n",
            "Epoch 19/25\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 2.3156 - accuracy: 0.0995 - val_loss: 2.3127 - val_accuracy: 0.1018\n",
            "Epoch 20/25\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 2.3164 - accuracy: 0.0984 - val_loss: 2.3118 - val_accuracy: 0.1018\n",
            "Epoch 21/25\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 2.3156 - accuracy: 0.0981 - val_loss: 2.3137 - val_accuracy: 0.1031\n",
            "Epoch 22/25\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 2.3146 - accuracy: 0.1005 - val_loss: 2.3102 - val_accuracy: 0.0951\n",
            "Epoch 23/25\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 2.3145 - accuracy: 0.0997 - val_loss: 2.3064 - val_accuracy: 0.0983\n",
            "Epoch 24/25\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 2.3145 - accuracy: 0.1007 - val_loss: 2.3240 - val_accuracy: 0.1068\n",
            "Epoch 25/25\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 2.3166 - accuracy: 0.1008 - val_loss: 2.3122 - val_accuracy: 0.1050\n",
            "Score for fold 1: loss of 2.31221866607666; accuracy of 10.499999672174454%\n",
            "Model: \"sequential_15\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_45 (Conv2D)          (None, 32, 32, 32)        896       \n",
            "                                                                 \n",
            " max_pooling2d_15 (MaxPoolin  multiple                 0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_46 (Conv2D)          (None, 16, 16, 64)        18496     \n",
            "                                                                 \n",
            " dropout_15 (Dropout)        (None, 8, 8, 64)          0         \n",
            "                                                                 \n",
            " conv2d_47 (Conv2D)          (None, 8, 8, 128)         73856     \n",
            "                                                                 \n",
            " flatten_15 (Flatten)        (None, 2048)              0         \n",
            "                                                                 \n",
            " dense_15 (Dense)            (None, 10)                20490     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 113,738\n",
            "Trainable params: 113,738\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/25\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 8.2825 - accuracy: 0.1008 - val_loss: 2.3162 - val_accuracy: 0.0958\n",
            "Epoch 2/25\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 2.3145 - accuracy: 0.1007 - val_loss: 2.3123 - val_accuracy: 0.0983\n",
            "Epoch 3/25\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 2.3139 - accuracy: 0.0974 - val_loss: 2.3345 - val_accuracy: 0.0983\n",
            "Epoch 4/25\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 2.3153 - accuracy: 0.0998 - val_loss: 2.3114 - val_accuracy: 0.0988\n",
            "Epoch 5/25\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 2.3265 - accuracy: 0.0985 - val_loss: 2.3158 - val_accuracy: 0.0973\n",
            "Epoch 6/25\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 2.3171 - accuracy: 0.0996 - val_loss: 2.3215 - val_accuracy: 0.0973\n",
            "Epoch 7/25\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 2.3155 - accuracy: 0.0991 - val_loss: 2.3213 - val_accuracy: 0.1013\n",
            "Epoch 8/25\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 2.3159 - accuracy: 0.0998 - val_loss: 2.3153 - val_accuracy: 0.1003\n",
            "Epoch 9/25\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 2.3153 - accuracy: 0.0997 - val_loss: 2.3169 - val_accuracy: 0.1024\n",
            "Epoch 10/25\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 2.3155 - accuracy: 0.1016 - val_loss: 2.3179 - val_accuracy: 0.0988\n",
            "Epoch 11/25\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 2.3164 - accuracy: 0.0979 - val_loss: 2.3123 - val_accuracy: 0.0983\n",
            "Epoch 12/25\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 2.3147 - accuracy: 0.1018 - val_loss: 2.3226 - val_accuracy: 0.1013\n",
            "Epoch 13/25\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 2.3162 - accuracy: 0.0995 - val_loss: 2.3265 - val_accuracy: 0.0983\n",
            "Epoch 14/25\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 2.3156 - accuracy: 0.0996 - val_loss: 2.3056 - val_accuracy: 0.0983\n",
            "Epoch 15/25\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 2.3149 - accuracy: 0.0985 - val_loss: 2.3152 - val_accuracy: 0.0958\n",
            "Epoch 16/25\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 2.3151 - accuracy: 0.0983 - val_loss: 2.3189 - val_accuracy: 0.0973\n",
            "Epoch 17/25\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 2.3164 - accuracy: 0.0984 - val_loss: 2.3146 - val_accuracy: 0.1024\n",
            "Epoch 18/25\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 2.3151 - accuracy: 0.1001 - val_loss: 2.3141 - val_accuracy: 0.0983\n",
            "Epoch 19/25\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 2.3151 - accuracy: 0.0990 - val_loss: 2.3071 - val_accuracy: 0.0988\n",
            "Epoch 20/25\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 2.3147 - accuracy: 0.1006 - val_loss: 2.3101 - val_accuracy: 0.0973\n",
            "Epoch 21/25\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 2.3147 - accuracy: 0.1012 - val_loss: 2.3112 - val_accuracy: 0.0984\n",
            "Epoch 22/25\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 2.3162 - accuracy: 0.0988 - val_loss: 2.3132 - val_accuracy: 0.1004\n",
            "Epoch 23/25\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 2.3153 - accuracy: 0.1010 - val_loss: 2.3174 - val_accuracy: 0.1013\n",
            "Epoch 24/25\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 2.3148 - accuracy: 0.1010 - val_loss: 2.3141 - val_accuracy: 0.1004\n",
            "Epoch 25/25\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 2.3167 - accuracy: 0.0984 - val_loss: 2.3177 - val_accuracy: 0.1003\n",
            "Score for fold 2: loss of 2.3177082538604736; accuracy of 10.029999911785126%\n",
            "Model: \"sequential_16\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_48 (Conv2D)          (None, 32, 32, 32)        896       \n",
            "                                                                 \n",
            " max_pooling2d_16 (MaxPoolin  multiple                 0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_49 (Conv2D)          (None, 16, 16, 64)        18496     \n",
            "                                                                 \n",
            " dropout_16 (Dropout)        (None, 8, 8, 64)          0         \n",
            "                                                                 \n",
            " conv2d_50 (Conv2D)          (None, 8, 8, 128)         73856     \n",
            "                                                                 \n",
            " flatten_16 (Flatten)        (None, 2048)              0         \n",
            "                                                                 \n",
            " dense_16 (Dense)            (None, 10)                20490     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 113,738\n",
            "Trainable params: 113,738\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/25\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 5.1355 - accuracy: 0.0998 - val_loss: 2.3066 - val_accuracy: 0.1002\n",
            "Epoch 2/25\n",
            "1250/1250 [==============================] - 15s 12ms/step - loss: 2.3153 - accuracy: 0.1001 - val_loss: 2.3124 - val_accuracy: 0.1035\n",
            "Epoch 3/25\n",
            "1250/1250 [==============================] - 16s 12ms/step - loss: 2.3156 - accuracy: 0.0995 - val_loss: 2.3301 - val_accuracy: 0.0998\n",
            "Epoch 4/25\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 2.3158 - accuracy: 0.0981 - val_loss: 2.3201 - val_accuracy: 0.0999\n",
            "Epoch 5/25\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 2.3152 - accuracy: 0.0985 - val_loss: 2.3161 - val_accuracy: 0.0970\n",
            "Epoch 6/25\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 2.3153 - accuracy: 0.0971 - val_loss: 2.3140 - val_accuracy: 0.0999\n",
            "Epoch 7/25\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 2.3148 - accuracy: 0.1007 - val_loss: 2.3088 - val_accuracy: 0.1021\n",
            "Epoch 8/25\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 2.3151 - accuracy: 0.0998 - val_loss: 2.3211 - val_accuracy: 0.0998\n",
            "Epoch 9/25\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 2.3160 - accuracy: 0.1003 - val_loss: 2.3152 - val_accuracy: 0.1035\n",
            "Epoch 10/25\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 2.3161 - accuracy: 0.0986 - val_loss: 2.3135 - val_accuracy: 0.0970\n",
            "Epoch 11/25\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 2.3163 - accuracy: 0.0980 - val_loss: 2.3121 - val_accuracy: 0.0999\n",
            "Epoch 12/25\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 2.3159 - accuracy: 0.0994 - val_loss: 2.3158 - val_accuracy: 0.0999\n",
            "Epoch 13/25\n",
            "1250/1250 [==============================] - 16s 12ms/step - loss: 2.3154 - accuracy: 0.0997 - val_loss: 2.3082 - val_accuracy: 0.0970\n",
            "Epoch 14/25\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 2.3150 - accuracy: 0.1005 - val_loss: 2.3227 - val_accuracy: 0.1053\n",
            "Epoch 15/25\n",
            "1250/1250 [==============================] - 16s 12ms/step - loss: 2.3157 - accuracy: 0.1004 - val_loss: 2.3140 - val_accuracy: 0.1002\n",
            "Epoch 16/25\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 2.3147 - accuracy: 0.0966 - val_loss: 2.3136 - val_accuracy: 0.0998\n",
            "Epoch 17/25\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 2.3155 - accuracy: 0.0988 - val_loss: 2.3260 - val_accuracy: 0.0999\n",
            "Epoch 18/25\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 2.3167 - accuracy: 0.0994 - val_loss: 2.3128 - val_accuracy: 0.1002\n",
            "Epoch 19/25\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 2.3144 - accuracy: 0.1007 - val_loss: 2.3140 - val_accuracy: 0.1021\n",
            "Epoch 20/25\n",
            "1250/1250 [==============================] - 15s 12ms/step - loss: 2.3173 - accuracy: 0.1000 - val_loss: 2.3303 - val_accuracy: 0.0970\n",
            "Epoch 21/25\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 2.3155 - accuracy: 0.0989 - val_loss: 2.3154 - val_accuracy: 0.1053\n",
            "Epoch 22/25\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 2.3157 - accuracy: 0.1006 - val_loss: 2.3170 - val_accuracy: 0.1053\n",
            "Epoch 23/25\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 2.3167 - accuracy: 0.0973 - val_loss: 2.3270 - val_accuracy: 0.1002\n",
            "Epoch 24/25\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 2.3150 - accuracy: 0.0990 - val_loss: 2.3100 - val_accuracy: 0.1035\n",
            "Epoch 25/25\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 2.3161 - accuracy: 0.0999 - val_loss: 2.3136 - val_accuracy: 0.0998\n",
            "Score for fold 3: loss of 2.313586950302124; accuracy of 9.97999981045723%\n",
            "Model: \"sequential_17\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_51 (Conv2D)          (None, 32, 32, 32)        896       \n",
            "                                                                 \n",
            " max_pooling2d_17 (MaxPoolin  multiple                 0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_52 (Conv2D)          (None, 16, 16, 64)        18496     \n",
            "                                                                 \n",
            " dropout_17 (Dropout)        (None, 8, 8, 64)          0         \n",
            "                                                                 \n",
            " conv2d_53 (Conv2D)          (None, 8, 8, 128)         73856     \n",
            "                                                                 \n",
            " flatten_17 (Flatten)        (None, 2048)              0         \n",
            "                                                                 \n",
            " dense_17 (Dense)            (None, 10)                20490     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 113,738\n",
            "Trainable params: 113,738\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/25\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 4.8384 - accuracy: 0.1010 - val_loss: 2.3120 - val_accuracy: 0.0953\n",
            "Epoch 2/25\n",
            "1250/1250 [==============================] - 16s 12ms/step - loss: 2.3160 - accuracy: 0.0997 - val_loss: 2.3199 - val_accuracy: 0.0953\n",
            "Epoch 3/25\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 2.3145 - accuracy: 0.0978 - val_loss: 2.3047 - val_accuracy: 0.1003\n",
            "Epoch 4/25\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 2.3153 - accuracy: 0.0985 - val_loss: 2.3138 - val_accuracy: 0.1003\n",
            "Epoch 5/25\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 2.3163 - accuracy: 0.0949 - val_loss: 2.3087 - val_accuracy: 0.0996\n",
            "Epoch 6/25\n",
            "1250/1250 [==============================] - 15s 12ms/step - loss: 2.3149 - accuracy: 0.1027 - val_loss: 2.3242 - val_accuracy: 0.0953\n",
            "Epoch 7/25\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 2.3166 - accuracy: 0.1009 - val_loss: 2.3134 - val_accuracy: 0.1027\n",
            "Epoch 8/25\n",
            "1250/1250 [==============================] - 15s 12ms/step - loss: 2.3151 - accuracy: 0.0989 - val_loss: 2.3099 - val_accuracy: 0.1013\n",
            "Epoch 9/25\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 2.3154 - accuracy: 0.1014 - val_loss: 2.3201 - val_accuracy: 0.1013\n",
            "Epoch 10/25\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 2.3162 - accuracy: 0.0975 - val_loss: 2.3186 - val_accuracy: 0.1027\n",
            "Epoch 11/25\n",
            "1250/1250 [==============================] - 16s 12ms/step - loss: 2.3139 - accuracy: 0.1001 - val_loss: 2.3170 - val_accuracy: 0.0989\n",
            "Epoch 12/25\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 2.3148 - accuracy: 0.0974 - val_loss: 2.3204 - val_accuracy: 0.1007\n",
            "Epoch 13/25\n",
            "1250/1250 [==============================] - 16s 12ms/step - loss: 2.3159 - accuracy: 0.0991 - val_loss: 2.3208 - val_accuracy: 0.1008\n",
            "Epoch 14/25\n",
            "1250/1250 [==============================] - 16s 12ms/step - loss: 2.3139 - accuracy: 0.0993 - val_loss: 2.3118 - val_accuracy: 0.0996\n",
            "Epoch 15/25\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 2.3155 - accuracy: 0.0987 - val_loss: 2.3141 - val_accuracy: 0.1024\n",
            "Epoch 16/25\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 2.3145 - accuracy: 0.1014 - val_loss: 2.3158 - val_accuracy: 0.1027\n",
            "Epoch 17/25\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 2.3149 - accuracy: 0.1021 - val_loss: 2.3147 - val_accuracy: 0.0980\n",
            "Epoch 18/25\n",
            "1250/1250 [==============================] - 15s 12ms/step - loss: 2.3152 - accuracy: 0.1000 - val_loss: 2.3280 - val_accuracy: 0.1007\n",
            "Epoch 19/25\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 2.3148 - accuracy: 0.1028 - val_loss: 2.3248 - val_accuracy: 0.0989\n",
            "Epoch 20/25\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 2.3162 - accuracy: 0.0993 - val_loss: 2.3113 - val_accuracy: 0.1024\n",
            "Epoch 21/25\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 2.3162 - accuracy: 0.0974 - val_loss: 2.3131 - val_accuracy: 0.0953\n",
            "Epoch 22/25\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 2.3154 - accuracy: 0.0982 - val_loss: 2.3188 - val_accuracy: 0.0996\n",
            "Epoch 23/25\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 2.3161 - accuracy: 0.0989 - val_loss: 2.3088 - val_accuracy: 0.0980\n",
            "Epoch 24/25\n",
            "1250/1250 [==============================] - 15s 12ms/step - loss: 2.3159 - accuracy: 0.0961 - val_loss: 2.3195 - val_accuracy: 0.0989\n",
            "Epoch 25/25\n",
            "1250/1250 [==============================] - 16s 12ms/step - loss: 2.3164 - accuracy: 0.0996 - val_loss: 2.3102 - val_accuracy: 0.0996\n",
            "Score for fold 4: loss of 2.310189962387085; accuracy of 9.960000216960907%\n",
            "Model: \"sequential_18\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_54 (Conv2D)          (None, 32, 32, 32)        896       \n",
            "                                                                 \n",
            " max_pooling2d_18 (MaxPoolin  multiple                 0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_55 (Conv2D)          (None, 16, 16, 64)        18496     \n",
            "                                                                 \n",
            " dropout_18 (Dropout)        (None, 8, 8, 64)          0         \n",
            "                                                                 \n",
            " conv2d_56 (Conv2D)          (None, 8, 8, 128)         73856     \n",
            "                                                                 \n",
            " flatten_18 (Flatten)        (None, 2048)              0         \n",
            "                                                                 \n",
            " dense_18 (Dense)            (None, 10)                20490     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 113,738\n",
            "Trainable params: 113,738\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/25\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 8.5029 - accuracy: 0.0994 - val_loss: 2.3098 - val_accuracy: 0.1008\n",
            "Epoch 2/25\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 2.3161 - accuracy: 0.0996 - val_loss: 2.3217 - val_accuracy: 0.1033\n",
            "Epoch 3/25\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 2.3160 - accuracy: 0.0997 - val_loss: 2.3159 - val_accuracy: 0.0984\n",
            "Epoch 4/25\n",
            "1250/1250 [==============================] - 15s 12ms/step - loss: 2.3156 - accuracy: 0.1002 - val_loss: 2.3153 - val_accuracy: 0.0919\n",
            "Epoch 5/25\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 2.3158 - accuracy: 0.0994 - val_loss: 2.3133 - val_accuracy: 0.0982\n",
            "Epoch 6/25\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 2.3157 - accuracy: 0.1011 - val_loss: 2.3128 - val_accuracy: 0.1033\n",
            "Epoch 7/25\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 2.3145 - accuracy: 0.1020 - val_loss: 2.3166 - val_accuracy: 0.1033\n",
            "Epoch 8/25\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 2.3167 - accuracy: 0.1003 - val_loss: 2.3185 - val_accuracy: 0.1015\n",
            "Epoch 9/25\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 2.3153 - accuracy: 0.1016 - val_loss: 2.3129 - val_accuracy: 0.1064\n",
            "Epoch 10/25\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 2.3154 - accuracy: 0.1007 - val_loss: 2.3083 - val_accuracy: 0.1028\n",
            "Epoch 11/25\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 2.3149 - accuracy: 0.1017 - val_loss: 2.3117 - val_accuracy: 0.0992\n",
            "Epoch 12/25\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 2.3147 - accuracy: 0.1004 - val_loss: 2.3149 - val_accuracy: 0.0984\n",
            "Epoch 13/25\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 2.3162 - accuracy: 0.0991 - val_loss: 2.3241 - val_accuracy: 0.1033\n",
            "Epoch 14/25\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 2.3160 - accuracy: 0.1001 - val_loss: 2.3214 - val_accuracy: 0.0982\n",
            "Epoch 15/25\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 2.3138 - accuracy: 0.1030 - val_loss: 2.3174 - val_accuracy: 0.0975\n",
            "Epoch 16/25\n",
            "1250/1250 [==============================] - 15s 12ms/step - loss: 2.3154 - accuracy: 0.1006 - val_loss: 2.3150 - val_accuracy: 0.1028\n",
            "Epoch 17/25\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 2.3154 - accuracy: 0.0998 - val_loss: 2.3121 - val_accuracy: 0.1015\n",
            "Epoch 18/25\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 2.3151 - accuracy: 0.0999 - val_loss: 2.3192 - val_accuracy: 0.1008\n",
            "Epoch 19/25\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 2.3147 - accuracy: 0.0996 - val_loss: 2.3124 - val_accuracy: 0.0992\n",
            "Epoch 20/25\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 2.3157 - accuracy: 0.1001 - val_loss: 2.3331 - val_accuracy: 0.1028\n",
            "Epoch 21/25\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 2.3154 - accuracy: 0.1015 - val_loss: 2.3298 - val_accuracy: 0.1064\n",
            "Epoch 22/25\n",
            "1250/1250 [==============================] - 15s 12ms/step - loss: 2.3168 - accuracy: 0.0978 - val_loss: 2.3155 - val_accuracy: 0.0982\n",
            "Epoch 23/25\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 2.3152 - accuracy: 0.1021 - val_loss: 2.3075 - val_accuracy: 0.0982\n",
            "Epoch 24/25\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 2.3146 - accuracy: 0.1003 - val_loss: 2.3073 - val_accuracy: 0.1028\n",
            "Epoch 25/25\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 2.3149 - accuracy: 0.0991 - val_loss: 2.3155 - val_accuracy: 0.0919\n",
            "Score for fold 5: loss of 2.315509080886841; accuracy of 9.189999848604202%\n",
            "------------------------------------------------------------------------\n",
            "Score per fold\n",
            "------------------------------------------------------------------------\n",
            "> Fold 1 - Loss: 2.309495687484741 - Accuracy: 10.409999638795853%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 2 - Loss: 2.305002450942993 - Accuracy: 9.600000083446503%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 3 - Loss: 2.3078553676605225 - Accuracy: 9.809999912977219%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 4 - Loss: 2.306668996810913 - Accuracy: 10.130000114440918%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 5 - Loss: 2.306671380996704 - Accuracy: 9.939999878406525%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 6 - Loss: 2.31221866607666 - Accuracy: 10.499999672174454%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 7 - Loss: 2.3177082538604736 - Accuracy: 10.029999911785126%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 8 - Loss: 2.313586950302124 - Accuracy: 9.97999981045723%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 9 - Loss: 2.310189962387085 - Accuracy: 9.960000216960907%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 10 - Loss: 2.315509080886841 - Accuracy: 9.189999848604202%\n",
            "------------------------------------------------------------------------\n",
            "Average scores for all folds:\n",
            "> Accuracy: 9.954999908804893 (+- 0.35685424682385614)\n",
            "> Loss: 2.310490679740906\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Learning_rate=0.1 & Step decay"
      ],
      "metadata": {
        "id": "T_Zyq8YVtZ4i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# K-fold Cross Validation model evaluation\n",
        "acc_per_fold = []\n",
        "loss_per_fold = []\n",
        "\n",
        "with tf.device('/device:GPU:0'):\n",
        "  fold_no = 1\n",
        "  for train, valid in kfold.split(trainX, trainy):\n",
        "\n",
        "    model = CNN_Model(batch_size=32, depth=3, hidden_neurons_1=32, hidden_neurons_2=64, hidden_neurons_3=128, kernel_size_1=3, kernel_size_2=3, kernel_size_3=3, \n",
        "               dropout_rate=0.2, batch_norm=False, pooling='max', layer_activation_1='relu', layer_activation_2='relu', layer_activation_3='relu',\n",
        "               kernel_initialiser='glorot_uniform', bias_initialiser='zeros', optimiser='Adam', weight_decay=None, learning_rate=0.1, output_activation='softmax', SGD_momentum=0) \n",
        "  \n",
        "    train_gen = tf.keras.preprocessing.image.ImageDataGenerator(width_shift_range=0.1,\n",
        "      height_shift_range=0.1,\n",
        "      horizontal_flip=True)\n",
        "    train_gen.fit(trainX[train]) #Data Augmentation\n",
        "  \n",
        "    print('------------------------------------------------------------------------')\n",
        "    print(f'Training for fold {fold_no} ...')\n",
        "\n",
        "    history = model.train(\n",
        "              train_gen,\n",
        "                trainX[train],\n",
        "                  trainy[train],\n",
        "                trainX[valid],\n",
        "                  trainy[valid],\n",
        "                  lr_rate_type='step') #Step learning scheduler\n",
        "\n",
        "    scores = model.evaluate(trainX[valid], trainy[valid])\n",
        "\n",
        "    print(f'Score for fold {fold_no}: loss of {scores[0]}; accuracy of {scores[1]*100}%')\n",
        "    acc_per_fold.append(scores[1] * 100)\n",
        "    loss_per_fold.append(scores[0])\n",
        "\n",
        "    fold_no+=1\n",
        "\n",
        "# == Provide average scores ==\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Score per fold')\n",
        "for i in range(0, len(acc_per_fold)):\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Average scores for all folds:')\n",
        "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
        "print(f'> Loss: {np.mean(loss_per_fold)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sGaWWHfUtc6D",
        "outputId": "e47cb48b-8d1e-4396-91d4-77456edc3f1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_15 (Conv2D)          (None, 32, 32, 32)        896       \n",
            "                                                                 \n",
            " max_pooling2d_5 (MaxPooling  multiple                 0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_16 (Conv2D)          (None, 16, 16, 64)        18496     \n",
            "                                                                 \n",
            " dropout_5 (Dropout)         (None, 8, 8, 64)          0         \n",
            "                                                                 \n",
            " conv2d_17 (Conv2D)          (None, 8, 8, 128)         73856     \n",
            "                                                                 \n",
            " flatten_5 (Flatten)         (None, 2048)              0         \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 10)                20490     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 113,738\n",
            "Trainable params: 113,738\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 6.0852 - accuracy: 0.0988 - val_loss: 2.3344 - val_accuracy: 0.0987 - lr: 0.1000\n",
            "Epoch 2/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 2.3156 - accuracy: 0.1005 - val_loss: 2.3103 - val_accuracy: 0.0966 - lr: 0.1000\n",
            "Epoch 3/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 2.3158 - accuracy: 0.1013 - val_loss: 2.3089 - val_accuracy: 0.0987 - lr: 0.1000\n",
            "Epoch 4/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 2.3156 - accuracy: 0.0976 - val_loss: 2.3290 - val_accuracy: 0.0994 - lr: 0.1000\n",
            "Epoch 5/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 2.3162 - accuracy: 0.0981 - val_loss: 2.3145 - val_accuracy: 0.1019 - lr: 0.1000\n",
            "Epoch 6/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 2.3149 - accuracy: 0.1009 - val_loss: 2.3180 - val_accuracy: 0.1013 - lr: 0.1000\n",
            "Epoch 7/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 2.3148 - accuracy: 0.0985 - val_loss: 2.3150 - val_accuracy: 0.1007 - lr: 0.1000\n",
            "Epoch 8/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 2.3260 - accuracy: 0.0983 - val_loss: 2.3137 - val_accuracy: 0.1007 - lr: 0.1000\n",
            "Epoch 9/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 2.3152 - accuracy: 0.1000 - val_loss: 2.3156 - val_accuracy: 0.1013 - lr: 0.1000\n",
            "Epoch 10/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 2.3157 - accuracy: 0.0989 - val_loss: 2.3171 - val_accuracy: 0.1006 - lr: 0.1000\n",
            "Epoch 11/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 2.3093 - accuracy: 0.1000 - val_loss: 2.3122 - val_accuracy: 0.1006 - lr: 0.0500\n",
            "Epoch 12/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 2.3092 - accuracy: 0.1008 - val_loss: 2.3092 - val_accuracy: 0.0987 - lr: 0.0500\n",
            "Epoch 13/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 2.3088 - accuracy: 0.0971 - val_loss: 2.3094 - val_accuracy: 0.1019 - lr: 0.0500\n",
            "Epoch 14/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 2.3098 - accuracy: 0.0974 - val_loss: 2.3106 - val_accuracy: 0.0966 - lr: 0.0500\n",
            "Epoch 15/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 2.3094 - accuracy: 0.0992 - val_loss: 2.3114 - val_accuracy: 0.1007 - lr: 0.0500\n",
            "Epoch 16/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 2.3091 - accuracy: 0.1009 - val_loss: 2.3076 - val_accuracy: 0.0966 - lr: 0.0500\n",
            "Epoch 17/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 2.3090 - accuracy: 0.1025 - val_loss: 2.3101 - val_accuracy: 0.0958 - lr: 0.0500\n",
            "Epoch 18/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 2.3095 - accuracy: 0.0981 - val_loss: 2.3073 - val_accuracy: 0.0958 - lr: 0.0500\n",
            "Epoch 19/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 2.3094 - accuracy: 0.1010 - val_loss: 2.3116 - val_accuracy: 0.0987 - lr: 0.0500\n",
            "Epoch 20/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 2.3093 - accuracy: 0.1011 - val_loss: 2.3069 - val_accuracy: 0.0994 - lr: 0.0500\n",
            "Epoch 21/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 2.3044 - accuracy: 0.1033 - val_loss: 2.3030 - val_accuracy: 0.1013 - lr: 0.0125\n",
            "Epoch 22/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 2.3042 - accuracy: 0.0985 - val_loss: 2.3044 - val_accuracy: 0.1006 - lr: 0.0125\n",
            "Epoch 23/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 2.3041 - accuracy: 0.1010 - val_loss: 2.3055 - val_accuracy: 0.1013 - lr: 0.0125\n",
            "Epoch 24/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 2.3043 - accuracy: 0.0990 - val_loss: 2.3046 - val_accuracy: 0.0966 - lr: 0.0125\n",
            "Epoch 25/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 2.3047 - accuracy: 0.0977 - val_loss: 2.3065 - val_accuracy: 0.1007 - lr: 0.0125\n",
            "Epoch 26/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 2.3042 - accuracy: 0.1008 - val_loss: 2.3054 - val_accuracy: 0.0958 - lr: 0.0125\n",
            "Epoch 27/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 2.3044 - accuracy: 0.0976 - val_loss: 2.3044 - val_accuracy: 0.0987 - lr: 0.0125\n",
            "Epoch 28/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 2.3045 - accuracy: 0.1017 - val_loss: 2.3052 - val_accuracy: 0.0966 - lr: 0.0125\n",
            "Epoch 29/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 2.3044 - accuracy: 0.0979 - val_loss: 2.3043 - val_accuracy: 0.0994 - lr: 0.0125\n",
            "Epoch 30/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 2.3042 - accuracy: 0.0996 - val_loss: 2.3045 - val_accuracy: 0.1006 - lr: 0.0125\n",
            "Score for fold 1: loss of 2.3044633865356445; accuracy of 10.05999967455864%\n",
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_18 (Conv2D)          (None, 32, 32, 32)        896       \n",
            "                                                                 \n",
            " max_pooling2d_6 (MaxPooling  multiple                 0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_19 (Conv2D)          (None, 16, 16, 64)        18496     \n",
            "                                                                 \n",
            " dropout_6 (Dropout)         (None, 8, 8, 64)          0         \n",
            "                                                                 \n",
            " conv2d_20 (Conv2D)          (None, 8, 8, 128)         73856     \n",
            "                                                                 \n",
            " flatten_6 (Flatten)         (None, 2048)              0         \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 10)                20490     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 113,738\n",
            "Trainable params: 113,738\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 16.1031 - accuracy: 0.0989 - val_loss: 2.3090 - val_accuracy: 0.1015 - lr: 0.1000\n",
            "Epoch 2/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 2.3148 - accuracy: 0.1004 - val_loss: 2.3158 - val_accuracy: 0.0986 - lr: 0.1000\n",
            "Epoch 3/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 2.3158 - accuracy: 0.0997 - val_loss: 2.3115 - val_accuracy: 0.1015 - lr: 0.1000\n",
            "Epoch 4/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 2.3171 - accuracy: 0.0977 - val_loss: 2.3169 - val_accuracy: 0.0986 - lr: 0.1000\n",
            "Epoch 5/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 2.3152 - accuracy: 0.0987 - val_loss: 2.3105 - val_accuracy: 0.0963 - lr: 0.1000\n",
            "Epoch 6/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 2.3150 - accuracy: 0.1005 - val_loss: 2.3130 - val_accuracy: 0.1015 - lr: 0.1000\n",
            "Epoch 7/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 2.3151 - accuracy: 0.1000 - val_loss: 2.3126 - val_accuracy: 0.1044 - lr: 0.1000\n",
            "Epoch 8/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 2.3151 - accuracy: 0.1012 - val_loss: 2.3192 - val_accuracy: 0.1011 - lr: 0.1000\n",
            "Epoch 9/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 2.3153 - accuracy: 0.1013 - val_loss: 2.3140 - val_accuracy: 0.0963 - lr: 0.1000\n",
            "Epoch 10/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 2.3166 - accuracy: 0.1012 - val_loss: 2.3193 - val_accuracy: 0.0986 - lr: 0.1000\n",
            "Epoch 11/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 2.3095 - accuracy: 0.0987 - val_loss: 2.3066 - val_accuracy: 0.0998 - lr: 0.0500\n",
            "Epoch 12/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 2.3092 - accuracy: 0.0996 - val_loss: 2.3082 - val_accuracy: 0.0998 - lr: 0.0500\n",
            "Epoch 13/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 2.3090 - accuracy: 0.0978 - val_loss: 2.3094 - val_accuracy: 0.0963 - lr: 0.0500\n",
            "Epoch 14/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 2.3092 - accuracy: 0.1004 - val_loss: 2.3086 - val_accuracy: 0.0963 - lr: 0.0500\n",
            "Epoch 15/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 2.3088 - accuracy: 0.0996 - val_loss: 2.3099 - val_accuracy: 0.0998 - lr: 0.0500\n",
            "Epoch 16/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 2.3097 - accuracy: 0.0997 - val_loss: 2.3072 - val_accuracy: 0.0975 - lr: 0.0500\n",
            "Epoch 17/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 2.3104 - accuracy: 0.0962 - val_loss: 2.3067 - val_accuracy: 0.1015 - lr: 0.0500\n",
            "Epoch 18/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 2.3095 - accuracy: 0.0979 - val_loss: 2.3087 - val_accuracy: 0.1011 - lr: 0.0500\n",
            "Epoch 19/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 2.3097 - accuracy: 0.0994 - val_loss: 2.3049 - val_accuracy: 0.1011 - lr: 0.0500\n",
            "Epoch 20/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 2.3091 - accuracy: 0.1009 - val_loss: 2.3092 - val_accuracy: 0.0963 - lr: 0.0500\n",
            "Epoch 21/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 2.3045 - accuracy: 0.0973 - val_loss: 2.3046 - val_accuracy: 0.0966 - lr: 0.0125\n",
            "Epoch 22/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 2.3040 - accuracy: 0.1003 - val_loss: 2.3048 - val_accuracy: 0.0975 - lr: 0.0125\n",
            "Epoch 23/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 2.3043 - accuracy: 0.0985 - val_loss: 2.3039 - val_accuracy: 0.0998 - lr: 0.0125\n",
            "Epoch 24/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 2.3046 - accuracy: 0.0967 - val_loss: 2.3055 - val_accuracy: 0.0963 - lr: 0.0125\n",
            "Epoch 25/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 2.3044 - accuracy: 0.1010 - val_loss: 2.3054 - val_accuracy: 0.0966 - lr: 0.0125\n",
            "Epoch 26/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 2.3043 - accuracy: 0.0980 - val_loss: 2.3032 - val_accuracy: 0.1011 - lr: 0.0125\n",
            "Epoch 27/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 2.3043 - accuracy: 0.1000 - val_loss: 2.3055 - val_accuracy: 0.0998 - lr: 0.0125\n",
            "Epoch 28/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 2.3046 - accuracy: 0.0985 - val_loss: 2.3031 - val_accuracy: 0.1027 - lr: 0.0125\n",
            "Epoch 29/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 2.3043 - accuracy: 0.0993 - val_loss: 2.3051 - val_accuracy: 0.1011 - lr: 0.0125\n",
            "Epoch 30/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 2.3042 - accuracy: 0.1008 - val_loss: 2.3037 - val_accuracy: 0.0975 - lr: 0.0125\n",
            "Score for fold 2: loss of 2.3036904335021973; accuracy of 9.749999642372131%\n",
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_21 (Conv2D)          (None, 32, 32, 32)        896       \n",
            "                                                                 \n",
            " max_pooling2d_7 (MaxPooling  multiple                 0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_22 (Conv2D)          (None, 16, 16, 64)        18496     \n",
            "                                                                 \n",
            " dropout_7 (Dropout)         (None, 8, 8, 64)          0         \n",
            "                                                                 \n",
            " conv2d_23 (Conv2D)          (None, 8, 8, 128)         73856     \n",
            "                                                                 \n",
            " flatten_7 (Flatten)         (None, 2048)              0         \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 10)                20490     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 113,738\n",
            "Trainable params: 113,738\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 7.7657 - accuracy: 0.0967 - val_loss: 2.3074 - val_accuracy: 0.0974 - lr: 0.1000\n",
            "Epoch 2/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 2.3152 - accuracy: 0.1004 - val_loss: 2.3119 - val_accuracy: 0.0985 - lr: 0.1000\n",
            "Epoch 3/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 2.3157 - accuracy: 0.1014 - val_loss: 2.3083 - val_accuracy: 0.1020 - lr: 0.1000\n",
            "Epoch 4/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 2.3151 - accuracy: 0.1005 - val_loss: 2.3126 - val_accuracy: 0.0985 - lr: 0.1000\n",
            "Epoch 5/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 2.3239 - accuracy: 0.0978 - val_loss: 2.3161 - val_accuracy: 0.0974 - lr: 0.1000\n",
            "Epoch 6/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 2.3155 - accuracy: 0.1007 - val_loss: 2.3092 - val_accuracy: 0.0965 - lr: 0.1000\n",
            "Epoch 7/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 2.3161 - accuracy: 0.1009 - val_loss: 2.3143 - val_accuracy: 0.1020 - lr: 0.1000\n",
            "Epoch 8/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 2.3158 - accuracy: 0.0987 - val_loss: 2.3129 - val_accuracy: 0.1020 - lr: 0.1000\n",
            "Epoch 9/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 2.3145 - accuracy: 0.0985 - val_loss: 2.3184 - val_accuracy: 0.1024 - lr: 0.1000\n",
            "Epoch 10/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 2.3159 - accuracy: 0.0987 - val_loss: 2.3172 - val_accuracy: 0.0990 - lr: 0.1000\n",
            "Epoch 11/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 2.3099 - accuracy: 0.1010 - val_loss: 2.3157 - val_accuracy: 0.0985 - lr: 0.0500\n",
            "Epoch 12/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 2.3092 - accuracy: 0.0997 - val_loss: 2.3057 - val_accuracy: 0.0965 - lr: 0.0500\n",
            "Epoch 13/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 2.3091 - accuracy: 0.0995 - val_loss: 2.3149 - val_accuracy: 0.1024 - lr: 0.0500\n",
            "Epoch 14/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 2.3096 - accuracy: 0.0980 - val_loss: 2.3053 - val_accuracy: 0.0974 - lr: 0.0500\n",
            "Epoch 15/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 2.3090 - accuracy: 0.0976 - val_loss: 2.3076 - val_accuracy: 0.0965 - lr: 0.0500\n",
            "Epoch 16/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 2.3091 - accuracy: 0.1011 - val_loss: 2.3116 - val_accuracy: 0.1005 - lr: 0.0500\n",
            "Epoch 17/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 2.3094 - accuracy: 0.0984 - val_loss: 2.3141 - val_accuracy: 0.0985 - lr: 0.0500\n",
            "Epoch 18/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 2.3091 - accuracy: 0.1000 - val_loss: 2.3128 - val_accuracy: 0.0985 - lr: 0.0500\n",
            "Epoch 19/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 2.3094 - accuracy: 0.1016 - val_loss: 2.3086 - val_accuracy: 0.1020 - lr: 0.0500\n",
            "Epoch 20/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 2.3095 - accuracy: 0.1020 - val_loss: 2.3135 - val_accuracy: 0.0991 - lr: 0.0500\n",
            "Epoch 21/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 2.3046 - accuracy: 0.1004 - val_loss: 2.3036 - val_accuracy: 0.0985 - lr: 0.0125\n",
            "Epoch 22/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 2.3043 - accuracy: 0.0989 - val_loss: 2.3036 - val_accuracy: 0.0974 - lr: 0.0125\n",
            "Epoch 23/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 2.3040 - accuracy: 0.1025 - val_loss: 2.3048 - val_accuracy: 0.1005 - lr: 0.0125\n",
            "Epoch 24/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 2.3043 - accuracy: 0.1006 - val_loss: 2.3034 - val_accuracy: 0.0965 - lr: 0.0125\n",
            "Epoch 25/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 2.3045 - accuracy: 0.0990 - val_loss: 2.3030 - val_accuracy: 0.0991 - lr: 0.0125\n",
            "Epoch 26/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 2.3042 - accuracy: 0.1016 - val_loss: 2.3052 - val_accuracy: 0.0965 - lr: 0.0125\n",
            "Epoch 27/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 2.3043 - accuracy: 0.0972 - val_loss: 2.3037 - val_accuracy: 0.1005 - lr: 0.0125\n",
            "Epoch 28/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 2.3042 - accuracy: 0.1002 - val_loss: 2.3037 - val_accuracy: 0.0991 - lr: 0.0125\n",
            "Epoch 29/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 2.3042 - accuracy: 0.1020 - val_loss: 2.3039 - val_accuracy: 0.0965 - lr: 0.0125\n",
            "Epoch 30/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 2.3043 - accuracy: 0.1013 - val_loss: 2.3029 - val_accuracy: 0.1061 - lr: 0.0125\n",
            "Score for fold 3: loss of 2.302896738052368; accuracy of 10.610000044107437%\n",
            "Model: \"sequential_8\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_24 (Conv2D)          (None, 32, 32, 32)        896       \n",
            "                                                                 \n",
            " max_pooling2d_8 (MaxPooling  multiple                 0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_25 (Conv2D)          (None, 16, 16, 64)        18496     \n",
            "                                                                 \n",
            " dropout_8 (Dropout)         (None, 8, 8, 64)          0         \n",
            "                                                                 \n",
            " conv2d_26 (Conv2D)          (None, 8, 8, 128)         73856     \n",
            "                                                                 \n",
            " flatten_8 (Flatten)         (None, 2048)              0         \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 10)                20490     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 113,738\n",
            "Trainable params: 113,738\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 6.9592 - accuracy: 0.1004 - val_loss: 2.3233 - val_accuracy: 0.0974 - lr: 0.1000\n",
            "Epoch 2/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 2.3143 - accuracy: 0.0989 - val_loss: 2.3134 - val_accuracy: 0.0954 - lr: 0.1000\n",
            "Epoch 3/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 2.3140 - accuracy: 0.1011 - val_loss: 2.3105 - val_accuracy: 0.1018 - lr: 0.1000\n",
            "Epoch 4/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 2.3152 - accuracy: 0.1010 - val_loss: 2.3196 - val_accuracy: 0.0986 - lr: 0.1000\n",
            "Epoch 5/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 2.3148 - accuracy: 0.1019 - val_loss: 2.3148 - val_accuracy: 0.1020 - lr: 0.1000\n",
            "Epoch 6/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 2.3158 - accuracy: 0.0995 - val_loss: 2.3186 - val_accuracy: 0.1018 - lr: 0.1000\n",
            "Epoch 7/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 2.3157 - accuracy: 0.0988 - val_loss: 2.3132 - val_accuracy: 0.1030 - lr: 0.1000\n",
            "Epoch 8/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 2.3205 - accuracy: 0.0993 - val_loss: 2.3142 - val_accuracy: 0.0974 - lr: 0.1000\n",
            "Epoch 9/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 2.3165 - accuracy: 0.0988 - val_loss: 2.3131 - val_accuracy: 0.0974 - lr: 0.1000\n",
            "Epoch 10/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 2.3152 - accuracy: 0.0999 - val_loss: 2.3097 - val_accuracy: 0.0967 - lr: 0.1000\n",
            "Epoch 11/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 2.3101 - accuracy: 0.0968 - val_loss: 2.3129 - val_accuracy: 0.0967 - lr: 0.0500\n",
            "Epoch 12/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 2.3092 - accuracy: 0.0993 - val_loss: 2.3103 - val_accuracy: 0.0967 - lr: 0.0500\n",
            "Epoch 13/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 2.3091 - accuracy: 0.1010 - val_loss: 2.3086 - val_accuracy: 0.1025 - lr: 0.0500\n",
            "Epoch 14/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 2.3096 - accuracy: 0.0983 - val_loss: 2.3120 - val_accuracy: 0.0967 - lr: 0.0500\n",
            "Epoch 15/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 2.3095 - accuracy: 0.0993 - val_loss: 2.3067 - val_accuracy: 0.0967 - lr: 0.0500\n",
            "Epoch 16/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 2.3091 - accuracy: 0.1027 - val_loss: 2.3090 - val_accuracy: 0.1018 - lr: 0.0500\n",
            "Epoch 17/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 2.3091 - accuracy: 0.1004 - val_loss: 2.3082 - val_accuracy: 0.1030 - lr: 0.0500\n",
            "Epoch 18/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 2.3093 - accuracy: 0.0990 - val_loss: 2.3214 - val_accuracy: 0.0967 - lr: 0.0500\n",
            "Epoch 19/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 2.3090 - accuracy: 0.1018 - val_loss: 2.3103 - val_accuracy: 0.0993 - lr: 0.0500\n",
            "Epoch 20/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 2.3088 - accuracy: 0.1003 - val_loss: 2.3071 - val_accuracy: 0.0974 - lr: 0.0500\n",
            "Epoch 21/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 2.3043 - accuracy: 0.0987 - val_loss: 2.3047 - val_accuracy: 0.0993 - lr: 0.0125\n",
            "Epoch 22/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 2.3047 - accuracy: 0.0996 - val_loss: 2.3027 - val_accuracy: 0.1033 - lr: 0.0125\n",
            "Epoch 23/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 2.3041 - accuracy: 0.0988 - val_loss: 2.3044 - val_accuracy: 0.1030 - lr: 0.0125\n",
            "Epoch 24/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 2.3042 - accuracy: 0.1029 - val_loss: 2.3037 - val_accuracy: 0.0967 - lr: 0.0125\n",
            "Epoch 25/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 2.3042 - accuracy: 0.0994 - val_loss: 2.3066 - val_accuracy: 0.0974 - lr: 0.0125\n",
            "Epoch 26/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 2.3044 - accuracy: 0.1000 - val_loss: 2.3038 - val_accuracy: 0.0967 - lr: 0.0125\n",
            "Epoch 27/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 2.3041 - accuracy: 0.1015 - val_loss: 2.3056 - val_accuracy: 0.0967 - lr: 0.0125\n",
            "Epoch 28/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 2.3044 - accuracy: 0.0996 - val_loss: 2.3048 - val_accuracy: 0.0954 - lr: 0.0125\n",
            "Epoch 29/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 2.3045 - accuracy: 0.0978 - val_loss: 2.3041 - val_accuracy: 0.1025 - lr: 0.0125\n",
            "Epoch 30/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 2.3045 - accuracy: 0.0971 - val_loss: 2.3036 - val_accuracy: 0.1025 - lr: 0.0125\n",
            "Score for fold 4: loss of 2.3036134243011475; accuracy of 10.249999910593033%\n",
            "Model: \"sequential_9\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_27 (Conv2D)          (None, 32, 32, 32)        896       \n",
            "                                                                 \n",
            " max_pooling2d_9 (MaxPooling  multiple                 0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_28 (Conv2D)          (None, 16, 16, 64)        18496     \n",
            "                                                                 \n",
            " dropout_9 (Dropout)         (None, 8, 8, 64)          0         \n",
            "                                                                 \n",
            " conv2d_29 (Conv2D)          (None, 8, 8, 128)         73856     \n",
            "                                                                 \n",
            " flatten_9 (Flatten)         (None, 2048)              0         \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 10)                20490     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 113,738\n",
            "Trainable params: 113,738\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 9.5958 - accuracy: 0.0981 - val_loss: 2.3108 - val_accuracy: 0.0981 - lr: 0.1000\n",
            "Epoch 2/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 2.3158 - accuracy: 0.1011 - val_loss: 2.3141 - val_accuracy: 0.0981 - lr: 0.1000\n",
            "Epoch 3/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 2.3166 - accuracy: 0.0981 - val_loss: 2.3116 - val_accuracy: 0.0978 - lr: 0.1000\n",
            "Epoch 4/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 2.3150 - accuracy: 0.0995 - val_loss: 2.3125 - val_accuracy: 0.1015 - lr: 0.1000\n",
            "Epoch 5/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 2.3158 - accuracy: 0.1002 - val_loss: 2.3221 - val_accuracy: 0.0978 - lr: 0.1000\n",
            "Epoch 6/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 2.3156 - accuracy: 0.0996 - val_loss: 2.3087 - val_accuracy: 0.0970 - lr: 0.1000\n",
            "Epoch 7/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 2.3156 - accuracy: 0.0993 - val_loss: 2.3208 - val_accuracy: 0.0992 - lr: 0.1000\n",
            "Epoch 8/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 2.3156 - accuracy: 0.0998 - val_loss: 2.3150 - val_accuracy: 0.1020 - lr: 0.1000\n",
            "Epoch 9/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 2.3153 - accuracy: 0.0995 - val_loss: 2.3233 - val_accuracy: 0.0992 - lr: 0.1000\n",
            "Epoch 10/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 2.3157 - accuracy: 0.0996 - val_loss: 2.3130 - val_accuracy: 0.0978 - lr: 0.1000\n",
            "Epoch 11/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 2.3089 - accuracy: 0.1016 - val_loss: 2.3035 - val_accuracy: 0.1026 - lr: 0.0500\n",
            "Epoch 12/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 2.3088 - accuracy: 0.1031 - val_loss: 2.3093 - val_accuracy: 0.1015 - lr: 0.0500\n",
            "Epoch 13/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 2.3086 - accuracy: 0.1009 - val_loss: 2.3090 - val_accuracy: 0.0981 - lr: 0.0500\n",
            "Epoch 14/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 2.3087 - accuracy: 0.0998 - val_loss: 2.3079 - val_accuracy: 0.1029 - lr: 0.0500\n",
            "Epoch 15/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 2.3083 - accuracy: 0.1027 - val_loss: 2.3052 - val_accuracy: 0.1015 - lr: 0.0500\n",
            "Epoch 16/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 2.3092 - accuracy: 0.1012 - val_loss: 2.3139 - val_accuracy: 0.1007 - lr: 0.0500\n",
            "Epoch 17/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 2.3091 - accuracy: 0.0978 - val_loss: 2.3072 - val_accuracy: 0.0970 - lr: 0.0500\n",
            "Epoch 18/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 2.3091 - accuracy: 0.1024 - val_loss: 2.3138 - val_accuracy: 0.1007 - lr: 0.0500\n",
            "Epoch 19/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 2.3085 - accuracy: 0.1024 - val_loss: 2.3137 - val_accuracy: 0.1029 - lr: 0.0500\n",
            "Epoch 20/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 2.3090 - accuracy: 0.1006 - val_loss: 2.3112 - val_accuracy: 0.0978 - lr: 0.0500\n",
            "Epoch 21/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 2.3045 - accuracy: 0.0983 - val_loss: 2.3040 - val_accuracy: 0.0978 - lr: 0.0125\n",
            "Epoch 22/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 2.3043 - accuracy: 0.1004 - val_loss: 2.3043 - val_accuracy: 0.0982 - lr: 0.0125\n",
            "Epoch 23/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 2.3044 - accuracy: 0.0975 - val_loss: 2.3054 - val_accuracy: 0.0981 - lr: 0.0125\n",
            "Epoch 24/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 2.3044 - accuracy: 0.0981 - val_loss: 2.3036 - val_accuracy: 0.0982 - lr: 0.0125\n",
            "Epoch 25/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 2.3044 - accuracy: 0.0992 - val_loss: 2.3046 - val_accuracy: 0.0982 - lr: 0.0125\n",
            "Epoch 26/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 2.3044 - accuracy: 0.0988 - val_loss: 2.3064 - val_accuracy: 0.0978 - lr: 0.0125\n",
            "Epoch 27/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 2.3043 - accuracy: 0.0984 - val_loss: 2.3039 - val_accuracy: 0.0982 - lr: 0.0125\n",
            "Epoch 28/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 2.3043 - accuracy: 0.0985 - val_loss: 2.3067 - val_accuracy: 0.1020 - lr: 0.0125\n",
            "Epoch 29/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 2.3044 - accuracy: 0.0993 - val_loss: 2.3045 - val_accuracy: 0.1026 - lr: 0.0125\n",
            "Epoch 30/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 2.3043 - accuracy: 0.0990 - val_loss: 2.3036 - val_accuracy: 0.1015 - lr: 0.0125\n",
            "Score for fold 5: loss of 2.303586483001709; accuracy of 10.14999970793724%\n",
            "------------------------------------------------------------------------\n",
            "Score per fold\n",
            "------------------------------------------------------------------------\n",
            "> Fold 1 - Loss: 2.3044633865356445 - Accuracy: 10.05999967455864%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 2 - Loss: 2.3036904335021973 - Accuracy: 9.749999642372131%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 3 - Loss: 2.302896738052368 - Accuracy: 10.610000044107437%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 4 - Loss: 2.3036134243011475 - Accuracy: 10.249999910593033%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 5 - Loss: 2.303586483001709 - Accuracy: 10.14999970793724%\n",
            "------------------------------------------------------------------------\n",
            "Average scores for all folds:\n",
            "> Accuracy: 10.163999795913696 (+- 0.2788262526920275)\n",
            "> Loss: 2.3036500930786135\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Learning_rate=1e-3 & Exponential decay"
      ],
      "metadata": {
        "id": "34vAr34A_Sh9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# K-fold Cross Validation model evaluation\n",
        "acc_per_fold = []\n",
        "loss_per_fold = []\n",
        "\n",
        "with tf.device('/device:GPU:0'):\n",
        "  fold_no = 1\n",
        "  for train, valid in kfold.split(trainX, trainy):\n",
        "\n",
        "    model = CNN_Model(batch_size=32, depth=3, hidden_neurons_1=32, hidden_neurons_2=64, hidden_neurons_3=128, kernel_size_1=3, kernel_size_2=3, kernel_size_3=3, \n",
        "               dropout_rate=0.2, batch_norm=False, pooling='max', layer_activation_1='relu', layer_activation_2='relu', layer_activation_3='relu',\n",
        "               kernel_initialiser='glorot_uniform', bias_initialiser='zeros', optimiser='Adam', weight_decay=None, learning_rate=1e-3, output_activation='softmax', SGD_momentum=0)\n",
        "  \n",
        "    train_gen = tf.keras.preprocessing.image.ImageDataGenerator(width_shift_range=0.1,\n",
        "      height_shift_range=0.1,\n",
        "      horizontal_flip=True)\n",
        "    train_gen.fit(trainX[train])  #Data Augmentation\n",
        "  \n",
        "    print('------------------------------------------------------------------------')\n",
        "    print(f'Training for fold {fold_no} ...')\n",
        "\n",
        "    history = model.train(\n",
        "              train_gen,\n",
        "                trainX[train],\n",
        "                  trainy[train],\n",
        "                trainX[valid],\n",
        "                  trainy[valid])\n",
        "\n",
        "    scores = model.evaluate(trainX[valid], trainy[valid])\n",
        "\n",
        "    print(f'Score for fold {fold_no}: loss of {scores[0]}; accuracy of {scores[1]*100}%')\n",
        "    acc_per_fold.append(scores[1] * 100)\n",
        "    loss_per_fold.append(scores[0])\n",
        "\n",
        "    fold_no+=1\n",
        "\n",
        "# == Provide average scores ==\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Score per fold')\n",
        "for i in range(0, len(acc_per_fold)):\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Average scores for all folds:')\n",
        "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
        "print(f'> Loss: {np.mean(loss_per_fold)}')"
      ],
      "metadata": {
        "id": "i14jRNki_V5N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0b28a12-fc8b-4550-bfa2-78ec4327ecd4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_10\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_30 (Conv2D)          (None, 32, 32, 32)        896       \n",
            "                                                                 \n",
            " max_pooling2d_10 (MaxPoolin  multiple                 0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_31 (Conv2D)          (None, 16, 16, 64)        18496     \n",
            "                                                                 \n",
            " dropout_10 (Dropout)        (None, 8, 8, 64)          0         \n",
            "                                                                 \n",
            " conv2d_32 (Conv2D)          (None, 8, 8, 128)         73856     \n",
            "                                                                 \n",
            " flatten_10 (Flatten)        (None, 2048)              0         \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 10)                20490     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 113,738\n",
            "Trainable params: 113,738\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.5753 - accuracy: 0.4300 - val_loss: 1.2089 - val_accuracy: 0.5801 - lr: 0.0010\n",
            "Epoch 2/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.2167 - accuracy: 0.5698 - val_loss: 1.0675 - val_accuracy: 0.6220 - lr: 0.0010\n",
            "Epoch 3/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 1.0684 - accuracy: 0.6264 - val_loss: 0.9139 - val_accuracy: 0.6886 - lr: 0.0010\n",
            "Epoch 4/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.9929 - accuracy: 0.6561 - val_loss: 0.8506 - val_accuracy: 0.7140 - lr: 0.0010\n",
            "Epoch 5/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.9334 - accuracy: 0.6769 - val_loss: 0.8191 - val_accuracy: 0.7179 - lr: 0.0010\n",
            "Epoch 6/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.8917 - accuracy: 0.6919 - val_loss: 0.8414 - val_accuracy: 0.7126 - lr: 0.0010\n",
            "Epoch 7/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 0.8632 - accuracy: 0.6988 - val_loss: 0.8247 - val_accuracy: 0.7145 - lr: 0.0010\n",
            "Epoch 8/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.8325 - accuracy: 0.7114 - val_loss: 0.8117 - val_accuracy: 0.7269 - lr: 0.0010\n",
            "Epoch 9/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.8091 - accuracy: 0.7212 - val_loss: 0.7313 - val_accuracy: 0.7530 - lr: 0.0010\n",
            "Epoch 10/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.7829 - accuracy: 0.7269 - val_loss: 0.7794 - val_accuracy: 0.7341 - lr: 0.0010\n",
            "Epoch 11/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.7695 - accuracy: 0.7335 - val_loss: 0.7446 - val_accuracy: 0.7475 - lr: 0.0010\n",
            "Epoch 12/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.7539 - accuracy: 0.7391 - val_loss: 0.7344 - val_accuracy: 0.7520 - lr: 0.0010\n",
            "Epoch 13/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.7430 - accuracy: 0.7411 - val_loss: 0.7028 - val_accuracy: 0.7642 - lr: 0.0010\n",
            "Epoch 14/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.7287 - accuracy: 0.7484 - val_loss: 0.7032 - val_accuracy: 0.7633 - lr: 0.0010\n",
            "Epoch 15/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.7248 - accuracy: 0.7479 - val_loss: 0.7054 - val_accuracy: 0.7614 - lr: 0.0010\n",
            "Epoch 16/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.6980 - accuracy: 0.7575 - val_loss: 0.7204 - val_accuracy: 0.7569 - lr: 9.0484e-04\n",
            "Epoch 17/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.6737 - accuracy: 0.7643 - val_loss: 0.6667 - val_accuracy: 0.7734 - lr: 8.1873e-04\n",
            "Epoch 18/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.6587 - accuracy: 0.7715 - val_loss: 0.6608 - val_accuracy: 0.7762 - lr: 7.4082e-04\n",
            "Epoch 19/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.6415 - accuracy: 0.7778 - val_loss: 0.6495 - val_accuracy: 0.7797 - lr: 6.7032e-04\n",
            "Epoch 20/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.6251 - accuracy: 0.7816 - val_loss: 0.6468 - val_accuracy: 0.7788 - lr: 6.0653e-04\n",
            "Epoch 21/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.6158 - accuracy: 0.7882 - val_loss: 0.6206 - val_accuracy: 0.7888 - lr: 5.4881e-04\n",
            "Epoch 22/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.6005 - accuracy: 0.7916 - val_loss: 0.6318 - val_accuracy: 0.7869 - lr: 4.9659e-04\n",
            "Epoch 23/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.5804 - accuracy: 0.7965 - val_loss: 0.6613 - val_accuracy: 0.7836 - lr: 4.4933e-04\n",
            "Epoch 24/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.5756 - accuracy: 0.7998 - val_loss: 0.6139 - val_accuracy: 0.7947 - lr: 4.0657e-04\n",
            "Epoch 25/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.5625 - accuracy: 0.8049 - val_loss: 0.6240 - val_accuracy: 0.7908 - lr: 3.6788e-04\n",
            "Epoch 26/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.5574 - accuracy: 0.8052 - val_loss: 0.5998 - val_accuracy: 0.8017 - lr: 3.3287e-04\n",
            "Epoch 27/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.5446 - accuracy: 0.8098 - val_loss: 0.6199 - val_accuracy: 0.7924 - lr: 3.0119e-04\n",
            "Epoch 28/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.5393 - accuracy: 0.8143 - val_loss: 0.6033 - val_accuracy: 0.8020 - lr: 2.7253e-04\n",
            "Epoch 29/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.5289 - accuracy: 0.8163 - val_loss: 0.5892 - val_accuracy: 0.7988 - lr: 2.4660e-04\n",
            "Epoch 30/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.5296 - accuracy: 0.8166 - val_loss: 0.5939 - val_accuracy: 0.8043 - lr: 2.2313e-04\n",
            "Score for fold 1: loss of 0.5938509702682495; accuracy of 80.43000102043152%\n",
            "Model: \"sequential_11\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_33 (Conv2D)          (None, 32, 32, 32)        896       \n",
            "                                                                 \n",
            " max_pooling2d_11 (MaxPoolin  multiple                 0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_34 (Conv2D)          (None, 16, 16, 64)        18496     \n",
            "                                                                 \n",
            " dropout_11 (Dropout)        (None, 8, 8, 64)          0         \n",
            "                                                                 \n",
            " conv2d_35 (Conv2D)          (None, 8, 8, 128)         73856     \n",
            "                                                                 \n",
            " flatten_11 (Flatten)        (None, 2048)              0         \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 10)                20490     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 113,738\n",
            "Trainable params: 113,738\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.5822 - accuracy: 0.4249 - val_loss: 1.2803 - val_accuracy: 0.5461 - lr: 0.0010\n",
            "Epoch 2/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.2307 - accuracy: 0.5657 - val_loss: 1.0660 - val_accuracy: 0.6259 - lr: 0.0010\n",
            "Epoch 3/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.0866 - accuracy: 0.6180 - val_loss: 0.9532 - val_accuracy: 0.6688 - lr: 0.0010\n",
            "Epoch 4/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.0022 - accuracy: 0.6518 - val_loss: 0.8667 - val_accuracy: 0.7023 - lr: 0.0010\n",
            "Epoch 5/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.9486 - accuracy: 0.6714 - val_loss: 0.8662 - val_accuracy: 0.7025 - lr: 0.0010\n",
            "Epoch 6/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.9092 - accuracy: 0.6838 - val_loss: 0.8150 - val_accuracy: 0.7217 - lr: 0.0010\n",
            "Epoch 7/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.8681 - accuracy: 0.6993 - val_loss: 0.8415 - val_accuracy: 0.7112 - lr: 0.0010\n",
            "Epoch 8/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.8404 - accuracy: 0.7080 - val_loss: 0.8142 - val_accuracy: 0.7161 - lr: 0.0010\n",
            "Epoch 9/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.8209 - accuracy: 0.7149 - val_loss: 0.7966 - val_accuracy: 0.7285 - lr: 0.0010\n",
            "Epoch 10/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.8043 - accuracy: 0.7209 - val_loss: 0.8575 - val_accuracy: 0.7114 - lr: 0.0010\n",
            "Epoch 11/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.7865 - accuracy: 0.7253 - val_loss: 0.7650 - val_accuracy: 0.7389 - lr: 0.0010\n",
            "Epoch 12/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.7703 - accuracy: 0.7321 - val_loss: 0.7650 - val_accuracy: 0.7424 - lr: 0.0010\n",
            "Epoch 13/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.7522 - accuracy: 0.7378 - val_loss: 0.8073 - val_accuracy: 0.7312 - lr: 0.0010\n",
            "Epoch 14/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.7453 - accuracy: 0.7415 - val_loss: 0.7334 - val_accuracy: 0.7505 - lr: 0.0010\n",
            "Epoch 15/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.7318 - accuracy: 0.7473 - val_loss: 0.7382 - val_accuracy: 0.7592 - lr: 0.0010\n",
            "Epoch 16/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.7052 - accuracy: 0.7540 - val_loss: 0.7151 - val_accuracy: 0.7616 - lr: 9.0484e-04\n",
            "Epoch 17/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.6958 - accuracy: 0.7606 - val_loss: 0.6988 - val_accuracy: 0.7633 - lr: 8.1873e-04\n",
            "Epoch 18/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.6615 - accuracy: 0.7706 - val_loss: 0.7273 - val_accuracy: 0.7584 - lr: 7.4082e-04\n",
            "Epoch 19/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.6565 - accuracy: 0.7714 - val_loss: 0.6686 - val_accuracy: 0.7724 - lr: 6.7032e-04\n",
            "Epoch 20/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.6396 - accuracy: 0.7765 - val_loss: 0.6757 - val_accuracy: 0.7707 - lr: 6.0653e-04\n",
            "Epoch 21/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.6225 - accuracy: 0.7823 - val_loss: 0.6671 - val_accuracy: 0.7745 - lr: 5.4881e-04\n",
            "Epoch 22/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.6041 - accuracy: 0.7903 - val_loss: 0.6396 - val_accuracy: 0.7879 - lr: 4.9659e-04\n",
            "Epoch 23/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.5946 - accuracy: 0.7944 - val_loss: 0.6672 - val_accuracy: 0.7820 - lr: 4.4933e-04\n",
            "Epoch 24/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.5830 - accuracy: 0.7965 - val_loss: 0.6270 - val_accuracy: 0.7888 - lr: 4.0657e-04\n",
            "Epoch 25/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.5728 - accuracy: 0.8002 - val_loss: 0.6246 - val_accuracy: 0.7915 - lr: 3.6788e-04\n",
            "Epoch 26/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.5629 - accuracy: 0.8042 - val_loss: 0.6433 - val_accuracy: 0.7837 - lr: 3.3287e-04\n",
            "Epoch 27/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.5627 - accuracy: 0.8030 - val_loss: 0.6387 - val_accuracy: 0.7909 - lr: 3.0119e-04\n",
            "Epoch 28/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.5501 - accuracy: 0.8095 - val_loss: 0.6147 - val_accuracy: 0.7943 - lr: 2.7253e-04\n",
            "Epoch 29/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.5450 - accuracy: 0.8112 - val_loss: 0.6155 - val_accuracy: 0.7937 - lr: 2.4660e-04\n",
            "Epoch 30/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.5331 - accuracy: 0.8122 - val_loss: 0.6154 - val_accuracy: 0.7946 - lr: 2.2313e-04\n",
            "Score for fold 2: loss of 0.615408718585968; accuracy of 79.46000099182129%\n",
            "Model: \"sequential_12\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_36 (Conv2D)          (None, 32, 32, 32)        896       \n",
            "                                                                 \n",
            " max_pooling2d_12 (MaxPoolin  multiple                 0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_37 (Conv2D)          (None, 16, 16, 64)        18496     \n",
            "                                                                 \n",
            " dropout_12 (Dropout)        (None, 8, 8, 64)          0         \n",
            "                                                                 \n",
            " conv2d_38 (Conv2D)          (None, 8, 8, 128)         73856     \n",
            "                                                                 \n",
            " flatten_12 (Flatten)        (None, 2048)              0         \n",
            "                                                                 \n",
            " dense_12 (Dense)            (None, 10)                20490     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 113,738\n",
            "Trainable params: 113,738\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.5874 - accuracy: 0.4241 - val_loss: 1.2656 - val_accuracy: 0.5627 - lr: 0.0010\n",
            "Epoch 2/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.2221 - accuracy: 0.5681 - val_loss: 1.0523 - val_accuracy: 0.6253 - lr: 0.0010\n",
            "Epoch 3/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.0871 - accuracy: 0.6197 - val_loss: 0.9707 - val_accuracy: 0.6642 - lr: 0.0010\n",
            "Epoch 4/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 1.0072 - accuracy: 0.6505 - val_loss: 0.8792 - val_accuracy: 0.6950 - lr: 0.0010\n",
            "Epoch 5/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.9450 - accuracy: 0.6701 - val_loss: 0.8819 - val_accuracy: 0.6985 - lr: 0.0010\n",
            "Epoch 6/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.9055 - accuracy: 0.6846 - val_loss: 0.8733 - val_accuracy: 0.6924 - lr: 0.0010\n",
            "Epoch 7/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.8671 - accuracy: 0.6977 - val_loss: 0.8088 - val_accuracy: 0.7186 - lr: 0.0010\n",
            "Epoch 8/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.8453 - accuracy: 0.7056 - val_loss: 0.8141 - val_accuracy: 0.7245 - lr: 0.0010\n",
            "Epoch 9/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.8184 - accuracy: 0.7162 - val_loss: 0.7763 - val_accuracy: 0.7357 - lr: 0.0010\n",
            "Epoch 10/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.7981 - accuracy: 0.7235 - val_loss: 0.7465 - val_accuracy: 0.7435 - lr: 0.0010\n",
            "Epoch 11/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.7839 - accuracy: 0.7289 - val_loss: 0.7497 - val_accuracy: 0.7398 - lr: 0.0010\n",
            "Epoch 12/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.7662 - accuracy: 0.7339 - val_loss: 0.7349 - val_accuracy: 0.7462 - lr: 0.0010\n",
            "Epoch 13/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.7566 - accuracy: 0.7401 - val_loss: 0.7207 - val_accuracy: 0.7535 - lr: 0.0010\n",
            "Epoch 14/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.7437 - accuracy: 0.7417 - val_loss: 0.7191 - val_accuracy: 0.7502 - lr: 0.0010\n",
            "Epoch 15/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.7308 - accuracy: 0.7454 - val_loss: 0.7400 - val_accuracy: 0.7451 - lr: 0.0010\n",
            "Epoch 16/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.7085 - accuracy: 0.7526 - val_loss: 0.6886 - val_accuracy: 0.7682 - lr: 9.0484e-04\n",
            "Epoch 17/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.6866 - accuracy: 0.7601 - val_loss: 0.6879 - val_accuracy: 0.7613 - lr: 8.1873e-04\n",
            "Epoch 18/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.6715 - accuracy: 0.7657 - val_loss: 0.6606 - val_accuracy: 0.7732 - lr: 7.4082e-04\n",
            "Epoch 19/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.6611 - accuracy: 0.7701 - val_loss: 0.6596 - val_accuracy: 0.7742 - lr: 6.7032e-04\n",
            "Epoch 20/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.6471 - accuracy: 0.7756 - val_loss: 0.6319 - val_accuracy: 0.7858 - lr: 6.0653e-04\n",
            "Epoch 21/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.6301 - accuracy: 0.7805 - val_loss: 0.6410 - val_accuracy: 0.7832 - lr: 5.4881e-04\n",
            "Epoch 22/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.6210 - accuracy: 0.7851 - val_loss: 0.6275 - val_accuracy: 0.7865 - lr: 4.9659e-04\n",
            "Epoch 23/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.5975 - accuracy: 0.7911 - val_loss: 0.6158 - val_accuracy: 0.7889 - lr: 4.4933e-04\n",
            "Epoch 24/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.5920 - accuracy: 0.7952 - val_loss: 0.6087 - val_accuracy: 0.7945 - lr: 4.0657e-04\n",
            "Epoch 25/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.5839 - accuracy: 0.7967 - val_loss: 0.6299 - val_accuracy: 0.7856 - lr: 3.6788e-04\n",
            "Epoch 26/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.5789 - accuracy: 0.7982 - val_loss: 0.6042 - val_accuracy: 0.7957 - lr: 3.3287e-04\n",
            "Epoch 27/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.5681 - accuracy: 0.8028 - val_loss: 0.5835 - val_accuracy: 0.8007 - lr: 3.0119e-04\n",
            "Epoch 28/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.5614 - accuracy: 0.8045 - val_loss: 0.6195 - val_accuracy: 0.7882 - lr: 2.7253e-04\n",
            "Epoch 29/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.5536 - accuracy: 0.8066 - val_loss: 0.6281 - val_accuracy: 0.7880 - lr: 2.4660e-04\n",
            "Epoch 30/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.5471 - accuracy: 0.8094 - val_loss: 0.6190 - val_accuracy: 0.7919 - lr: 2.2313e-04\n",
            "Score for fold 3: loss of 0.6190310716629028; accuracy of 79.18999791145325%\n",
            "Model: \"sequential_13\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_39 (Conv2D)          (None, 32, 32, 32)        896       \n",
            "                                                                 \n",
            " max_pooling2d_13 (MaxPoolin  multiple                 0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_40 (Conv2D)          (None, 16, 16, 64)        18496     \n",
            "                                                                 \n",
            " dropout_13 (Dropout)        (None, 8, 8, 64)          0         \n",
            "                                                                 \n",
            " conv2d_41 (Conv2D)          (None, 8, 8, 128)         73856     \n",
            "                                                                 \n",
            " flatten_13 (Flatten)        (None, 2048)              0         \n",
            "                                                                 \n",
            " dense_13 (Dense)            (None, 10)                20490     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 113,738\n",
            "Trainable params: 113,738\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.5799 - accuracy: 0.4266 - val_loss: 1.2059 - val_accuracy: 0.5750 - lr: 0.0010\n",
            "Epoch 2/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.2284 - accuracy: 0.5636 - val_loss: 1.0493 - val_accuracy: 0.6374 - lr: 0.0010\n",
            "Epoch 3/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.0886 - accuracy: 0.6150 - val_loss: 0.9478 - val_accuracy: 0.6666 - lr: 0.0010\n",
            "Epoch 4/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.0051 - accuracy: 0.6474 - val_loss: 0.8928 - val_accuracy: 0.6887 - lr: 0.0010\n",
            "Epoch 5/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.9417 - accuracy: 0.6712 - val_loss: 0.8549 - val_accuracy: 0.7063 - lr: 0.0010\n",
            "Epoch 6/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.9082 - accuracy: 0.6839 - val_loss: 0.8534 - val_accuracy: 0.7142 - lr: 0.0010\n",
            "Epoch 7/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.8662 - accuracy: 0.6971 - val_loss: 0.8377 - val_accuracy: 0.7161 - lr: 0.0010\n",
            "Epoch 8/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.8431 - accuracy: 0.7091 - val_loss: 0.8080 - val_accuracy: 0.7199 - lr: 0.0010\n",
            "Epoch 9/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.8159 - accuracy: 0.7173 - val_loss: 0.7631 - val_accuracy: 0.7360 - lr: 0.0010\n",
            "Epoch 10/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.8025 - accuracy: 0.7205 - val_loss: 0.8098 - val_accuracy: 0.7231 - lr: 0.0010\n",
            "Epoch 11/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.7787 - accuracy: 0.7297 - val_loss: 0.7751 - val_accuracy: 0.7394 - lr: 0.0010\n",
            "Epoch 12/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.7673 - accuracy: 0.7381 - val_loss: 0.7218 - val_accuracy: 0.7538 - lr: 0.0010\n",
            "Epoch 13/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.7528 - accuracy: 0.7386 - val_loss: 0.7398 - val_accuracy: 0.7478 - lr: 0.0010\n",
            "Epoch 14/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.7454 - accuracy: 0.7397 - val_loss: 0.7720 - val_accuracy: 0.7382 - lr: 0.0010\n",
            "Epoch 15/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.7364 - accuracy: 0.7429 - val_loss: 0.7089 - val_accuracy: 0.7570 - lr: 0.0010\n",
            "Epoch 16/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.7111 - accuracy: 0.7535 - val_loss: 0.7002 - val_accuracy: 0.7579 - lr: 9.0484e-04\n",
            "Epoch 17/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.6903 - accuracy: 0.7613 - val_loss: 0.6741 - val_accuracy: 0.7662 - lr: 8.1873e-04\n",
            "Epoch 18/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.6729 - accuracy: 0.7668 - val_loss: 0.6461 - val_accuracy: 0.7732 - lr: 7.4082e-04\n",
            "Epoch 19/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.6512 - accuracy: 0.7730 - val_loss: 0.6571 - val_accuracy: 0.7741 - lr: 6.7032e-04\n",
            "Epoch 20/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.6414 - accuracy: 0.7768 - val_loss: 0.6701 - val_accuracy: 0.7702 - lr: 6.0653e-04\n",
            "Epoch 21/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.6236 - accuracy: 0.7836 - val_loss: 0.6498 - val_accuracy: 0.7735 - lr: 5.4881e-04\n",
            "Epoch 22/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.6103 - accuracy: 0.7876 - val_loss: 0.6455 - val_accuracy: 0.7776 - lr: 4.9659e-04\n",
            "Epoch 23/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.6048 - accuracy: 0.7879 - val_loss: 0.6279 - val_accuracy: 0.7819 - lr: 4.4933e-04\n",
            "Epoch 24/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.5880 - accuracy: 0.7948 - val_loss: 0.6427 - val_accuracy: 0.7771 - lr: 4.0657e-04\n",
            "Epoch 25/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.5835 - accuracy: 0.7958 - val_loss: 0.6084 - val_accuracy: 0.7934 - lr: 3.6788e-04\n",
            "Epoch 26/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.5740 - accuracy: 0.8007 - val_loss: 0.6087 - val_accuracy: 0.7914 - lr: 3.3287e-04\n",
            "Epoch 27/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.5664 - accuracy: 0.8018 - val_loss: 0.6033 - val_accuracy: 0.7936 - lr: 3.0119e-04\n",
            "Epoch 28/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.5558 - accuracy: 0.8060 - val_loss: 0.6012 - val_accuracy: 0.7950 - lr: 2.7253e-04\n",
            "Epoch 29/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.5502 - accuracy: 0.8098 - val_loss: 0.5935 - val_accuracy: 0.7975 - lr: 2.4660e-04\n",
            "Epoch 30/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.5466 - accuracy: 0.8105 - val_loss: 0.6030 - val_accuracy: 0.7967 - lr: 2.2313e-04\n",
            "Score for fold 4: loss of 0.6029559373855591; accuracy of 79.67000007629395%\n",
            "Model: \"sequential_14\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_42 (Conv2D)          (None, 32, 32, 32)        896       \n",
            "                                                                 \n",
            " max_pooling2d_14 (MaxPoolin  multiple                 0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_43 (Conv2D)          (None, 16, 16, 64)        18496     \n",
            "                                                                 \n",
            " dropout_14 (Dropout)        (None, 8, 8, 64)          0         \n",
            "                                                                 \n",
            " conv2d_44 (Conv2D)          (None, 8, 8, 128)         73856     \n",
            "                                                                 \n",
            " flatten_14 (Flatten)        (None, 2048)              0         \n",
            "                                                                 \n",
            " dense_14 (Dense)            (None, 10)                20490     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 113,738\n",
            "Trainable params: 113,738\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.5785 - accuracy: 0.4271 - val_loss: 1.1966 - val_accuracy: 0.5803 - lr: 0.0010\n",
            "Epoch 2/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.2153 - accuracy: 0.5700 - val_loss: 1.0151 - val_accuracy: 0.6473 - lr: 0.0010\n",
            "Epoch 3/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.0732 - accuracy: 0.6264 - val_loss: 0.9130 - val_accuracy: 0.6825 - lr: 0.0010\n",
            "Epoch 4/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.9871 - accuracy: 0.6549 - val_loss: 0.8584 - val_accuracy: 0.6963 - lr: 0.0010\n",
            "Epoch 5/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.9324 - accuracy: 0.6755 - val_loss: 0.8657 - val_accuracy: 0.7039 - lr: 0.0010\n",
            "Epoch 6/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.8889 - accuracy: 0.6895 - val_loss: 0.8409 - val_accuracy: 0.7064 - lr: 0.0010\n",
            "Epoch 7/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.8556 - accuracy: 0.7034 - val_loss: 0.8751 - val_accuracy: 0.7016 - lr: 0.0010\n",
            "Epoch 8/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.8264 - accuracy: 0.7118 - val_loss: 0.7625 - val_accuracy: 0.7360 - lr: 0.0010\n",
            "Epoch 9/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.8009 - accuracy: 0.7206 - val_loss: 0.7609 - val_accuracy: 0.7404 - lr: 0.0010\n",
            "Epoch 10/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.7835 - accuracy: 0.7276 - val_loss: 0.7084 - val_accuracy: 0.7545 - lr: 0.0010\n",
            "Epoch 11/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.7644 - accuracy: 0.7347 - val_loss: 0.7178 - val_accuracy: 0.7570 - lr: 0.0010\n",
            "Epoch 12/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.7431 - accuracy: 0.7427 - val_loss: 0.7140 - val_accuracy: 0.7563 - lr: 0.0010\n",
            "Epoch 13/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.7388 - accuracy: 0.7446 - val_loss: 0.7740 - val_accuracy: 0.7349 - lr: 0.0010\n",
            "Epoch 14/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.7250 - accuracy: 0.7484 - val_loss: 0.6818 - val_accuracy: 0.7652 - lr: 0.0010\n",
            "Epoch 15/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.7067 - accuracy: 0.7559 - val_loss: 0.6720 - val_accuracy: 0.7710 - lr: 0.0010\n",
            "Epoch 16/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.6847 - accuracy: 0.7632 - val_loss: 0.6952 - val_accuracy: 0.7630 - lr: 9.0484e-04\n",
            "Epoch 17/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.6720 - accuracy: 0.7661 - val_loss: 0.6933 - val_accuracy: 0.7706 - lr: 8.1873e-04\n",
            "Epoch 18/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.6533 - accuracy: 0.7742 - val_loss: 0.6727 - val_accuracy: 0.7770 - lr: 7.4082e-04\n",
            "Epoch 19/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.6312 - accuracy: 0.7811 - val_loss: 0.6876 - val_accuracy: 0.7688 - lr: 6.7032e-04\n",
            "Epoch 20/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.6201 - accuracy: 0.7839 - val_loss: 0.6366 - val_accuracy: 0.7850 - lr: 6.0653e-04\n",
            "Epoch 21/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.6040 - accuracy: 0.7892 - val_loss: 0.6746 - val_accuracy: 0.7738 - lr: 5.4881e-04\n",
            "Epoch 22/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.5909 - accuracy: 0.7929 - val_loss: 0.6122 - val_accuracy: 0.7963 - lr: 4.9659e-04\n",
            "Epoch 23/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.5753 - accuracy: 0.8010 - val_loss: 0.6148 - val_accuracy: 0.7936 - lr: 4.4933e-04\n",
            "Epoch 24/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.5676 - accuracy: 0.8014 - val_loss: 0.6113 - val_accuracy: 0.7946 - lr: 4.0657e-04\n",
            "Epoch 25/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.5596 - accuracy: 0.8063 - val_loss: 0.6147 - val_accuracy: 0.7983 - lr: 3.6788e-04\n",
            "Epoch 26/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.5496 - accuracy: 0.8097 - val_loss: 0.6077 - val_accuracy: 0.7965 - lr: 3.3287e-04\n",
            "Epoch 27/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.5400 - accuracy: 0.8103 - val_loss: 0.6165 - val_accuracy: 0.7949 - lr: 3.0119e-04\n",
            "Epoch 28/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.5340 - accuracy: 0.8148 - val_loss: 0.5884 - val_accuracy: 0.8029 - lr: 2.7253e-04\n",
            "Epoch 29/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.5285 - accuracy: 0.8175 - val_loss: 0.5802 - val_accuracy: 0.8044 - lr: 2.4660e-04\n",
            "Epoch 30/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.5242 - accuracy: 0.8180 - val_loss: 0.6048 - val_accuracy: 0.7964 - lr: 2.2313e-04\n",
            "Score for fold 5: loss of 0.6048241257667542; accuracy of 79.64000105857849%\n",
            "------------------------------------------------------------------------\n",
            "Score per fold\n",
            "------------------------------------------------------------------------\n",
            "> Fold 1 - Loss: 0.5938509702682495 - Accuracy: 80.43000102043152%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 2 - Loss: 0.615408718585968 - Accuracy: 79.46000099182129%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 3 - Loss: 0.6190310716629028 - Accuracy: 79.18999791145325%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 4 - Loss: 0.6029559373855591 - Accuracy: 79.67000007629395%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 5 - Loss: 0.6048241257667542 - Accuracy: 79.64000105857849%\n",
            "------------------------------------------------------------------------\n",
            "Average scores for all folds:\n",
            "> Accuracy: 79.6780002117157 (+- 0.41296078725495805)\n",
            "> Loss: 0.6072141647338867\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Learning_rate=1e-3 & No decay"
      ],
      "metadata": {
        "id": "ceNYv23_vAym"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# K-fold Cross Validation model evaluation\n",
        "acc_per_fold = []\n",
        "loss_per_fold = []\n",
        "\n",
        "with tf.device('/device:GPU:0'):\n",
        "  fold_no = 1\n",
        "  for train, valid in kfold.split(trainX, trainy):\n",
        "\n",
        "    model = CNN_Model(batch_size=32, depth=3, hidden_neurons_1=32, hidden_neurons_2=64, hidden_neurons_3=128, kernel_size_1=3, kernel_size_2=3, kernel_size_3=3, \n",
        "               dropout_rate=0.4, batch_norm=False, pooling='max', layer_activation_1='relu', layer_activation_2='relu', layer_activation_3='relu',\n",
        "               kernel_initialiser='glorot_uniform', bias_initialiser='zeros', optimiser='Adam', weight_decay=None, learning_rate=1e-3, output_activation='softmax', SGD_momentum=0) \n",
        "  \n",
        "    train_gen = tf.keras.preprocessing.image.ImageDataGenerator(width_shift_range=0.1,\n",
        "      height_shift_range=0.1,\n",
        "      horizontal_flip=True)\n",
        "    train_gen.fit(trainX[train])  #Data Augmentation\n",
        "  \n",
        "    print('------------------------------------------------------------------------')\n",
        "    print(f'Training for fold {fold_no} ...')\n",
        "\n",
        "    history = model.train(\n",
        "              train_gen,\n",
        "                trainX[train],\n",
        "                  trainy[train],\n",
        "                trainX[valid],\n",
        "                  trainy[valid],\n",
        "                  lr_rate=False)  \n",
        "\n",
        "    scores = model.evaluate(trainX[valid], trainy[valid])\n",
        "\n",
        "    print(f'Score for fold {fold_no}: loss of {scores[0]}; accuracy of {scores[1]*100}%')\n",
        "    acc_per_fold.append(scores[1] * 100)\n",
        "    loss_per_fold.append(scores[0])\n",
        "\n",
        "    fold_no+=1\n",
        "\n",
        "# == Provide average scores ==\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Score per fold')\n",
        "for i in range(0, len(acc_per_fold)):\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Average scores for all folds:')\n",
        "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
        "print(f'> Loss: {np.mean(loss_per_fold)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-yauCkTqYehI",
        "outputId": "3aae09a0-50ba-4069-b613-e2de2e9fdcf1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_15\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_45 (Conv2D)          (None, 32, 32, 32)        896       \n",
            "                                                                 \n",
            " max_pooling2d_15 (MaxPoolin  multiple                 0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_46 (Conv2D)          (None, 16, 16, 64)        18496     \n",
            "                                                                 \n",
            " dropout_15 (Dropout)        (None, 8, 8, 64)          0         \n",
            "                                                                 \n",
            " conv2d_47 (Conv2D)          (None, 8, 8, 128)         73856     \n",
            "                                                                 \n",
            " flatten_15 (Flatten)        (None, 2048)              0         \n",
            "                                                                 \n",
            " dense_15 (Dense)            (None, 10)                20490     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 113,738\n",
            "Trainable params: 113,738\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.5900 - accuracy: 0.4235 - val_loss: 1.2590 - val_accuracy: 0.5525\n",
            "Epoch 2/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.2623 - accuracy: 0.5502 - val_loss: 1.1200 - val_accuracy: 0.6068\n",
            "Epoch 3/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.1179 - accuracy: 0.6065 - val_loss: 0.9752 - val_accuracy: 0.6643\n",
            "Epoch 4/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.0372 - accuracy: 0.6360 - val_loss: 0.9244 - val_accuracy: 0.6879\n",
            "Epoch 5/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.9835 - accuracy: 0.6557 - val_loss: 0.9079 - val_accuracy: 0.6927\n",
            "Epoch 6/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.9379 - accuracy: 0.6737 - val_loss: 0.9403 - val_accuracy: 0.6794\n",
            "Epoch 7/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.9066 - accuracy: 0.6836 - val_loss: 0.8680 - val_accuracy: 0.6997\n",
            "Epoch 8/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.8769 - accuracy: 0.6947 - val_loss: 0.8321 - val_accuracy: 0.7195\n",
            "Epoch 9/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.8494 - accuracy: 0.7035 - val_loss: 0.8204 - val_accuracy: 0.7209\n",
            "Epoch 10/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.8369 - accuracy: 0.7099 - val_loss: 0.7598 - val_accuracy: 0.7441\n",
            "Epoch 11/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.8205 - accuracy: 0.7142 - val_loss: 0.8576 - val_accuracy: 0.7129\n",
            "Epoch 12/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.8042 - accuracy: 0.7209 - val_loss: 0.7627 - val_accuracy: 0.7442\n",
            "Epoch 13/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.8001 - accuracy: 0.7223 - val_loss: 0.7921 - val_accuracy: 0.7333\n",
            "Epoch 14/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.7789 - accuracy: 0.7264 - val_loss: 0.7481 - val_accuracy: 0.7461\n",
            "Epoch 15/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.7753 - accuracy: 0.7304 - val_loss: 0.7810 - val_accuracy: 0.7384\n",
            "Epoch 16/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.7680 - accuracy: 0.7343 - val_loss: 0.7973 - val_accuracy: 0.7294\n",
            "Epoch 17/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.7625 - accuracy: 0.7358 - val_loss: 0.6945 - val_accuracy: 0.7653\n",
            "Epoch 18/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.7448 - accuracy: 0.7386 - val_loss: 0.7424 - val_accuracy: 0.7467\n",
            "Epoch 19/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.7332 - accuracy: 0.7457 - val_loss: 0.7156 - val_accuracy: 0.7582\n",
            "Epoch 20/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.7371 - accuracy: 0.7459 - val_loss: 0.7512 - val_accuracy: 0.7482\n",
            "Epoch 21/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.7320 - accuracy: 0.7466 - val_loss: 0.7838 - val_accuracy: 0.7466\n",
            "Epoch 22/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.7274 - accuracy: 0.7456 - val_loss: 0.6773 - val_accuracy: 0.7677\n",
            "Epoch 23/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.7204 - accuracy: 0.7493 - val_loss: 0.7010 - val_accuracy: 0.7643\n",
            "Epoch 24/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.7135 - accuracy: 0.7487 - val_loss: 0.6959 - val_accuracy: 0.7642\n",
            "Epoch 25/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.7072 - accuracy: 0.7551 - val_loss: 0.6955 - val_accuracy: 0.7596\n",
            "Epoch 26/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.6995 - accuracy: 0.7566 - val_loss: 0.6949 - val_accuracy: 0.7633\n",
            "Epoch 27/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.7021 - accuracy: 0.7549 - val_loss: 0.6929 - val_accuracy: 0.7687\n",
            "Epoch 28/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.6959 - accuracy: 0.7555 - val_loss: 0.6827 - val_accuracy: 0.7743\n",
            "Epoch 29/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.6953 - accuracy: 0.7577 - val_loss: 0.7116 - val_accuracy: 0.7606\n",
            "Epoch 30/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.6856 - accuracy: 0.7634 - val_loss: 0.6529 - val_accuracy: 0.7765\n",
            "Score for fold 1: loss of 0.6528710126876831; accuracy of 77.64999866485596%\n",
            "Model: \"sequential_16\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_48 (Conv2D)          (None, 32, 32, 32)        896       \n",
            "                                                                 \n",
            " max_pooling2d_16 (MaxPoolin  multiple                 0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_49 (Conv2D)          (None, 16, 16, 64)        18496     \n",
            "                                                                 \n",
            " dropout_16 (Dropout)        (None, 8, 8, 64)          0         \n",
            "                                                                 \n",
            " conv2d_50 (Conv2D)          (None, 8, 8, 128)         73856     \n",
            "                                                                 \n",
            " flatten_16 (Flatten)        (None, 2048)              0         \n",
            "                                                                 \n",
            " dense_16 (Dense)            (None, 10)                20490     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 113,738\n",
            "Trainable params: 113,738\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.6126 - accuracy: 0.4136 - val_loss: 1.2419 - val_accuracy: 0.5602\n",
            "Epoch 2/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.2587 - accuracy: 0.5509 - val_loss: 1.0652 - val_accuracy: 0.6186\n",
            "Epoch 3/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.1154 - accuracy: 0.6107 - val_loss: 0.9213 - val_accuracy: 0.6848\n",
            "Epoch 4/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.0305 - accuracy: 0.6382 - val_loss: 0.9408 - val_accuracy: 0.6657\n",
            "Epoch 5/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.9804 - accuracy: 0.6597 - val_loss: 0.8568 - val_accuracy: 0.7023\n",
            "Epoch 6/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.9457 - accuracy: 0.6687 - val_loss: 0.9013 - val_accuracy: 0.6881\n",
            "Epoch 7/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.9100 - accuracy: 0.6847 - val_loss: 0.7838 - val_accuracy: 0.7228\n",
            "Epoch 8/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.8893 - accuracy: 0.6892 - val_loss: 0.7770 - val_accuracy: 0.7272\n",
            "Epoch 9/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.8614 - accuracy: 0.7023 - val_loss: 0.8238 - val_accuracy: 0.7123\n",
            "Epoch 10/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.8416 - accuracy: 0.7089 - val_loss: 0.7485 - val_accuracy: 0.7425\n",
            "Epoch 11/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.8230 - accuracy: 0.7151 - val_loss: 0.7486 - val_accuracy: 0.7363\n",
            "Epoch 12/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.8147 - accuracy: 0.7161 - val_loss: 0.7545 - val_accuracy: 0.7378\n",
            "Epoch 13/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.8042 - accuracy: 0.7211 - val_loss: 0.7326 - val_accuracy: 0.7516\n",
            "Epoch 14/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.7944 - accuracy: 0.7223 - val_loss: 0.7218 - val_accuracy: 0.7494\n",
            "Epoch 15/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.7766 - accuracy: 0.7299 - val_loss: 0.6774 - val_accuracy: 0.7648\n",
            "Epoch 16/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.7725 - accuracy: 0.7302 - val_loss: 0.7218 - val_accuracy: 0.7520\n",
            "Epoch 17/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.7624 - accuracy: 0.7355 - val_loss: 0.7669 - val_accuracy: 0.7446\n",
            "Epoch 18/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.7572 - accuracy: 0.7373 - val_loss: 0.7072 - val_accuracy: 0.7521\n",
            "Epoch 19/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.7505 - accuracy: 0.7386 - val_loss: 0.6577 - val_accuracy: 0.7759\n",
            "Epoch 20/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.7416 - accuracy: 0.7413 - val_loss: 0.7061 - val_accuracy: 0.7615\n",
            "Epoch 21/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.7307 - accuracy: 0.7454 - val_loss: 0.6987 - val_accuracy: 0.7598\n",
            "Epoch 22/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.7318 - accuracy: 0.7445 - val_loss: 0.6767 - val_accuracy: 0.7699\n",
            "Epoch 23/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.7239 - accuracy: 0.7473 - val_loss: 0.6614 - val_accuracy: 0.7723\n",
            "Epoch 24/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.7211 - accuracy: 0.7494 - val_loss: 0.6801 - val_accuracy: 0.7636\n",
            "Epoch 25/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.7171 - accuracy: 0.7500 - val_loss: 0.6424 - val_accuracy: 0.7811\n",
            "Epoch 26/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.7097 - accuracy: 0.7534 - val_loss: 0.6669 - val_accuracy: 0.7757\n",
            "Epoch 27/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.7043 - accuracy: 0.7563 - val_loss: 0.6490 - val_accuracy: 0.7746\n",
            "Epoch 28/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.7039 - accuracy: 0.7579 - val_loss: 0.6649 - val_accuracy: 0.7694\n",
            "Epoch 29/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.7022 - accuracy: 0.7555 - val_loss: 0.6476 - val_accuracy: 0.7771\n",
            "Epoch 30/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.6977 - accuracy: 0.7562 - val_loss: 0.6164 - val_accuracy: 0.7857\n",
            "Score for fold 2: loss of 0.616449773311615; accuracy of 78.57000231742859%\n",
            "Model: \"sequential_17\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_51 (Conv2D)          (None, 32, 32, 32)        896       \n",
            "                                                                 \n",
            " max_pooling2d_17 (MaxPoolin  multiple                 0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_52 (Conv2D)          (None, 16, 16, 64)        18496     \n",
            "                                                                 \n",
            " dropout_17 (Dropout)        (None, 8, 8, 64)          0         \n",
            "                                                                 \n",
            " conv2d_53 (Conv2D)          (None, 8, 8, 128)         73856     \n",
            "                                                                 \n",
            " flatten_17 (Flatten)        (None, 2048)              0         \n",
            "                                                                 \n",
            " dense_17 (Dense)            (None, 10)                20490     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 113,738\n",
            "Trainable params: 113,738\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.6134 - accuracy: 0.4161 - val_loss: 1.3253 - val_accuracy: 0.5283\n",
            "Epoch 2/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.2907 - accuracy: 0.5409 - val_loss: 1.0926 - val_accuracy: 0.6151\n",
            "Epoch 3/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.1520 - accuracy: 0.5926 - val_loss: 0.9656 - val_accuracy: 0.6583\n",
            "Epoch 4/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.0745 - accuracy: 0.6202 - val_loss: 0.9667 - val_accuracy: 0.6694\n",
            "Epoch 5/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.0116 - accuracy: 0.6450 - val_loss: 0.8798 - val_accuracy: 0.6916\n",
            "Epoch 6/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.9709 - accuracy: 0.6615 - val_loss: 0.8476 - val_accuracy: 0.7016\n",
            "Epoch 7/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.9328 - accuracy: 0.6764 - val_loss: 0.8516 - val_accuracy: 0.7023\n",
            "Epoch 8/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.9187 - accuracy: 0.6802 - val_loss: 0.8914 - val_accuracy: 0.6851\n",
            "Epoch 9/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.8830 - accuracy: 0.6910 - val_loss: 0.7932 - val_accuracy: 0.7257\n",
            "Epoch 10/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.8684 - accuracy: 0.6972 - val_loss: 0.7759 - val_accuracy: 0.7358\n",
            "Epoch 11/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 0.8551 - accuracy: 0.7036 - val_loss: 0.7528 - val_accuracy: 0.7371\n",
            "Epoch 12/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.8377 - accuracy: 0.7097 - val_loss: 0.7836 - val_accuracy: 0.7299\n",
            "Epoch 13/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 0.8273 - accuracy: 0.7143 - val_loss: 0.7465 - val_accuracy: 0.7436\n",
            "Epoch 14/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.8111 - accuracy: 0.7180 - val_loss: 0.7855 - val_accuracy: 0.7298\n",
            "Epoch 15/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.7980 - accuracy: 0.7221 - val_loss: 0.7672 - val_accuracy: 0.7398\n",
            "Epoch 16/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.7904 - accuracy: 0.7250 - val_loss: 0.7172 - val_accuracy: 0.7509\n",
            "Epoch 17/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.7821 - accuracy: 0.7275 - val_loss: 0.7147 - val_accuracy: 0.7536\n",
            "Epoch 18/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.7755 - accuracy: 0.7316 - val_loss: 0.7650 - val_accuracy: 0.7369\n",
            "Epoch 19/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.7683 - accuracy: 0.7326 - val_loss: 0.7363 - val_accuracy: 0.7515\n",
            "Epoch 20/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.7703 - accuracy: 0.7327 - val_loss: 0.6740 - val_accuracy: 0.7726\n",
            "Epoch 21/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.7567 - accuracy: 0.7355 - val_loss: 0.7021 - val_accuracy: 0.7585\n",
            "Epoch 22/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.7558 - accuracy: 0.7366 - val_loss: 0.6674 - val_accuracy: 0.7680\n",
            "Epoch 23/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.7506 - accuracy: 0.7407 - val_loss: 0.7279 - val_accuracy: 0.7468\n",
            "Epoch 24/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.7363 - accuracy: 0.7427 - val_loss: 0.6870 - val_accuracy: 0.7591\n",
            "Epoch 25/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.7357 - accuracy: 0.7440 - val_loss: 0.6663 - val_accuracy: 0.7674\n",
            "Epoch 26/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.7326 - accuracy: 0.7444 - val_loss: 0.6768 - val_accuracy: 0.7641\n",
            "Epoch 27/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.7244 - accuracy: 0.7496 - val_loss: 0.7067 - val_accuracy: 0.7532\n",
            "Epoch 28/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.7265 - accuracy: 0.7464 - val_loss: 0.6395 - val_accuracy: 0.7824\n",
            "Epoch 29/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.7231 - accuracy: 0.7479 - val_loss: 0.7297 - val_accuracy: 0.7435\n",
            "Epoch 30/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.7136 - accuracy: 0.7511 - val_loss: 0.6507 - val_accuracy: 0.7751\n",
            "Score for fold 3: loss of 0.6506800055503845; accuracy of 77.50999927520752%\n",
            "Model: \"sequential_18\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_54 (Conv2D)          (None, 32, 32, 32)        896       \n",
            "                                                                 \n",
            " max_pooling2d_18 (MaxPoolin  multiple                 0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_55 (Conv2D)          (None, 16, 16, 64)        18496     \n",
            "                                                                 \n",
            " dropout_18 (Dropout)        (None, 8, 8, 64)          0         \n",
            "                                                                 \n",
            " conv2d_56 (Conv2D)          (None, 8, 8, 128)         73856     \n",
            "                                                                 \n",
            " flatten_18 (Flatten)        (None, 2048)              0         \n",
            "                                                                 \n",
            " dense_18 (Dense)            (None, 10)                20490     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 113,738\n",
            "Trainable params: 113,738\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.6244 - accuracy: 0.4061 - val_loss: 1.2847 - val_accuracy: 0.5405\n",
            "Epoch 2/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.2927 - accuracy: 0.5386 - val_loss: 1.1523 - val_accuracy: 0.6004\n",
            "Epoch 3/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 1.1612 - accuracy: 0.5923 - val_loss: 1.0368 - val_accuracy: 0.6456\n",
            "Epoch 4/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 1.0758 - accuracy: 0.6229 - val_loss: 0.9659 - val_accuracy: 0.6673\n",
            "Epoch 5/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 1.0161 - accuracy: 0.6442 - val_loss: 0.8530 - val_accuracy: 0.7083\n",
            "Epoch 6/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.9701 - accuracy: 0.6623 - val_loss: 0.8442 - val_accuracy: 0.7154\n",
            "Epoch 7/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.9385 - accuracy: 0.6713 - val_loss: 0.8111 - val_accuracy: 0.7212\n",
            "Epoch 8/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.9153 - accuracy: 0.6778 - val_loss: 0.7842 - val_accuracy: 0.7336\n",
            "Epoch 9/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.8879 - accuracy: 0.6892 - val_loss: 0.8550 - val_accuracy: 0.7078\n",
            "Epoch 10/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.8716 - accuracy: 0.6965 - val_loss: 0.7795 - val_accuracy: 0.7320\n",
            "Epoch 11/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.8459 - accuracy: 0.7023 - val_loss: 0.7960 - val_accuracy: 0.7274\n",
            "Epoch 12/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.8380 - accuracy: 0.7078 - val_loss: 0.8159 - val_accuracy: 0.7173\n",
            "Epoch 13/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.8341 - accuracy: 0.7096 - val_loss: 0.7710 - val_accuracy: 0.7381\n",
            "Epoch 14/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.8128 - accuracy: 0.7150 - val_loss: 0.7740 - val_accuracy: 0.7308\n",
            "Epoch 15/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.8023 - accuracy: 0.7217 - val_loss: 0.7438 - val_accuracy: 0.7450\n",
            "Epoch 16/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.7906 - accuracy: 0.7248 - val_loss: 0.7536 - val_accuracy: 0.7365\n",
            "Epoch 17/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.7859 - accuracy: 0.7258 - val_loss: 0.7396 - val_accuracy: 0.7438\n",
            "Epoch 18/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.7820 - accuracy: 0.7286 - val_loss: 0.7002 - val_accuracy: 0.7563\n",
            "Epoch 19/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.7708 - accuracy: 0.7310 - val_loss: 0.7120 - val_accuracy: 0.7565\n",
            "Epoch 20/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.7642 - accuracy: 0.7341 - val_loss: 0.7085 - val_accuracy: 0.7553\n",
            "Epoch 21/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.7566 - accuracy: 0.7360 - val_loss: 0.6884 - val_accuracy: 0.7642\n",
            "Epoch 22/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.7522 - accuracy: 0.7400 - val_loss: 0.6945 - val_accuracy: 0.7620\n",
            "Epoch 23/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.7503 - accuracy: 0.7361 - val_loss: 0.6996 - val_accuracy: 0.7625\n",
            "Epoch 24/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.7348 - accuracy: 0.7447 - val_loss: 0.6867 - val_accuracy: 0.7670\n",
            "Epoch 25/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.7410 - accuracy: 0.7409 - val_loss: 0.7166 - val_accuracy: 0.7577\n",
            "Epoch 26/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.7378 - accuracy: 0.7435 - val_loss: 0.6758 - val_accuracy: 0.7694\n",
            "Epoch 27/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.7219 - accuracy: 0.7465 - val_loss: 0.6434 - val_accuracy: 0.7759\n",
            "Epoch 28/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.7227 - accuracy: 0.7509 - val_loss: 0.6935 - val_accuracy: 0.7597\n",
            "Epoch 29/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.7195 - accuracy: 0.7496 - val_loss: 0.6807 - val_accuracy: 0.7648\n",
            "Epoch 30/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.7189 - accuracy: 0.7473 - val_loss: 0.6677 - val_accuracy: 0.7677\n",
            "Score for fold 4: loss of 0.6676653623580933; accuracy of 76.77000164985657%\n",
            "Model: \"sequential_19\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_57 (Conv2D)          (None, 32, 32, 32)        896       \n",
            "                                                                 \n",
            " max_pooling2d_19 (MaxPoolin  multiple                 0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_58 (Conv2D)          (None, 16, 16, 64)        18496     \n",
            "                                                                 \n",
            " dropout_19 (Dropout)        (None, 8, 8, 64)          0         \n",
            "                                                                 \n",
            " conv2d_59 (Conv2D)          (None, 8, 8, 128)         73856     \n",
            "                                                                 \n",
            " flatten_19 (Flatten)        (None, 2048)              0         \n",
            "                                                                 \n",
            " dense_19 (Dense)            (None, 10)                20490     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 113,738\n",
            "Trainable params: 113,738\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.6215 - accuracy: 0.4085 - val_loss: 1.3184 - val_accuracy: 0.5415\n",
            "Epoch 2/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.2787 - accuracy: 0.5454 - val_loss: 1.1079 - val_accuracy: 0.6167\n",
            "Epoch 3/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.1485 - accuracy: 0.5909 - val_loss: 0.9853 - val_accuracy: 0.6628\n",
            "Epoch 4/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.0674 - accuracy: 0.6223 - val_loss: 0.9575 - val_accuracy: 0.6731\n",
            "Epoch 5/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.0127 - accuracy: 0.6479 - val_loss: 0.8905 - val_accuracy: 0.6949\n",
            "Epoch 6/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.9594 - accuracy: 0.6639 - val_loss: 0.8467 - val_accuracy: 0.7107\n",
            "Epoch 7/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.9321 - accuracy: 0.6747 - val_loss: 0.8050 - val_accuracy: 0.7243\n",
            "Epoch 8/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.9088 - accuracy: 0.6840 - val_loss: 0.8492 - val_accuracy: 0.7024\n",
            "Epoch 9/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.8808 - accuracy: 0.6922 - val_loss: 0.7939 - val_accuracy: 0.7286\n",
            "Epoch 10/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.8637 - accuracy: 0.6970 - val_loss: 0.7768 - val_accuracy: 0.7370\n",
            "Epoch 11/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.8439 - accuracy: 0.7052 - val_loss: 0.7780 - val_accuracy: 0.7282\n",
            "Epoch 12/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.8268 - accuracy: 0.7104 - val_loss: 0.7605 - val_accuracy: 0.7410\n",
            "Epoch 13/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.8157 - accuracy: 0.7139 - val_loss: 0.7908 - val_accuracy: 0.7292\n",
            "Epoch 14/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.8090 - accuracy: 0.7178 - val_loss: 0.7754 - val_accuracy: 0.7333\n",
            "Epoch 15/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.8026 - accuracy: 0.7186 - val_loss: 0.8000 - val_accuracy: 0.7306\n",
            "Epoch 16/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.7848 - accuracy: 0.7268 - val_loss: 0.6935 - val_accuracy: 0.7624\n",
            "Epoch 17/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.7856 - accuracy: 0.7270 - val_loss: 0.7253 - val_accuracy: 0.7510\n",
            "Epoch 18/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.7761 - accuracy: 0.7295 - val_loss: 0.7311 - val_accuracy: 0.7499\n",
            "Epoch 19/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.7634 - accuracy: 0.7326 - val_loss: 0.6698 - val_accuracy: 0.7698\n",
            "Epoch 20/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.7597 - accuracy: 0.7335 - val_loss: 0.6899 - val_accuracy: 0.7616\n",
            "Epoch 21/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.7535 - accuracy: 0.7352 - val_loss: 0.7016 - val_accuracy: 0.7581\n",
            "Epoch 22/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.7470 - accuracy: 0.7424 - val_loss: 0.6876 - val_accuracy: 0.7610\n",
            "Epoch 23/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.7383 - accuracy: 0.7426 - val_loss: 0.6790 - val_accuracy: 0.7600\n",
            "Epoch 24/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.7333 - accuracy: 0.7450 - val_loss: 0.7233 - val_accuracy: 0.7528\n",
            "Epoch 25/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.7383 - accuracy: 0.7413 - val_loss: 0.6704 - val_accuracy: 0.7642\n",
            "Epoch 26/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.7305 - accuracy: 0.7478 - val_loss: 0.6773 - val_accuracy: 0.7648\n",
            "Epoch 27/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.7230 - accuracy: 0.7488 - val_loss: 0.7000 - val_accuracy: 0.7604\n",
            "Epoch 28/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.7138 - accuracy: 0.7504 - val_loss: 0.6621 - val_accuracy: 0.7764\n",
            "Epoch 29/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.7202 - accuracy: 0.7487 - val_loss: 0.6801 - val_accuracy: 0.7665\n",
            "Epoch 30/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.7111 - accuracy: 0.7532 - val_loss: 0.6409 - val_accuracy: 0.7840\n",
            "Score for fold 5: loss of 0.6408511400222778; accuracy of 78.39999794960022%\n",
            "------------------------------------------------------------------------\n",
            "Score per fold\n",
            "------------------------------------------------------------------------\n",
            "> Fold 1 - Loss: 0.6528710126876831 - Accuracy: 77.64999866485596%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 2 - Loss: 0.616449773311615 - Accuracy: 78.57000231742859%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 3 - Loss: 0.6506800055503845 - Accuracy: 77.50999927520752%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 4 - Loss: 0.6676653623580933 - Accuracy: 76.77000164985657%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 5 - Loss: 0.6408511400222778 - Accuracy: 78.39999794960022%\n",
            "------------------------------------------------------------------------\n",
            "Average scores for all folds:\n",
            "> Accuracy: 77.77999997138977 (+- 0.6509068327911711)\n",
            "> Loss: 0.6457034587860108\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Learning_rate=1e-3 & Step decay"
      ],
      "metadata": {
        "id": "dbJ_oHEnvYfj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# K-fold Cross Validation model evaluation\n",
        "acc_per_fold = []\n",
        "loss_per_fold = []\n",
        "\n",
        "with tf.device('/device:GPU:0'):\n",
        "  fold_no = 1\n",
        "  for train, valid in kfold.split(trainX, trainy):\n",
        "\n",
        "    model = CNN_Model(batch_size=32, depth=3, hidden_neurons_1=32, hidden_neurons_2=64, hidden_neurons_3=128, kernel_size_1=3, kernel_size_2=3, kernel_size_3=3, \n",
        "               dropout_rate=0.2, batch_norm=False, pooling='max', layer_activation_1='relu', layer_activation_2='relu', layer_activation_3='relu',\n",
        "               kernel_initialiser='glorot_uniform', bias_initialiser='zeros', optimiser='Adam', weight_decay=None, learning_rate=1e-3, output_activation='softmax', SGD_momentum=0)\n",
        "  \n",
        "    train_gen = tf.keras.preprocessing.image.ImageDataGenerator(width_shift_range=0.1,\n",
        "      height_shift_range=0.1,\n",
        "      horizontal_flip=True)\n",
        "    train_gen.fit(trainX[train])\n",
        "  \n",
        "    print('------------------------------------------------------------------------')\n",
        "    print(f'Training for fold {fold_no} ...')\n",
        "\n",
        "    history = model.train(\n",
        "              train_gen,\n",
        "                trainX[train],\n",
        "                  trainy[train],\n",
        "                trainX[valid],\n",
        "                  trainy[valid],\n",
        "                  lr_rate_type='step')\n",
        "\n",
        "    scores = model.evaluate(trainX[valid], trainy[valid])\n",
        "\n",
        "    print(f'Score for fold {fold_no}: loss of {scores[0]}; accuracy of {scores[1]*100}%')\n",
        "    acc_per_fold.append(scores[1] * 100)\n",
        "    loss_per_fold.append(scores[0])\n",
        "\n",
        "    fold_no+=1\n",
        "\n",
        "# == Provide average scores ==\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Score per fold')\n",
        "for i in range(0, len(acc_per_fold)):\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Average scores for all folds:')\n",
        "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
        "print(f'> Loss: {np.mean(loss_per_fold)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Zqwh6IlvYRC",
        "outputId": "acac10cf-5ebf-42d0-fbcd-53ca0e61eee0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_20\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_60 (Conv2D)          (None, 32, 32, 32)        896       \n",
            "                                                                 \n",
            " max_pooling2d_20 (MaxPoolin  multiple                 0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_61 (Conv2D)          (None, 16, 16, 64)        18496     \n",
            "                                                                 \n",
            " dropout_20 (Dropout)        (None, 8, 8, 64)          0         \n",
            "                                                                 \n",
            " conv2d_62 (Conv2D)          (None, 8, 8, 128)         73856     \n",
            "                                                                 \n",
            " flatten_20 (Flatten)        (None, 2048)              0         \n",
            "                                                                 \n",
            " dense_20 (Dense)            (None, 10)                20490     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 113,738\n",
            "Trainable params: 113,738\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 1.5613 - accuracy: 0.4370 - val_loss: 1.2872 - val_accuracy: 0.5526 - lr: 0.0010\n",
            "Epoch 2/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.2122 - accuracy: 0.5732 - val_loss: 1.0884 - val_accuracy: 0.6236 - lr: 0.0010\n",
            "Epoch 3/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.0849 - accuracy: 0.6205 - val_loss: 0.9646 - val_accuracy: 0.6682 - lr: 0.0010\n",
            "Epoch 4/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.0075 - accuracy: 0.6484 - val_loss: 0.8878 - val_accuracy: 0.6954 - lr: 0.0010\n",
            "Epoch 5/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.9480 - accuracy: 0.6729 - val_loss: 0.8708 - val_accuracy: 0.7009 - lr: 0.0010\n",
            "Epoch 6/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.9125 - accuracy: 0.6830 - val_loss: 0.8313 - val_accuracy: 0.7126 - lr: 0.0010\n",
            "Epoch 7/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.8726 - accuracy: 0.6957 - val_loss: 0.7863 - val_accuracy: 0.7312 - lr: 0.0010\n",
            "Epoch 8/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.8448 - accuracy: 0.7040 - val_loss: 0.8345 - val_accuracy: 0.7179 - lr: 0.0010\n",
            "Epoch 9/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.8269 - accuracy: 0.7127 - val_loss: 0.7829 - val_accuracy: 0.7315 - lr: 0.0010\n",
            "Epoch 10/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.8017 - accuracy: 0.7218 - val_loss: 0.7314 - val_accuracy: 0.7517 - lr: 0.0010\n",
            "Epoch 11/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.7338 - accuracy: 0.7431 - val_loss: 0.7190 - val_accuracy: 0.7564 - lr: 5.0000e-04\n",
            "Epoch 12/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.7133 - accuracy: 0.7505 - val_loss: 0.6890 - val_accuracy: 0.7621 - lr: 5.0000e-04\n",
            "Epoch 13/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.7042 - accuracy: 0.7569 - val_loss: 0.6798 - val_accuracy: 0.7676 - lr: 5.0000e-04\n",
            "Epoch 14/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.6928 - accuracy: 0.7617 - val_loss: 0.7232 - val_accuracy: 0.7573 - lr: 5.0000e-04\n",
            "Epoch 15/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.6803 - accuracy: 0.7630 - val_loss: 0.7053 - val_accuracy: 0.7608 - lr: 5.0000e-04\n",
            "Epoch 16/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.6737 - accuracy: 0.7673 - val_loss: 0.6606 - val_accuracy: 0.7759 - lr: 5.0000e-04\n",
            "Epoch 17/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.6704 - accuracy: 0.7681 - val_loss: 0.6462 - val_accuracy: 0.7801 - lr: 5.0000e-04\n",
            "Epoch 18/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.6612 - accuracy: 0.7681 - val_loss: 0.6638 - val_accuracy: 0.7754 - lr: 5.0000e-04\n",
            "Epoch 19/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.6540 - accuracy: 0.7740 - val_loss: 0.6515 - val_accuracy: 0.7807 - lr: 5.0000e-04\n",
            "Epoch 20/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.6446 - accuracy: 0.7776 - val_loss: 0.6320 - val_accuracy: 0.7873 - lr: 5.0000e-04\n",
            "Epoch 21/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.5935 - accuracy: 0.7933 - val_loss: 0.6193 - val_accuracy: 0.7904 - lr: 1.2500e-04\n",
            "Epoch 22/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.5893 - accuracy: 0.7934 - val_loss: 0.6292 - val_accuracy: 0.7878 - lr: 1.2500e-04\n",
            "Epoch 23/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.5861 - accuracy: 0.7976 - val_loss: 0.6120 - val_accuracy: 0.7955 - lr: 1.2500e-04\n",
            "Epoch 24/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.5809 - accuracy: 0.7965 - val_loss: 0.6088 - val_accuracy: 0.7942 - lr: 1.2500e-04\n",
            "Epoch 25/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.5803 - accuracy: 0.8001 - val_loss: 0.6054 - val_accuracy: 0.7984 - lr: 1.2500e-04\n",
            "Epoch 26/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.5770 - accuracy: 0.7999 - val_loss: 0.6060 - val_accuracy: 0.7948 - lr: 1.2500e-04\n",
            "Epoch 27/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.5755 - accuracy: 0.8012 - val_loss: 0.5997 - val_accuracy: 0.7995 - lr: 1.2500e-04\n",
            "Epoch 28/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 0.5758 - accuracy: 0.7995 - val_loss: 0.6020 - val_accuracy: 0.7978 - lr: 1.2500e-04\n",
            "Epoch 29/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.5660 - accuracy: 0.8032 - val_loss: 0.5962 - val_accuracy: 0.8005 - lr: 1.2500e-04\n",
            "Epoch 30/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.5692 - accuracy: 0.8031 - val_loss: 0.5948 - val_accuracy: 0.8015 - lr: 1.2500e-04\n",
            "Score for fold 1: loss of 0.5948396325111389; accuracy of 80.15000224113464%\n",
            "Model: \"sequential_21\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_63 (Conv2D)          (None, 32, 32, 32)        896       \n",
            "                                                                 \n",
            " max_pooling2d_21 (MaxPoolin  multiple                 0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_64 (Conv2D)          (None, 16, 16, 64)        18496     \n",
            "                                                                 \n",
            " dropout_21 (Dropout)        (None, 8, 8, 64)          0         \n",
            "                                                                 \n",
            " conv2d_65 (Conv2D)          (None, 8, 8, 128)         73856     \n",
            "                                                                 \n",
            " flatten_21 (Flatten)        (None, 2048)              0         \n",
            "                                                                 \n",
            " dense_21 (Dense)            (None, 10)                20490     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 113,738\n",
            "Trainable params: 113,738\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.5662 - accuracy: 0.4288 - val_loss: 1.2118 - val_accuracy: 0.5649 - lr: 0.0010\n",
            "Epoch 2/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.2204 - accuracy: 0.5688 - val_loss: 1.0411 - val_accuracy: 0.6336 - lr: 0.0010\n",
            "Epoch 3/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.0840 - accuracy: 0.6179 - val_loss: 0.9541 - val_accuracy: 0.6713 - lr: 0.0010\n",
            "Epoch 4/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.0027 - accuracy: 0.6496 - val_loss: 0.9108 - val_accuracy: 0.6907 - lr: 0.0010\n",
            "Epoch 5/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.9456 - accuracy: 0.6678 - val_loss: 0.8166 - val_accuracy: 0.7216 - lr: 0.0010\n",
            "Epoch 6/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.8982 - accuracy: 0.6882 - val_loss: 0.8474 - val_accuracy: 0.7117 - lr: 0.0010\n",
            "Epoch 7/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.8612 - accuracy: 0.7002 - val_loss: 0.8460 - val_accuracy: 0.7106 - lr: 0.0010\n",
            "Epoch 8/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.8338 - accuracy: 0.7106 - val_loss: 0.8007 - val_accuracy: 0.7265 - lr: 0.0010\n",
            "Epoch 9/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.8133 - accuracy: 0.7174 - val_loss: 0.7735 - val_accuracy: 0.7356 - lr: 0.0010\n",
            "Epoch 10/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.7892 - accuracy: 0.7255 - val_loss: 0.7265 - val_accuracy: 0.7490 - lr: 0.0010\n",
            "Epoch 11/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.7256 - accuracy: 0.7473 - val_loss: 0.6838 - val_accuracy: 0.7632 - lr: 5.0000e-04\n",
            "Epoch 12/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.7017 - accuracy: 0.7562 - val_loss: 0.6867 - val_accuracy: 0.7651 - lr: 5.0000e-04\n",
            "Epoch 13/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.6856 - accuracy: 0.7637 - val_loss: 0.6731 - val_accuracy: 0.7685 - lr: 5.0000e-04\n",
            "Epoch 14/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.6822 - accuracy: 0.7630 - val_loss: 0.7041 - val_accuracy: 0.7612 - lr: 5.0000e-04\n",
            "Epoch 15/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.6674 - accuracy: 0.7658 - val_loss: 0.6417 - val_accuracy: 0.7812 - lr: 5.0000e-04\n",
            "Epoch 16/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.6627 - accuracy: 0.7701 - val_loss: 0.7120 - val_accuracy: 0.7623 - lr: 5.0000e-04\n",
            "Epoch 17/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.6569 - accuracy: 0.7734 - val_loss: 0.6505 - val_accuracy: 0.7786 - lr: 5.0000e-04\n",
            "Epoch 18/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.6482 - accuracy: 0.7759 - val_loss: 0.6331 - val_accuracy: 0.7862 - lr: 5.0000e-04\n",
            "Epoch 19/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.6389 - accuracy: 0.7774 - val_loss: 0.6847 - val_accuracy: 0.7658 - lr: 5.0000e-04\n",
            "Epoch 20/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.6336 - accuracy: 0.7804 - val_loss: 0.6525 - val_accuracy: 0.7797 - lr: 5.0000e-04\n",
            "Epoch 21/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.5838 - accuracy: 0.7973 - val_loss: 0.6248 - val_accuracy: 0.7891 - lr: 1.2500e-04\n",
            "Epoch 22/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.5762 - accuracy: 0.7988 - val_loss: 0.6197 - val_accuracy: 0.7897 - lr: 1.2500e-04\n",
            "Epoch 23/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.5719 - accuracy: 0.8020 - val_loss: 0.6081 - val_accuracy: 0.7936 - lr: 1.2500e-04\n",
            "Epoch 24/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.5645 - accuracy: 0.8027 - val_loss: 0.6197 - val_accuracy: 0.7917 - lr: 1.2500e-04\n",
            "Epoch 25/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.5700 - accuracy: 0.8024 - val_loss: 0.5983 - val_accuracy: 0.7985 - lr: 1.2500e-04\n",
            "Epoch 26/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.5589 - accuracy: 0.8066 - val_loss: 0.5990 - val_accuracy: 0.7984 - lr: 1.2500e-04\n",
            "Epoch 27/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.5608 - accuracy: 0.8060 - val_loss: 0.6015 - val_accuracy: 0.7961 - lr: 1.2500e-04\n",
            "Epoch 28/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.5600 - accuracy: 0.8084 - val_loss: 0.5908 - val_accuracy: 0.7982 - lr: 1.2500e-04\n",
            "Epoch 29/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.5567 - accuracy: 0.8073 - val_loss: 0.5983 - val_accuracy: 0.7967 - lr: 1.2500e-04\n",
            "Epoch 30/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.5509 - accuracy: 0.8081 - val_loss: 0.5806 - val_accuracy: 0.8055 - lr: 1.2500e-04\n",
            "Score for fold 2: loss of 0.5805827379226685; accuracy of 80.54999709129333%\n",
            "Model: \"sequential_22\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_66 (Conv2D)          (None, 32, 32, 32)        896       \n",
            "                                                                 \n",
            " max_pooling2d_22 (MaxPoolin  multiple                 0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_67 (Conv2D)          (None, 16, 16, 64)        18496     \n",
            "                                                                 \n",
            " dropout_22 (Dropout)        (None, 8, 8, 64)          0         \n",
            "                                                                 \n",
            " conv2d_68 (Conv2D)          (None, 8, 8, 128)         73856     \n",
            "                                                                 \n",
            " flatten_22 (Flatten)        (None, 2048)              0         \n",
            "                                                                 \n",
            " dense_22 (Dense)            (None, 10)                20490     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 113,738\n",
            "Trainable params: 113,738\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.5898 - accuracy: 0.4230 - val_loss: 1.2860 - val_accuracy: 0.5370 - lr: 0.0010\n",
            "Epoch 2/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.2255 - accuracy: 0.5612 - val_loss: 1.0265 - val_accuracy: 0.6391 - lr: 0.0010\n",
            "Epoch 3/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.0757 - accuracy: 0.6219 - val_loss: 0.9799 - val_accuracy: 0.6599 - lr: 0.0010\n",
            "Epoch 4/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.9851 - accuracy: 0.6555 - val_loss: 0.8754 - val_accuracy: 0.6956 - lr: 0.0010\n",
            "Epoch 5/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.9318 - accuracy: 0.6751 - val_loss: 0.8529 - val_accuracy: 0.7066 - lr: 0.0010\n",
            "Epoch 6/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.8883 - accuracy: 0.6910 - val_loss: 0.8714 - val_accuracy: 0.7062 - lr: 0.0010\n",
            "Epoch 7/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.8618 - accuracy: 0.7006 - val_loss: 0.7894 - val_accuracy: 0.7341 - lr: 0.0010\n",
            "Epoch 8/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.8282 - accuracy: 0.7124 - val_loss: 0.9034 - val_accuracy: 0.6897 - lr: 0.0010\n",
            "Epoch 9/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.8089 - accuracy: 0.7189 - val_loss: 0.7937 - val_accuracy: 0.7327 - lr: 0.0010\n",
            "Epoch 10/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.7920 - accuracy: 0.7244 - val_loss: 0.7607 - val_accuracy: 0.7399 - lr: 0.0010\n",
            "Epoch 11/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.7126 - accuracy: 0.7501 - val_loss: 0.7104 - val_accuracy: 0.7590 - lr: 5.0000e-04\n",
            "Epoch 12/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.6980 - accuracy: 0.7562 - val_loss: 0.6828 - val_accuracy: 0.7677 - lr: 5.0000e-04\n",
            "Epoch 13/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.6837 - accuracy: 0.7635 - val_loss: 0.7319 - val_accuracy: 0.7498 - lr: 5.0000e-04\n",
            "Epoch 14/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.6770 - accuracy: 0.7642 - val_loss: 0.6755 - val_accuracy: 0.7693 - lr: 5.0000e-04\n",
            "Epoch 15/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.6630 - accuracy: 0.7716 - val_loss: 0.6933 - val_accuracy: 0.7594 - lr: 5.0000e-04\n",
            "Epoch 16/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.6549 - accuracy: 0.7710 - val_loss: 0.6753 - val_accuracy: 0.7708 - lr: 5.0000e-04\n",
            "Epoch 17/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.6527 - accuracy: 0.7721 - val_loss: 0.6831 - val_accuracy: 0.7662 - lr: 5.0000e-04\n",
            "Epoch 18/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.6419 - accuracy: 0.7764 - val_loss: 0.6852 - val_accuracy: 0.7682 - lr: 5.0000e-04\n",
            "Epoch 19/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.6352 - accuracy: 0.7775 - val_loss: 0.6225 - val_accuracy: 0.7888 - lr: 5.0000e-04\n",
            "Epoch 20/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.6332 - accuracy: 0.7806 - val_loss: 0.6569 - val_accuracy: 0.7727 - lr: 5.0000e-04\n",
            "Epoch 21/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.5764 - accuracy: 0.7987 - val_loss: 0.6145 - val_accuracy: 0.7890 - lr: 1.2500e-04\n",
            "Epoch 22/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.5743 - accuracy: 0.8002 - val_loss: 0.6069 - val_accuracy: 0.7908 - lr: 1.2500e-04\n",
            "Epoch 23/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.5703 - accuracy: 0.8004 - val_loss: 0.6154 - val_accuracy: 0.7872 - lr: 1.2500e-04\n",
            "Epoch 24/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.5660 - accuracy: 0.8037 - val_loss: 0.6152 - val_accuracy: 0.7896 - lr: 1.2500e-04\n",
            "Epoch 25/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.5612 - accuracy: 0.8045 - val_loss: 0.6076 - val_accuracy: 0.7947 - lr: 1.2500e-04\n",
            "Epoch 26/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.5588 - accuracy: 0.8065 - val_loss: 0.6207 - val_accuracy: 0.7913 - lr: 1.2500e-04\n",
            "Epoch 27/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.5577 - accuracy: 0.8074 - val_loss: 0.6202 - val_accuracy: 0.7889 - lr: 1.2500e-04\n",
            "Epoch 28/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.5544 - accuracy: 0.8079 - val_loss: 0.6091 - val_accuracy: 0.7926 - lr: 1.2500e-04\n",
            "Epoch 29/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.5528 - accuracy: 0.8077 - val_loss: 0.6141 - val_accuracy: 0.7936 - lr: 1.2500e-04\n",
            "Epoch 30/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.5513 - accuracy: 0.8083 - val_loss: 0.6094 - val_accuracy: 0.7941 - lr: 1.2500e-04\n",
            "Score for fold 3: loss of 0.6093853712081909; accuracy of 79.40999865531921%\n",
            "Model: \"sequential_23\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_69 (Conv2D)          (None, 32, 32, 32)        896       \n",
            "                                                                 \n",
            " max_pooling2d_23 (MaxPoolin  multiple                 0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_70 (Conv2D)          (None, 16, 16, 64)        18496     \n",
            "                                                                 \n",
            " dropout_23 (Dropout)        (None, 8, 8, 64)          0         \n",
            "                                                                 \n",
            " conv2d_71 (Conv2D)          (None, 8, 8, 128)         73856     \n",
            "                                                                 \n",
            " flatten_23 (Flatten)        (None, 2048)              0         \n",
            "                                                                 \n",
            " dense_23 (Dense)            (None, 10)                20490     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 113,738\n",
            "Trainable params: 113,738\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/30\n",
            "1250/1250 [==============================] - 19s 14ms/step - loss: 1.5862 - accuracy: 0.4254 - val_loss: 1.2177 - val_accuracy: 0.5704 - lr: 0.0010\n",
            "Epoch 2/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.2188 - accuracy: 0.5680 - val_loss: 1.0261 - val_accuracy: 0.6383 - lr: 0.0010\n",
            "Epoch 3/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.0850 - accuracy: 0.6154 - val_loss: 0.9514 - val_accuracy: 0.6654 - lr: 0.0010\n",
            "Epoch 4/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.0002 - accuracy: 0.6506 - val_loss: 0.8511 - val_accuracy: 0.7082 - lr: 0.0010\n",
            "Epoch 5/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.9473 - accuracy: 0.6701 - val_loss: 0.9048 - val_accuracy: 0.6874 - lr: 0.0010\n",
            "Epoch 6/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.9039 - accuracy: 0.6841 - val_loss: 0.8224 - val_accuracy: 0.7143 - lr: 0.0010\n",
            "Epoch 7/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.8706 - accuracy: 0.6973 - val_loss: 0.8662 - val_accuracy: 0.7013 - lr: 0.0010\n",
            "Epoch 8/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.8413 - accuracy: 0.7042 - val_loss: 0.8020 - val_accuracy: 0.7216 - lr: 0.0010\n",
            "Epoch 9/30\n",
            "1250/1250 [==============================] - 20s 16ms/step - loss: 0.8185 - accuracy: 0.7146 - val_loss: 0.7517 - val_accuracy: 0.7424 - lr: 0.0010\n",
            "Epoch 10/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 0.8005 - accuracy: 0.7220 - val_loss: 0.7303 - val_accuracy: 0.7510 - lr: 0.0010\n",
            "Epoch 11/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.7366 - accuracy: 0.7417 - val_loss: 0.7402 - val_accuracy: 0.7484 - lr: 5.0000e-04\n",
            "Epoch 12/30\n",
            "1250/1250 [==============================] - 21s 16ms/step - loss: 0.7154 - accuracy: 0.7504 - val_loss: 0.7394 - val_accuracy: 0.7413 - lr: 5.0000e-04\n",
            "Epoch 13/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.7077 - accuracy: 0.7543 - val_loss: 0.6663 - val_accuracy: 0.7709 - lr: 5.0000e-04\n",
            "Epoch 14/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.6938 - accuracy: 0.7564 - val_loss: 0.6587 - val_accuracy: 0.7744 - lr: 5.0000e-04\n",
            "Epoch 15/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 0.6877 - accuracy: 0.7623 - val_loss: 0.6661 - val_accuracy: 0.7743 - lr: 5.0000e-04\n",
            "Epoch 16/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.6831 - accuracy: 0.7630 - val_loss: 0.6398 - val_accuracy: 0.7777 - lr: 5.0000e-04\n",
            "Epoch 17/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.6694 - accuracy: 0.7663 - val_loss: 0.6509 - val_accuracy: 0.7764 - lr: 5.0000e-04\n",
            "Epoch 18/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.6652 - accuracy: 0.7675 - val_loss: 0.6845 - val_accuracy: 0.7677 - lr: 5.0000e-04\n",
            "Epoch 19/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.6578 - accuracy: 0.7705 - val_loss: 0.6329 - val_accuracy: 0.7823 - lr: 5.0000e-04\n",
            "Epoch 20/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.6482 - accuracy: 0.7709 - val_loss: 0.6474 - val_accuracy: 0.7789 - lr: 5.0000e-04\n",
            "Epoch 21/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.6011 - accuracy: 0.7894 - val_loss: 0.6066 - val_accuracy: 0.7926 - lr: 1.2500e-04\n",
            "Epoch 22/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.5946 - accuracy: 0.7917 - val_loss: 0.6064 - val_accuracy: 0.7958 - lr: 1.2500e-04\n",
            "Epoch 23/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.5857 - accuracy: 0.7955 - val_loss: 0.5941 - val_accuracy: 0.7929 - lr: 1.2500e-04\n",
            "Epoch 24/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.5843 - accuracy: 0.7954 - val_loss: 0.6123 - val_accuracy: 0.7919 - lr: 1.2500e-04\n",
            "Epoch 25/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.5833 - accuracy: 0.7976 - val_loss: 0.6101 - val_accuracy: 0.7909 - lr: 1.2500e-04\n",
            "Epoch 26/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.5848 - accuracy: 0.7962 - val_loss: 0.5990 - val_accuracy: 0.7984 - lr: 1.2500e-04\n",
            "Epoch 27/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.5830 - accuracy: 0.7986 - val_loss: 0.6180 - val_accuracy: 0.7902 - lr: 1.2500e-04\n",
            "Epoch 28/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.5708 - accuracy: 0.7997 - val_loss: 0.5974 - val_accuracy: 0.7964 - lr: 1.2500e-04\n",
            "Epoch 29/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.5736 - accuracy: 0.7982 - val_loss: 0.6005 - val_accuracy: 0.7937 - lr: 1.2500e-04\n",
            "Epoch 30/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 0.5719 - accuracy: 0.7994 - val_loss: 0.5975 - val_accuracy: 0.7960 - lr: 1.2500e-04\n",
            "Score for fold 4: loss of 0.5974644422531128; accuracy of 79.60000038146973%\n",
            "Model: \"sequential_24\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_72 (Conv2D)          (None, 32, 32, 32)        896       \n",
            "                                                                 \n",
            " max_pooling2d_24 (MaxPoolin  multiple                 0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_73 (Conv2D)          (None, 16, 16, 64)        18496     \n",
            "                                                                 \n",
            " dropout_24 (Dropout)        (None, 8, 8, 64)          0         \n",
            "                                                                 \n",
            " conv2d_74 (Conv2D)          (None, 8, 8, 128)         73856     \n",
            "                                                                 \n",
            " flatten_24 (Flatten)        (None, 2048)              0         \n",
            "                                                                 \n",
            " dense_24 (Dense)            (None, 10)                20490     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 113,738\n",
            "Trainable params: 113,738\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/30\n",
            "1250/1250 [==============================] - 20s 16ms/step - loss: 1.5801 - accuracy: 0.4271 - val_loss: 1.2330 - val_accuracy: 0.5640 - lr: 0.0010\n",
            "Epoch 2/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.2436 - accuracy: 0.5553 - val_loss: 1.1521 - val_accuracy: 0.5971 - lr: 0.0010\n",
            "Epoch 3/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.0923 - accuracy: 0.6151 - val_loss: 0.9385 - val_accuracy: 0.6782 - lr: 0.0010\n",
            "Epoch 4/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.9943 - accuracy: 0.6525 - val_loss: 0.8906 - val_accuracy: 0.6933 - lr: 0.0010\n",
            "Epoch 5/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.9329 - accuracy: 0.6774 - val_loss: 0.8746 - val_accuracy: 0.6995 - lr: 0.0010\n",
            "Epoch 6/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.8842 - accuracy: 0.6922 - val_loss: 0.7981 - val_accuracy: 0.7246 - lr: 0.0010\n",
            "Epoch 7/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.8558 - accuracy: 0.7020 - val_loss: 0.7884 - val_accuracy: 0.7303 - lr: 0.0010\n",
            "Epoch 8/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.8197 - accuracy: 0.7161 - val_loss: 0.7760 - val_accuracy: 0.7343 - lr: 0.0010\n",
            "Epoch 9/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.7968 - accuracy: 0.7226 - val_loss: 0.7297 - val_accuracy: 0.7533 - lr: 0.0010\n",
            "Epoch 10/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.7803 - accuracy: 0.7309 - val_loss: 0.7496 - val_accuracy: 0.7431 - lr: 0.0010\n",
            "Epoch 11/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.7049 - accuracy: 0.7558 - val_loss: 0.7278 - val_accuracy: 0.7521 - lr: 5.0000e-04\n",
            "Epoch 12/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.6859 - accuracy: 0.7599 - val_loss: 0.6766 - val_accuracy: 0.7716 - lr: 5.0000e-04\n",
            "Epoch 13/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.6721 - accuracy: 0.7659 - val_loss: 0.6953 - val_accuracy: 0.7669 - lr: 5.0000e-04\n",
            "Epoch 14/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.6671 - accuracy: 0.7684 - val_loss: 0.6660 - val_accuracy: 0.7715 - lr: 5.0000e-04\n",
            "Epoch 15/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.6597 - accuracy: 0.7713 - val_loss: 0.7089 - val_accuracy: 0.7620 - lr: 5.0000e-04\n",
            "Epoch 16/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 0.6449 - accuracy: 0.7770 - val_loss: 0.6872 - val_accuracy: 0.7685 - lr: 5.0000e-04\n",
            "Epoch 17/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.6385 - accuracy: 0.7751 - val_loss: 0.6415 - val_accuracy: 0.7823 - lr: 5.0000e-04\n",
            "Epoch 18/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.6264 - accuracy: 0.7825 - val_loss: 0.6263 - val_accuracy: 0.7833 - lr: 5.0000e-04\n",
            "Epoch 19/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.6263 - accuracy: 0.7806 - val_loss: 0.6225 - val_accuracy: 0.7892 - lr: 5.0000e-04\n",
            "Epoch 20/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.6163 - accuracy: 0.7886 - val_loss: 0.6344 - val_accuracy: 0.7835 - lr: 5.0000e-04\n",
            "Epoch 21/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.5666 - accuracy: 0.8047 - val_loss: 0.5996 - val_accuracy: 0.7978 - lr: 1.2500e-04\n",
            "Epoch 22/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.5603 - accuracy: 0.8041 - val_loss: 0.5869 - val_accuracy: 0.8003 - lr: 1.2500e-04\n",
            "Epoch 23/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.5617 - accuracy: 0.8055 - val_loss: 0.6017 - val_accuracy: 0.7962 - lr: 1.2500e-04\n",
            "Epoch 24/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.5541 - accuracy: 0.8058 - val_loss: 0.5998 - val_accuracy: 0.7976 - lr: 1.2500e-04\n",
            "Epoch 25/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.5542 - accuracy: 0.8074 - val_loss: 0.5896 - val_accuracy: 0.7995 - lr: 1.2500e-04\n",
            "Epoch 26/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 0.5503 - accuracy: 0.8079 - val_loss: 0.6056 - val_accuracy: 0.7974 - lr: 1.2500e-04\n",
            "Epoch 27/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.5450 - accuracy: 0.8121 - val_loss: 0.5977 - val_accuracy: 0.7998 - lr: 1.2500e-04\n",
            "Epoch 28/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.5469 - accuracy: 0.8091 - val_loss: 0.5959 - val_accuracy: 0.7983 - lr: 1.2500e-04\n",
            "Epoch 29/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.5457 - accuracy: 0.8101 - val_loss: 0.6086 - val_accuracy: 0.7967 - lr: 1.2500e-04\n",
            "Epoch 30/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.5411 - accuracy: 0.8118 - val_loss: 0.5999 - val_accuracy: 0.7971 - lr: 1.2500e-04\n",
            "Score for fold 5: loss of 0.5999082326889038; accuracy of 79.71000075340271%\n",
            "------------------------------------------------------------------------\n",
            "Score per fold\n",
            "------------------------------------------------------------------------\n",
            "> Fold 1 - Loss: 0.5948396325111389 - Accuracy: 80.15000224113464%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 2 - Loss: 0.5805827379226685 - Accuracy: 80.54999709129333%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 3 - Loss: 0.6093853712081909 - Accuracy: 79.40999865531921%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 4 - Loss: 0.5974644422531128 - Accuracy: 79.60000038146973%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 5 - Loss: 0.5999082326889038 - Accuracy: 79.71000075340271%\n",
            "------------------------------------------------------------------------\n",
            "Average scores for all folds:\n",
            "> Accuracy: 79.88399982452393 (+- 0.4122907017628801)\n",
            "> Loss: 0.596436083316803\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Learning_rate=1e-5 & Exponential decay"
      ],
      "metadata": {
        "id": "qhlFzjU5_Whk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "acc_per_fold = []\n",
        "loss_per_fold = []\n",
        "\n",
        "with tf.device('/device:GPU:0'):\n",
        "  fold_no = 1\n",
        "  for train, valid in kfold.split(trainX, trainy):\n",
        "\n",
        "    model = CNN_Model(batch_size=32, depth=3, hidden_neurons_1=32, hidden_neurons_2=64, hidden_neurons_3=128, kernel_size_1=3, kernel_size_2=3, kernel_size_3=3, \n",
        "               dropout_rate=0.4, batch_norm=False, pooling='max', layer_activation_1='relu', layer_activation_2='relu', layer_activation_3='relu',\n",
        "               kernel_initialiser='glorot_uniform', bias_initialiser='zeros', optimiser='Adam', weight_decay=None, learning_rate=1e-5, output_activation='softmax', SGD_momentum=0) \n",
        "  \n",
        "    train_gen = tf.keras.preprocessing.image.ImageDataGenerator(width_shift_range=0.1,\n",
        "      height_shift_range=0.1,\n",
        "      horizontal_flip=True)\n",
        "    train_gen.fit(trainX[train])\n",
        "  \n",
        "    print('------------------------------------------------------------------------')\n",
        "    print(f'Training for fold {fold_no} ...')\n",
        "\n",
        "    history = model.train(\n",
        "              train_gen,\n",
        "                trainX[train],\n",
        "                  trainy[train],\n",
        "                trainX[valid],\n",
        "                  trainy[valid])\n",
        "\n",
        "    scores = model.evaluate(trainX[valid], trainy[valid])\n",
        "\n",
        "    print(f'Score for fold {fold_no}: loss of {scores[0]}; accuracy of {scores[1]*100}%')\n",
        "    acc_per_fold.append(scores[1] * 100)\n",
        "    loss_per_fold.append(scores[0])\n",
        "\n",
        "    fold_no+=1\n",
        "\n",
        "# == Provide average scores ==\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Score per fold')\n",
        "for i in range(0, len(acc_per_fold)):\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Average scores for all folds:')\n",
        "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
        "print(f'> Loss: {np.mean(loss_per_fold)}')"
      ],
      "metadata": {
        "id": "iZZP1vaC_WTc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cfcbf7a9-b7be-407e-ef02-5fb6b31861b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_25\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_75 (Conv2D)          (None, 32, 32, 32)        896       \n",
            "                                                                 \n",
            " max_pooling2d_25 (MaxPoolin  multiple                 0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_76 (Conv2D)          (None, 16, 16, 64)        18496     \n",
            "                                                                 \n",
            " dropout_25 (Dropout)        (None, 8, 8, 64)          0         \n",
            "                                                                 \n",
            " conv2d_77 (Conv2D)          (None, 8, 8, 128)         73856     \n",
            "                                                                 \n",
            " flatten_25 (Flatten)        (None, 2048)              0         \n",
            "                                                                 \n",
            " dense_25 (Dense)            (None, 10)                20490     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 113,738\n",
            "Trainable params: 113,738\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 2.2815 - accuracy: 0.1376 - val_loss: 2.2358 - val_accuracy: 0.2239 - lr: 1.0000e-05\n",
            "Epoch 2/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 2.1259 - accuracy: 0.2418 - val_loss: 2.0272 - val_accuracy: 0.2812 - lr: 1.0000e-05\n",
            "Epoch 3/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 2.0086 - accuracy: 0.2822 - val_loss: 1.9609 - val_accuracy: 0.3100 - lr: 1.0000e-05\n",
            "Epoch 4/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.9636 - accuracy: 0.3036 - val_loss: 1.9105 - val_accuracy: 0.3351 - lr: 1.0000e-05\n",
            "Epoch 5/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 1.9164 - accuracy: 0.3254 - val_loss: 1.8569 - val_accuracy: 0.3616 - lr: 1.0000e-05\n",
            "Epoch 6/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.8733 - accuracy: 0.3441 - val_loss: 1.8124 - val_accuracy: 0.3718 - lr: 1.0000e-05\n",
            "Epoch 7/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.8362 - accuracy: 0.3556 - val_loss: 1.7802 - val_accuracy: 0.3765 - lr: 1.0000e-05\n",
            "Epoch 8/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.8063 - accuracy: 0.3657 - val_loss: 1.7543 - val_accuracy: 0.3870 - lr: 1.0000e-05\n",
            "Epoch 9/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 1.7785 - accuracy: 0.3728 - val_loss: 1.7274 - val_accuracy: 0.3938 - lr: 1.0000e-05\n",
            "Epoch 10/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.7577 - accuracy: 0.3759 - val_loss: 1.7121 - val_accuracy: 0.3966 - lr: 1.0000e-05\n",
            "Epoch 11/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.7368 - accuracy: 0.3873 - val_loss: 1.6923 - val_accuracy: 0.4035 - lr: 1.0000e-05\n",
            "Epoch 12/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.7191 - accuracy: 0.3891 - val_loss: 1.6771 - val_accuracy: 0.4100 - lr: 1.0000e-05\n",
            "Epoch 13/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.7021 - accuracy: 0.3929 - val_loss: 1.6763 - val_accuracy: 0.4101 - lr: 1.0000e-05\n",
            "Epoch 14/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.6888 - accuracy: 0.3992 - val_loss: 1.6402 - val_accuracy: 0.4189 - lr: 1.0000e-05\n",
            "Epoch 15/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.6759 - accuracy: 0.3999 - val_loss: 1.6417 - val_accuracy: 0.4182 - lr: 1.0000e-05\n",
            "Epoch 16/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.6620 - accuracy: 0.4073 - val_loss: 1.6160 - val_accuracy: 0.4318 - lr: 9.0484e-06\n",
            "Epoch 17/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.6518 - accuracy: 0.4079 - val_loss: 1.6206 - val_accuracy: 0.4278 - lr: 8.1873e-06\n",
            "Epoch 18/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.6428 - accuracy: 0.4112 - val_loss: 1.6036 - val_accuracy: 0.4314 - lr: 7.4082e-06\n",
            "Epoch 19/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.6360 - accuracy: 0.4130 - val_loss: 1.6141 - val_accuracy: 0.4278 - lr: 6.7032e-06\n",
            "Epoch 20/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 1.6285 - accuracy: 0.4160 - val_loss: 1.6112 - val_accuracy: 0.4295 - lr: 6.0653e-06\n",
            "Epoch 21/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 1.6234 - accuracy: 0.4194 - val_loss: 1.6035 - val_accuracy: 0.4283 - lr: 5.4881e-06\n",
            "Epoch 22/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.6193 - accuracy: 0.4191 - val_loss: 1.5904 - val_accuracy: 0.4377 - lr: 4.9659e-06\n",
            "Epoch 23/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.6145 - accuracy: 0.4193 - val_loss: 1.6000 - val_accuracy: 0.4310 - lr: 4.4933e-06\n",
            "Epoch 24/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.6079 - accuracy: 0.4225 - val_loss: 1.5857 - val_accuracy: 0.4374 - lr: 4.0657e-06\n",
            "Epoch 25/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.6063 - accuracy: 0.4248 - val_loss: 1.5949 - val_accuracy: 0.4316 - lr: 3.6788e-06\n",
            "Epoch 26/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.6034 - accuracy: 0.4249 - val_loss: 1.5762 - val_accuracy: 0.4389 - lr: 3.3287e-06\n",
            "Epoch 27/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.6020 - accuracy: 0.4237 - val_loss: 1.5701 - val_accuracy: 0.4438 - lr: 3.0119e-06\n",
            "Epoch 28/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.5980 - accuracy: 0.4279 - val_loss: 1.5790 - val_accuracy: 0.4402 - lr: 2.7253e-06\n",
            "Epoch 29/30\n",
            "1250/1250 [==============================] - 20s 16ms/step - loss: 1.5947 - accuracy: 0.4256 - val_loss: 1.5689 - val_accuracy: 0.4427 - lr: 2.4660e-06\n",
            "Epoch 30/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.5944 - accuracy: 0.4284 - val_loss: 1.5705 - val_accuracy: 0.4433 - lr: 2.2313e-06\n",
            "Score for fold 1: loss of 1.5704845190048218; accuracy of 44.33000087738037%\n",
            "Model: \"sequential_26\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_78 (Conv2D)          (None, 32, 32, 32)        896       \n",
            "                                                                 \n",
            " max_pooling2d_26 (MaxPoolin  multiple                 0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_79 (Conv2D)          (None, 16, 16, 64)        18496     \n",
            "                                                                 \n",
            " dropout_26 (Dropout)        (None, 8, 8, 64)          0         \n",
            "                                                                 \n",
            " conv2d_80 (Conv2D)          (None, 8, 8, 128)         73856     \n",
            "                                                                 \n",
            " flatten_26 (Flatten)        (None, 2048)              0         \n",
            "                                                                 \n",
            " dense_26 (Dense)            (None, 10)                20490     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 113,738\n",
            "Trainable params: 113,738\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/30\n",
            "1250/1250 [==============================] - 20s 15ms/step - loss: 2.2810 - accuracy: 0.1440 - val_loss: 2.2400 - val_accuracy: 0.1851 - lr: 1.0000e-05\n",
            "Epoch 2/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 2.1561 - accuracy: 0.2287 - val_loss: 2.0658 - val_accuracy: 0.2600 - lr: 1.0000e-05\n",
            "Epoch 3/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 2.0303 - accuracy: 0.2660 - val_loss: 1.9981 - val_accuracy: 0.2844 - lr: 1.0000e-05\n",
            "Epoch 4/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.9870 - accuracy: 0.2865 - val_loss: 1.9605 - val_accuracy: 0.3044 - lr: 1.0000e-05\n",
            "Epoch 5/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.9486 - accuracy: 0.3070 - val_loss: 1.9119 - val_accuracy: 0.3412 - lr: 1.0000e-05\n",
            "Epoch 6/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.9027 - accuracy: 0.3306 - val_loss: 1.8611 - val_accuracy: 0.3577 - lr: 1.0000e-05\n",
            "Epoch 7/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.8631 - accuracy: 0.3440 - val_loss: 1.8242 - val_accuracy: 0.3661 - lr: 1.0000e-05\n",
            "Epoch 8/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.8245 - accuracy: 0.3586 - val_loss: 1.7961 - val_accuracy: 0.3707 - lr: 1.0000e-05\n",
            "Epoch 9/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 1.7958 - accuracy: 0.3654 - val_loss: 1.7575 - val_accuracy: 0.3830 - lr: 1.0000e-05\n",
            "Epoch 10/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.7718 - accuracy: 0.3733 - val_loss: 1.7557 - val_accuracy: 0.3814 - lr: 1.0000e-05\n",
            "Epoch 11/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.7470 - accuracy: 0.3809 - val_loss: 1.7448 - val_accuracy: 0.3813 - lr: 1.0000e-05\n",
            "Epoch 12/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.7314 - accuracy: 0.3850 - val_loss: 1.7210 - val_accuracy: 0.3896 - lr: 1.0000e-05\n",
            "Epoch 13/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 1.7155 - accuracy: 0.3892 - val_loss: 1.7147 - val_accuracy: 0.3904 - lr: 1.0000e-05\n",
            "Epoch 14/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.6980 - accuracy: 0.3928 - val_loss: 1.6971 - val_accuracy: 0.3953 - lr: 1.0000e-05\n",
            "Epoch 15/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 1.6844 - accuracy: 0.3975 - val_loss: 1.6642 - val_accuracy: 0.4083 - lr: 1.0000e-05\n",
            "Epoch 16/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.6749 - accuracy: 0.4022 - val_loss: 1.6906 - val_accuracy: 0.3986 - lr: 9.0484e-06\n",
            "Epoch 17/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.6613 - accuracy: 0.4037 - val_loss: 1.6584 - val_accuracy: 0.4081 - lr: 8.1873e-06\n",
            "Epoch 18/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.6529 - accuracy: 0.4092 - val_loss: 1.6410 - val_accuracy: 0.4151 - lr: 7.4082e-06\n",
            "Epoch 19/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.6468 - accuracy: 0.4112 - val_loss: 1.6537 - val_accuracy: 0.4096 - lr: 6.7032e-06\n",
            "Epoch 20/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 1.6403 - accuracy: 0.4105 - val_loss: 1.6454 - val_accuracy: 0.4137 - lr: 6.0653e-06\n",
            "Epoch 21/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 1.6321 - accuracy: 0.4127 - val_loss: 1.6333 - val_accuracy: 0.4166 - lr: 5.4881e-06\n",
            "Epoch 22/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 1.6279 - accuracy: 0.4155 - val_loss: 1.6324 - val_accuracy: 0.4176 - lr: 4.9659e-06\n",
            "Epoch 23/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.6217 - accuracy: 0.4203 - val_loss: 1.6214 - val_accuracy: 0.4229 - lr: 4.4933e-06\n",
            "Epoch 24/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.6170 - accuracy: 0.4179 - val_loss: 1.6124 - val_accuracy: 0.4244 - lr: 4.0657e-06\n",
            "Epoch 25/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 1.6139 - accuracy: 0.4245 - val_loss: 1.6189 - val_accuracy: 0.4215 - lr: 3.6788e-06\n",
            "Epoch 26/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 1.6160 - accuracy: 0.4226 - val_loss: 1.6131 - val_accuracy: 0.4245 - lr: 3.3287e-06\n",
            "Epoch 27/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 1.6077 - accuracy: 0.4220 - val_loss: 1.6121 - val_accuracy: 0.4249 - lr: 3.0119e-06\n",
            "Epoch 28/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.6054 - accuracy: 0.4255 - val_loss: 1.6062 - val_accuracy: 0.4282 - lr: 2.7253e-06\n",
            "Epoch 29/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.6023 - accuracy: 0.4268 - val_loss: 1.6124 - val_accuracy: 0.4247 - lr: 2.4660e-06\n",
            "Epoch 30/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.6028 - accuracy: 0.4269 - val_loss: 1.6109 - val_accuracy: 0.4248 - lr: 2.2313e-06\n",
            "Score for fold 2: loss of 1.610882043838501; accuracy of 42.48000085353851%\n",
            "Model: \"sequential_27\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_81 (Conv2D)          (None, 32, 32, 32)        896       \n",
            "                                                                 \n",
            " max_pooling2d_27 (MaxPoolin  multiple                 0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_82 (Conv2D)          (None, 16, 16, 64)        18496     \n",
            "                                                                 \n",
            " dropout_27 (Dropout)        (None, 8, 8, 64)          0         \n",
            "                                                                 \n",
            " conv2d_83 (Conv2D)          (None, 8, 8, 128)         73856     \n",
            "                                                                 \n",
            " flatten_27 (Flatten)        (None, 2048)              0         \n",
            "                                                                 \n",
            " dense_27 (Dense)            (None, 10)                20490     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 113,738\n",
            "Trainable params: 113,738\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 2.2802 - accuracy: 0.1474 - val_loss: 2.2440 - val_accuracy: 0.2181 - lr: 1.0000e-05\n",
            "Epoch 2/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 2.1509 - accuracy: 0.2396 - val_loss: 2.0546 - val_accuracy: 0.2747 - lr: 1.0000e-05\n",
            "Epoch 3/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 2.0173 - accuracy: 0.2800 - val_loss: 1.9622 - val_accuracy: 0.2931 - lr: 1.0000e-05\n",
            "Epoch 4/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.9518 - accuracy: 0.3033 - val_loss: 1.8921 - val_accuracy: 0.3306 - lr: 1.0000e-05\n",
            "Epoch 5/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.8979 - accuracy: 0.3293 - val_loss: 1.8431 - val_accuracy: 0.3491 - lr: 1.0000e-05\n",
            "Epoch 6/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.8546 - accuracy: 0.3424 - val_loss: 1.8174 - val_accuracy: 0.3501 - lr: 1.0000e-05\n",
            "Epoch 7/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.8219 - accuracy: 0.3554 - val_loss: 1.7672 - val_accuracy: 0.3747 - lr: 1.0000e-05\n",
            "Epoch 8/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.7924 - accuracy: 0.3624 - val_loss: 1.7520 - val_accuracy: 0.3738 - lr: 1.0000e-05\n",
            "Epoch 9/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.7689 - accuracy: 0.3749 - val_loss: 1.7529 - val_accuracy: 0.3728 - lr: 1.0000e-05\n",
            "Epoch 10/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.7455 - accuracy: 0.3791 - val_loss: 1.7262 - val_accuracy: 0.3858 - lr: 1.0000e-05\n",
            "Epoch 11/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.7269 - accuracy: 0.3850 - val_loss: 1.7124 - val_accuracy: 0.3879 - lr: 1.0000e-05\n",
            "Epoch 12/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.7100 - accuracy: 0.3888 - val_loss: 1.7034 - val_accuracy: 0.3912 - lr: 1.0000e-05\n",
            "Epoch 13/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.6934 - accuracy: 0.3959 - val_loss: 1.6988 - val_accuracy: 0.3917 - lr: 1.0000e-05\n",
            "Epoch 14/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.6785 - accuracy: 0.3969 - val_loss: 1.6554 - val_accuracy: 0.4084 - lr: 1.0000e-05\n",
            "Epoch 15/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.6619 - accuracy: 0.4051 - val_loss: 1.6503 - val_accuracy: 0.4096 - lr: 1.0000e-05\n",
            "Epoch 16/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.6489 - accuracy: 0.4089 - val_loss: 1.6473 - val_accuracy: 0.4096 - lr: 9.0484e-06\n",
            "Epoch 17/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.6348 - accuracy: 0.4135 - val_loss: 1.6157 - val_accuracy: 0.4175 - lr: 8.1873e-06\n",
            "Epoch 18/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.6290 - accuracy: 0.4161 - val_loss: 1.6276 - val_accuracy: 0.4153 - lr: 7.4082e-06\n",
            "Epoch 19/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.6197 - accuracy: 0.4175 - val_loss: 1.6177 - val_accuracy: 0.4226 - lr: 6.7032e-06\n",
            "Epoch 20/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.6123 - accuracy: 0.4198 - val_loss: 1.6208 - val_accuracy: 0.4197 - lr: 6.0653e-06\n",
            "Epoch 21/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.6052 - accuracy: 0.4232 - val_loss: 1.5931 - val_accuracy: 0.4296 - lr: 5.4881e-06\n",
            "Epoch 22/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 1.5976 - accuracy: 0.4260 - val_loss: 1.6032 - val_accuracy: 0.4282 - lr: 4.9659e-06\n",
            "Epoch 23/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 1.5935 - accuracy: 0.4282 - val_loss: 1.5835 - val_accuracy: 0.4340 - lr: 4.4933e-06\n",
            "Epoch 24/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.5886 - accuracy: 0.4300 - val_loss: 1.5837 - val_accuracy: 0.4332 - lr: 4.0657e-06\n",
            "Epoch 25/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 1.5848 - accuracy: 0.4309 - val_loss: 1.5909 - val_accuracy: 0.4311 - lr: 3.6788e-06\n",
            "Epoch 26/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.5792 - accuracy: 0.4320 - val_loss: 1.5800 - val_accuracy: 0.4323 - lr: 3.3287e-06\n",
            "Epoch 27/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.5755 - accuracy: 0.4341 - val_loss: 1.5891 - val_accuracy: 0.4311 - lr: 3.0119e-06\n",
            "Epoch 28/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.5738 - accuracy: 0.4349 - val_loss: 1.5855 - val_accuracy: 0.4325 - lr: 2.7253e-06\n",
            "Epoch 29/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.5719 - accuracy: 0.4348 - val_loss: 1.5663 - val_accuracy: 0.4388 - lr: 2.4660e-06\n",
            "Epoch 30/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 1.5705 - accuracy: 0.4352 - val_loss: 1.5829 - val_accuracy: 0.4337 - lr: 2.2313e-06\n",
            "Score for fold 3: loss of 1.5829423666000366; accuracy of 43.36999952793121%\n",
            "Model: \"sequential_28\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_84 (Conv2D)          (None, 32, 32, 32)        896       \n",
            "                                                                 \n",
            " max_pooling2d_28 (MaxPoolin  multiple                 0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_85 (Conv2D)          (None, 16, 16, 64)        18496     \n",
            "                                                                 \n",
            " dropout_28 (Dropout)        (None, 8, 8, 64)          0         \n",
            "                                                                 \n",
            " conv2d_86 (Conv2D)          (None, 8, 8, 128)         73856     \n",
            "                                                                 \n",
            " flatten_28 (Flatten)        (None, 2048)              0         \n",
            "                                                                 \n",
            " dense_28 (Dense)            (None, 10)                20490     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 113,738\n",
            "Trainable params: 113,738\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/30\n",
            "1250/1250 [==============================] - 19s 14ms/step - loss: 2.2795 - accuracy: 0.1478 - val_loss: 2.2390 - val_accuracy: 0.2317 - lr: 1.0000e-05\n",
            "Epoch 2/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 2.1428 - accuracy: 0.2365 - val_loss: 2.0547 - val_accuracy: 0.2798 - lr: 1.0000e-05\n",
            "Epoch 3/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 2.0271 - accuracy: 0.2734 - val_loss: 1.9823 - val_accuracy: 0.3112 - lr: 1.0000e-05\n",
            "Epoch 4/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.9769 - accuracy: 0.2934 - val_loss: 1.9323 - val_accuracy: 0.3247 - lr: 1.0000e-05\n",
            "Epoch 5/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 1.9302 - accuracy: 0.3189 - val_loss: 1.8740 - val_accuracy: 0.3514 - lr: 1.0000e-05\n",
            "Epoch 6/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.8829 - accuracy: 0.3385 - val_loss: 1.8195 - val_accuracy: 0.3711 - lr: 1.0000e-05\n",
            "Epoch 7/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.8406 - accuracy: 0.3524 - val_loss: 1.7750 - val_accuracy: 0.3871 - lr: 1.0000e-05\n",
            "Epoch 8/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.8043 - accuracy: 0.3663 - val_loss: 1.7384 - val_accuracy: 0.3995 - lr: 1.0000e-05\n",
            "Epoch 9/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.7736 - accuracy: 0.3762 - val_loss: 1.7109 - val_accuracy: 0.4044 - lr: 1.0000e-05\n",
            "Epoch 10/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.7486 - accuracy: 0.3836 - val_loss: 1.6943 - val_accuracy: 0.4031 - lr: 1.0000e-05\n",
            "Epoch 11/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 1.7241 - accuracy: 0.3885 - val_loss: 1.6645 - val_accuracy: 0.4111 - lr: 1.0000e-05\n",
            "Epoch 12/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 1.7015 - accuracy: 0.3963 - val_loss: 1.6404 - val_accuracy: 0.4194 - lr: 1.0000e-05\n",
            "Epoch 13/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.6811 - accuracy: 0.4004 - val_loss: 1.6106 - val_accuracy: 0.4279 - lr: 1.0000e-05\n",
            "Epoch 14/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.6608 - accuracy: 0.4072 - val_loss: 1.6039 - val_accuracy: 0.4281 - lr: 1.0000e-05\n",
            "Epoch 15/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.6438 - accuracy: 0.4110 - val_loss: 1.5946 - val_accuracy: 0.4328 - lr: 1.0000e-05\n",
            "Epoch 16/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.6258 - accuracy: 0.4173 - val_loss: 1.5658 - val_accuracy: 0.4421 - lr: 9.0484e-06\n",
            "Epoch 17/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.6163 - accuracy: 0.4216 - val_loss: 1.5725 - val_accuracy: 0.4355 - lr: 8.1873e-06\n",
            "Epoch 18/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.6043 - accuracy: 0.4231 - val_loss: 1.5446 - val_accuracy: 0.4508 - lr: 7.4082e-06\n",
            "Epoch 19/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.5944 - accuracy: 0.4291 - val_loss: 1.5550 - val_accuracy: 0.4426 - lr: 6.7032e-06\n",
            "Epoch 20/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.5842 - accuracy: 0.4284 - val_loss: 1.5379 - val_accuracy: 0.4508 - lr: 6.0653e-06\n",
            "Epoch 21/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.5789 - accuracy: 0.4326 - val_loss: 1.5261 - val_accuracy: 0.4564 - lr: 5.4881e-06\n",
            "Epoch 22/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.5728 - accuracy: 0.4360 - val_loss: 1.5220 - val_accuracy: 0.4556 - lr: 4.9659e-06\n",
            "Epoch 23/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.5669 - accuracy: 0.4387 - val_loss: 1.5131 - val_accuracy: 0.4582 - lr: 4.4933e-06\n",
            "Epoch 24/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.5631 - accuracy: 0.4381 - val_loss: 1.5097 - val_accuracy: 0.4607 - lr: 4.0657e-06\n",
            "Epoch 25/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 1.5588 - accuracy: 0.4382 - val_loss: 1.5171 - val_accuracy: 0.4567 - lr: 3.6788e-06\n",
            "Epoch 26/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.5550 - accuracy: 0.4428 - val_loss: 1.5147 - val_accuracy: 0.4579 - lr: 3.3287e-06\n",
            "Epoch 27/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.5530 - accuracy: 0.4394 - val_loss: 1.4933 - val_accuracy: 0.4668 - lr: 3.0119e-06\n",
            "Epoch 28/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.5516 - accuracy: 0.4404 - val_loss: 1.5004 - val_accuracy: 0.4641 - lr: 2.7253e-06\n",
            "Epoch 29/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 1.5476 - accuracy: 0.4452 - val_loss: 1.4980 - val_accuracy: 0.4629 - lr: 2.4660e-06\n",
            "Epoch 30/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 1.5426 - accuracy: 0.4453 - val_loss: 1.4981 - val_accuracy: 0.4650 - lr: 2.2313e-06\n",
            "Score for fold 4: loss of 1.4980688095092773; accuracy of 46.50000035762787%\n",
            "Model: \"sequential_29\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_87 (Conv2D)          (None, 32, 32, 32)        896       \n",
            "                                                                 \n",
            " max_pooling2d_29 (MaxPoolin  multiple                 0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_88 (Conv2D)          (None, 16, 16, 64)        18496     \n",
            "                                                                 \n",
            " dropout_29 (Dropout)        (None, 8, 8, 64)          0         \n",
            "                                                                 \n",
            " conv2d_89 (Conv2D)          (None, 8, 8, 128)         73856     \n",
            "                                                                 \n",
            " flatten_29 (Flatten)        (None, 2048)              0         \n",
            "                                                                 \n",
            " dense_29 (Dense)            (None, 10)                20490     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 113,738\n",
            "Trainable params: 113,738\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 2.2617 - accuracy: 0.1534 - val_loss: 2.1961 - val_accuracy: 0.2347 - lr: 1.0000e-05\n",
            "Epoch 2/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 2.1000 - accuracy: 0.2421 - val_loss: 2.0282 - val_accuracy: 0.2886 - lr: 1.0000e-05\n",
            "Epoch 3/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 2.0071 - accuracy: 0.2738 - val_loss: 1.9658 - val_accuracy: 0.3164 - lr: 1.0000e-05\n",
            "Epoch 4/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.9632 - accuracy: 0.2970 - val_loss: 1.9141 - val_accuracy: 0.3276 - lr: 1.0000e-05\n",
            "Epoch 5/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.9144 - accuracy: 0.3174 - val_loss: 1.8609 - val_accuracy: 0.3452 - lr: 1.0000e-05\n",
            "Epoch 6/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.8650 - accuracy: 0.3390 - val_loss: 1.8075 - val_accuracy: 0.3627 - lr: 1.0000e-05\n",
            "Epoch 7/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.8220 - accuracy: 0.3552 - val_loss: 1.7730 - val_accuracy: 0.3743 - lr: 1.0000e-05\n",
            "Epoch 8/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.7874 - accuracy: 0.3670 - val_loss: 1.7668 - val_accuracy: 0.3714 - lr: 1.0000e-05\n",
            "Epoch 9/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.7599 - accuracy: 0.3739 - val_loss: 1.7324 - val_accuracy: 0.3828 - lr: 1.0000e-05\n",
            "Epoch 10/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.7340 - accuracy: 0.3832 - val_loss: 1.7104 - val_accuracy: 0.3874 - lr: 1.0000e-05\n",
            "Epoch 11/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.7090 - accuracy: 0.3911 - val_loss: 1.6915 - val_accuracy: 0.3937 - lr: 1.0000e-05\n",
            "Epoch 12/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.6934 - accuracy: 0.3952 - val_loss: 1.6694 - val_accuracy: 0.4008 - lr: 1.0000e-05\n",
            "Epoch 13/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.6732 - accuracy: 0.4017 - val_loss: 1.6683 - val_accuracy: 0.4008 - lr: 1.0000e-05\n",
            "Epoch 14/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.6559 - accuracy: 0.4076 - val_loss: 1.6596 - val_accuracy: 0.4058 - lr: 1.0000e-05\n",
            "Epoch 15/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.6397 - accuracy: 0.4123 - val_loss: 1.6198 - val_accuracy: 0.4159 - lr: 1.0000e-05\n",
            "Epoch 16/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 1.6272 - accuracy: 0.4179 - val_loss: 1.6301 - val_accuracy: 0.4161 - lr: 9.0484e-06\n",
            "Epoch 17/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.6143 - accuracy: 0.4197 - val_loss: 1.5935 - val_accuracy: 0.4285 - lr: 8.1873e-06\n",
            "Epoch 18/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.6053 - accuracy: 0.4268 - val_loss: 1.5923 - val_accuracy: 0.4295 - lr: 7.4082e-06\n",
            "Epoch 19/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.5941 - accuracy: 0.4259 - val_loss: 1.5738 - val_accuracy: 0.4346 - lr: 6.7032e-06\n",
            "Epoch 20/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.5858 - accuracy: 0.4306 - val_loss: 1.5922 - val_accuracy: 0.4282 - lr: 6.0653e-06\n",
            "Epoch 21/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.5800 - accuracy: 0.4326 - val_loss: 1.5743 - val_accuracy: 0.4354 - lr: 5.4881e-06\n",
            "Epoch 22/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.5741 - accuracy: 0.4338 - val_loss: 1.5712 - val_accuracy: 0.4353 - lr: 4.9659e-06\n",
            "Epoch 23/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.5719 - accuracy: 0.4338 - val_loss: 1.5673 - val_accuracy: 0.4377 - lr: 4.4933e-06\n",
            "Epoch 24/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.5645 - accuracy: 0.4363 - val_loss: 1.5699 - val_accuracy: 0.4369 - lr: 4.0657e-06\n",
            "Epoch 25/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.5592 - accuracy: 0.4397 - val_loss: 1.5623 - val_accuracy: 0.4405 - lr: 3.6788e-06\n",
            "Epoch 26/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.5562 - accuracy: 0.4414 - val_loss: 1.5419 - val_accuracy: 0.4467 - lr: 3.3287e-06\n",
            "Epoch 27/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.5561 - accuracy: 0.4406 - val_loss: 1.5602 - val_accuracy: 0.4399 - lr: 3.0119e-06\n",
            "Epoch 28/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.5504 - accuracy: 0.4435 - val_loss: 1.5589 - val_accuracy: 0.4402 - lr: 2.7253e-06\n",
            "Epoch 29/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 1.5482 - accuracy: 0.4455 - val_loss: 1.5433 - val_accuracy: 0.4449 - lr: 2.4660e-06\n",
            "Epoch 30/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.5466 - accuracy: 0.4448 - val_loss: 1.5531 - val_accuracy: 0.4421 - lr: 2.2313e-06\n",
            "Score for fold 5: loss of 1.5530766248703003; accuracy of 44.20999884605408%\n",
            "------------------------------------------------------------------------\n",
            "Score per fold\n",
            "------------------------------------------------------------------------\n",
            "> Fold 1 - Loss: 1.5704845190048218 - Accuracy: 44.33000087738037%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 2 - Loss: 1.610882043838501 - Accuracy: 42.48000085353851%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 3 - Loss: 1.5829423666000366 - Accuracy: 43.36999952793121%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 4 - Loss: 1.4980688095092773 - Accuracy: 46.50000035762787%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 5 - Loss: 1.5530766248703003 - Accuracy: 44.20999884605408%\n",
            "------------------------------------------------------------------------\n",
            "Average scores for all folds:\n",
            "> Accuracy: 44.17800009250641 (+- 1.3380493053654205)\n",
            "> Loss: 1.5630908727645874\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Learning_rate=1e-5 & No decay"
      ],
      "metadata": {
        "id": "8CUSG_zyvlxE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "acc_per_fold = []\n",
        "loss_per_fold = []\n",
        "\n",
        "with tf.device('/device:GPU:0'):\n",
        "  fold_no = 1\n",
        "  for train, valid in kfold.split(trainX, trainy):\n",
        "\n",
        "    model = CNN_Model(batch_size=32, depth=3, hidden_neurons_1=32, hidden_neurons_2=64, hidden_neurons_3=128, kernel_size_1=3, kernel_size_2=3, kernel_size_3=3, \n",
        "               dropout_rate=0.4, batch_norm=False, pooling='max', layer_activation_1='relu', layer_activation_2='relu', layer_activation_3='relu',\n",
        "               kernel_initialiser='glorot_uniform', bias_initialiser='zeros', optimiser='Adam', weight_decay=None, learning_rate=1e-5, output_activation='softmax', SGD_momentum=0) \n",
        "  \n",
        "    train_gen = tf.keras.preprocessing.image.ImageDataGenerator(width_shift_range=0.1,\n",
        "      height_shift_range=0.1,\n",
        "      horizontal_flip=True)\n",
        "    train_gen.fit(trainX[train])\n",
        "  \n",
        "    print('------------------------------------------------------------------------')\n",
        "    print(f'Training for fold {fold_no} ...')\n",
        "\n",
        "    history = model.train(\n",
        "              train_gen,\n",
        "                trainX[train],\n",
        "                  trainy[train],\n",
        "                trainX[valid],\n",
        "                  trainy[valid],\n",
        "                  lr_rate=False)\n",
        "\n",
        "    scores = model.evaluate(trainX[valid], trainy[valid])\n",
        "\n",
        "    print(f'Score for fold {fold_no}: loss of {scores[0]}; accuracy of {scores[1]*100}%')\n",
        "    acc_per_fold.append(scores[1] * 100)\n",
        "    loss_per_fold.append(scores[0])\n",
        "\n",
        "    fold_no+=1\n",
        "\n",
        "# == Provide average scores ==\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Score per fold')\n",
        "for i in range(0, len(acc_per_fold)):\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Average scores for all folds:')\n",
        "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
        "print(f'> Loss: {np.mean(loss_per_fold)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "osf7QBpvYkAo",
        "outputId": "c7cb52bb-dded-483b-c722-548b068d95a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_30\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_90 (Conv2D)          (None, 32, 32, 32)        896       \n",
            "                                                                 \n",
            " max_pooling2d_30 (MaxPoolin  multiple                 0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_91 (Conv2D)          (None, 16, 16, 64)        18496     \n",
            "                                                                 \n",
            " dropout_30 (Dropout)        (None, 8, 8, 64)          0         \n",
            "                                                                 \n",
            " conv2d_92 (Conv2D)          (None, 8, 8, 128)         73856     \n",
            "                                                                 \n",
            " flatten_30 (Flatten)        (None, 2048)              0         \n",
            "                                                                 \n",
            " dense_30 (Dense)            (None, 10)                20490     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 113,738\n",
            "Trainable params: 113,738\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 2.2849 - accuracy: 0.1335 - val_loss: 2.2587 - val_accuracy: 0.1750\n",
            "Epoch 2/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 2.1764 - accuracy: 0.2237 - val_loss: 2.0937 - val_accuracy: 0.2866\n",
            "Epoch 3/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 2.0348 - accuracy: 0.2709 - val_loss: 2.0041 - val_accuracy: 0.2954\n",
            "Epoch 4/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 1.9780 - accuracy: 0.2946 - val_loss: 1.9401 - val_accuracy: 0.3313\n",
            "Epoch 5/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.9270 - accuracy: 0.3186 - val_loss: 1.8822 - val_accuracy: 0.3471\n",
            "Epoch 6/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.8826 - accuracy: 0.3376 - val_loss: 1.8393 - val_accuracy: 0.3561\n",
            "Epoch 7/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.8459 - accuracy: 0.3485 - val_loss: 1.8047 - val_accuracy: 0.3700\n",
            "Epoch 8/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.8162 - accuracy: 0.3567 - val_loss: 1.7901 - val_accuracy: 0.3709\n",
            "Epoch 9/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.7895 - accuracy: 0.3645 - val_loss: 1.7699 - val_accuracy: 0.3745\n",
            "Epoch 10/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.7654 - accuracy: 0.3718 - val_loss: 1.7416 - val_accuracy: 0.3871\n",
            "Epoch 11/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.7426 - accuracy: 0.3796 - val_loss: 1.7186 - val_accuracy: 0.3948\n",
            "Epoch 12/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.7215 - accuracy: 0.3875 - val_loss: 1.7027 - val_accuracy: 0.4002\n",
            "Epoch 13/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.7085 - accuracy: 0.3888 - val_loss: 1.7096 - val_accuracy: 0.3935\n",
            "Epoch 14/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.6928 - accuracy: 0.3952 - val_loss: 1.6740 - val_accuracy: 0.4118\n",
            "Epoch 15/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.6739 - accuracy: 0.4010 - val_loss: 1.6688 - val_accuracy: 0.4072\n",
            "Epoch 16/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.6614 - accuracy: 0.4046 - val_loss: 1.6695 - val_accuracy: 0.4083\n",
            "Epoch 17/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.6433 - accuracy: 0.4098 - val_loss: 1.6496 - val_accuracy: 0.4148\n",
            "Epoch 18/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.6317 - accuracy: 0.4160 - val_loss: 1.6257 - val_accuracy: 0.4188\n",
            "Epoch 19/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.6181 - accuracy: 0.4195 - val_loss: 1.6016 - val_accuracy: 0.4305\n",
            "Epoch 20/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.6076 - accuracy: 0.4243 - val_loss: 1.6070 - val_accuracy: 0.4267\n",
            "Epoch 21/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.5977 - accuracy: 0.4266 - val_loss: 1.6163 - val_accuracy: 0.4219\n",
            "Epoch 22/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 1.5896 - accuracy: 0.4290 - val_loss: 1.5787 - val_accuracy: 0.4385\n",
            "Epoch 23/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 1.5751 - accuracy: 0.4344 - val_loss: 1.5774 - val_accuracy: 0.4369\n",
            "Epoch 24/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.5687 - accuracy: 0.4369 - val_loss: 1.5769 - val_accuracy: 0.4409\n",
            "Epoch 25/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.5554 - accuracy: 0.4422 - val_loss: 1.5793 - val_accuracy: 0.4422\n",
            "Epoch 26/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.5522 - accuracy: 0.4434 - val_loss: 1.5547 - val_accuracy: 0.4486\n",
            "Epoch 27/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.5402 - accuracy: 0.4456 - val_loss: 1.5506 - val_accuracy: 0.4523\n",
            "Epoch 28/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 1.5329 - accuracy: 0.4494 - val_loss: 1.5461 - val_accuracy: 0.4498\n",
            "Epoch 29/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.5250 - accuracy: 0.4513 - val_loss: 1.5150 - val_accuracy: 0.4602\n",
            "Epoch 30/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.5181 - accuracy: 0.4547 - val_loss: 1.5299 - val_accuracy: 0.4585\n",
            "Score for fold 1: loss of 1.5298523902893066; accuracy of 45.84999978542328%\n",
            "Model: \"sequential_31\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_93 (Conv2D)          (None, 32, 32, 32)        896       \n",
            "                                                                 \n",
            " max_pooling2d_31 (MaxPoolin  multiple                 0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_94 (Conv2D)          (None, 16, 16, 64)        18496     \n",
            "                                                                 \n",
            " dropout_31 (Dropout)        (None, 8, 8, 64)          0         \n",
            "                                                                 \n",
            " conv2d_95 (Conv2D)          (None, 8, 8, 128)         73856     \n",
            "                                                                 \n",
            " flatten_31 (Flatten)        (None, 2048)              0         \n",
            "                                                                 \n",
            " dense_31 (Dense)            (None, 10)                20490     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 113,738\n",
            "Trainable params: 113,738\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 2.2773 - accuracy: 0.1426 - val_loss: 2.2372 - val_accuracy: 0.2155\n",
            "Epoch 2/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 2.1622 - accuracy: 0.2225 - val_loss: 2.0714 - val_accuracy: 0.2903\n",
            "Epoch 3/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 2.0284 - accuracy: 0.2688 - val_loss: 1.9625 - val_accuracy: 0.3206\n",
            "Epoch 4/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.9564 - accuracy: 0.3036 - val_loss: 1.8979 - val_accuracy: 0.3452\n",
            "Epoch 5/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.9010 - accuracy: 0.3241 - val_loss: 1.8385 - val_accuracy: 0.3622\n",
            "Epoch 6/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.8547 - accuracy: 0.3444 - val_loss: 1.8079 - val_accuracy: 0.3663\n",
            "Epoch 7/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.8209 - accuracy: 0.3526 - val_loss: 1.7718 - val_accuracy: 0.3767\n",
            "Epoch 8/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.7922 - accuracy: 0.3656 - val_loss: 1.7553 - val_accuracy: 0.3799\n",
            "Epoch 9/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 1.7695 - accuracy: 0.3710 - val_loss: 1.7309 - val_accuracy: 0.3846\n",
            "Epoch 10/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.7453 - accuracy: 0.3804 - val_loss: 1.7220 - val_accuracy: 0.3854\n",
            "Epoch 11/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.7241 - accuracy: 0.3846 - val_loss: 1.6879 - val_accuracy: 0.3932\n",
            "Epoch 12/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.7046 - accuracy: 0.3891 - val_loss: 1.6752 - val_accuracy: 0.3969\n",
            "Epoch 13/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.6861 - accuracy: 0.3971 - val_loss: 1.6572 - val_accuracy: 0.4031\n",
            "Epoch 14/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.6723 - accuracy: 0.4004 - val_loss: 1.6708 - val_accuracy: 0.4015\n",
            "Epoch 15/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.6562 - accuracy: 0.4074 - val_loss: 1.6298 - val_accuracy: 0.4114\n",
            "Epoch 16/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.6428 - accuracy: 0.4084 - val_loss: 1.6290 - val_accuracy: 0.4088\n",
            "Epoch 17/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.6294 - accuracy: 0.4152 - val_loss: 1.6109 - val_accuracy: 0.4156\n",
            "Epoch 18/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.6157 - accuracy: 0.4189 - val_loss: 1.6154 - val_accuracy: 0.4197\n",
            "Epoch 19/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.6030 - accuracy: 0.4236 - val_loss: 1.6075 - val_accuracy: 0.4179\n",
            "Epoch 20/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.5924 - accuracy: 0.4267 - val_loss: 1.5531 - val_accuracy: 0.4398\n",
            "Epoch 21/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.5823 - accuracy: 0.4304 - val_loss: 1.5586 - val_accuracy: 0.4356\n",
            "Epoch 22/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.5751 - accuracy: 0.4333 - val_loss: 1.5503 - val_accuracy: 0.4394\n",
            "Epoch 23/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.5633 - accuracy: 0.4381 - val_loss: 1.5466 - val_accuracy: 0.4430\n",
            "Epoch 24/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.5538 - accuracy: 0.4397 - val_loss: 1.5151 - val_accuracy: 0.4534\n",
            "Epoch 25/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.5433 - accuracy: 0.4442 - val_loss: 1.5211 - val_accuracy: 0.4467\n",
            "Epoch 26/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.5358 - accuracy: 0.4472 - val_loss: 1.5108 - val_accuracy: 0.4539\n",
            "Epoch 27/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.5291 - accuracy: 0.4499 - val_loss: 1.4953 - val_accuracy: 0.4614\n",
            "Epoch 28/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.5232 - accuracy: 0.4523 - val_loss: 1.4845 - val_accuracy: 0.4648\n",
            "Epoch 29/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.5178 - accuracy: 0.4558 - val_loss: 1.4968 - val_accuracy: 0.4624\n",
            "Epoch 30/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.5063 - accuracy: 0.4586 - val_loss: 1.4710 - val_accuracy: 0.4681\n",
            "Score for fold 2: loss of 1.471041202545166; accuracy of 46.81000113487244%\n",
            "Model: \"sequential_32\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_96 (Conv2D)          (None, 32, 32, 32)        896       \n",
            "                                                                 \n",
            " max_pooling2d_32 (MaxPoolin  multiple                 0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_97 (Conv2D)          (None, 16, 16, 64)        18496     \n",
            "                                                                 \n",
            " dropout_32 (Dropout)        (None, 8, 8, 64)          0         \n",
            "                                                                 \n",
            " conv2d_98 (Conv2D)          (None, 8, 8, 128)         73856     \n",
            "                                                                 \n",
            " flatten_32 (Flatten)        (None, 2048)              0         \n",
            "                                                                 \n",
            " dense_32 (Dense)            (None, 10)                20490     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 113,738\n",
            "Trainable params: 113,738\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 2.2825 - accuracy: 0.1370 - val_loss: 2.2534 - val_accuracy: 0.1934\n",
            "Epoch 2/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 2.1784 - accuracy: 0.2081 - val_loss: 2.0887 - val_accuracy: 0.2769\n",
            "Epoch 3/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 2.0404 - accuracy: 0.2641 - val_loss: 1.9764 - val_accuracy: 0.3121\n",
            "Epoch 4/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.9790 - accuracy: 0.2914 - val_loss: 1.9208 - val_accuracy: 0.3260\n",
            "Epoch 5/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.9401 - accuracy: 0.3121 - val_loss: 1.8722 - val_accuracy: 0.3498\n",
            "Epoch 6/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.8945 - accuracy: 0.3294 - val_loss: 1.8387 - val_accuracy: 0.3535\n",
            "Epoch 7/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.8545 - accuracy: 0.3449 - val_loss: 1.7864 - val_accuracy: 0.3811\n",
            "Epoch 8/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.8212 - accuracy: 0.3541 - val_loss: 1.7713 - val_accuracy: 0.3766\n",
            "Epoch 9/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.7911 - accuracy: 0.3670 - val_loss: 1.7461 - val_accuracy: 0.3860\n",
            "Epoch 10/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.7665 - accuracy: 0.3742 - val_loss: 1.7264 - val_accuracy: 0.3911\n",
            "Epoch 11/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.7445 - accuracy: 0.3788 - val_loss: 1.7078 - val_accuracy: 0.3971\n",
            "Epoch 12/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.7221 - accuracy: 0.3870 - val_loss: 1.6973 - val_accuracy: 0.3987\n",
            "Epoch 13/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 1.7036 - accuracy: 0.3916 - val_loss: 1.6818 - val_accuracy: 0.4050\n",
            "Epoch 14/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 1.6857 - accuracy: 0.3977 - val_loss: 1.6947 - val_accuracy: 0.3990\n",
            "Epoch 15/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 1.6693 - accuracy: 0.4038 - val_loss: 1.6837 - val_accuracy: 0.4043\n",
            "Epoch 16/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.6570 - accuracy: 0.4066 - val_loss: 1.6120 - val_accuracy: 0.4296\n",
            "Epoch 17/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.6448 - accuracy: 0.4085 - val_loss: 1.6227 - val_accuracy: 0.4239\n",
            "Epoch 18/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 1.6321 - accuracy: 0.4157 - val_loss: 1.6307 - val_accuracy: 0.4227\n",
            "Epoch 19/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 1.6210 - accuracy: 0.4182 - val_loss: 1.5970 - val_accuracy: 0.4375\n",
            "Epoch 20/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 1.6112 - accuracy: 0.4229 - val_loss: 1.5767 - val_accuracy: 0.4409\n",
            "Epoch 21/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.6009 - accuracy: 0.4257 - val_loss: 1.5663 - val_accuracy: 0.4436\n",
            "Epoch 22/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.5855 - accuracy: 0.4324 - val_loss: 1.5430 - val_accuracy: 0.4514\n",
            "Epoch 23/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.5776 - accuracy: 0.4353 - val_loss: 1.5419 - val_accuracy: 0.4498\n",
            "Epoch 24/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.5722 - accuracy: 0.4353 - val_loss: 1.5683 - val_accuracy: 0.4443\n",
            "Epoch 25/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.5604 - accuracy: 0.4403 - val_loss: 1.5486 - val_accuracy: 0.4502\n",
            "Epoch 26/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.5522 - accuracy: 0.4432 - val_loss: 1.5618 - val_accuracy: 0.4474\n",
            "Epoch 27/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.5471 - accuracy: 0.4448 - val_loss: 1.5189 - val_accuracy: 0.4588\n",
            "Epoch 28/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.5411 - accuracy: 0.4435 - val_loss: 1.4978 - val_accuracy: 0.4684\n",
            "Epoch 29/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.5337 - accuracy: 0.4494 - val_loss: 1.5078 - val_accuracy: 0.4648\n",
            "Epoch 30/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.5253 - accuracy: 0.4543 - val_loss: 1.5290 - val_accuracy: 0.4546\n",
            "Score for fold 3: loss of 1.5290347337722778; accuracy of 45.46000063419342%\n",
            "Model: \"sequential_33\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_99 (Conv2D)          (None, 32, 32, 32)        896       \n",
            "                                                                 \n",
            " max_pooling2d_33 (MaxPoolin  multiple                 0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_100 (Conv2D)         (None, 16, 16, 64)        18496     \n",
            "                                                                 \n",
            " dropout_33 (Dropout)        (None, 8, 8, 64)          0         \n",
            "                                                                 \n",
            " conv2d_101 (Conv2D)         (None, 8, 8, 128)         73856     \n",
            "                                                                 \n",
            " flatten_33 (Flatten)        (None, 2048)              0         \n",
            "                                                                 \n",
            " dense_33 (Dense)            (None, 10)                20490     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 113,738\n",
            "Trainable params: 113,738\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 2.2716 - accuracy: 0.1470 - val_loss: 2.2230 - val_accuracy: 0.2243\n",
            "Epoch 2/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 2.1326 - accuracy: 0.2327 - val_loss: 2.0581 - val_accuracy: 0.2600\n",
            "Epoch 3/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 2.0261 - accuracy: 0.2726 - val_loss: 1.9883 - val_accuracy: 0.3046\n",
            "Epoch 4/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.9735 - accuracy: 0.2977 - val_loss: 1.9370 - val_accuracy: 0.3218\n",
            "Epoch 5/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.9335 - accuracy: 0.3176 - val_loss: 1.8890 - val_accuracy: 0.3433\n",
            "Epoch 6/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.8915 - accuracy: 0.3357 - val_loss: 1.8497 - val_accuracy: 0.3519\n",
            "Epoch 7/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.8549 - accuracy: 0.3464 - val_loss: 1.8085 - val_accuracy: 0.3635\n",
            "Epoch 8/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.8194 - accuracy: 0.3622 - val_loss: 1.7887 - val_accuracy: 0.3660\n",
            "Epoch 9/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.7876 - accuracy: 0.3706 - val_loss: 1.7641 - val_accuracy: 0.3739\n",
            "Epoch 10/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.7629 - accuracy: 0.3767 - val_loss: 1.7596 - val_accuracy: 0.3697\n",
            "Epoch 11/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.7391 - accuracy: 0.3866 - val_loss: 1.7273 - val_accuracy: 0.3811\n",
            "Epoch 12/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.7171 - accuracy: 0.3910 - val_loss: 1.7063 - val_accuracy: 0.3830\n",
            "Epoch 13/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.7005 - accuracy: 0.3948 - val_loss: 1.6794 - val_accuracy: 0.3964\n",
            "Epoch 14/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.6823 - accuracy: 0.4005 - val_loss: 1.6765 - val_accuracy: 0.3949\n",
            "Epoch 15/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.6692 - accuracy: 0.4044 - val_loss: 1.6486 - val_accuracy: 0.4106\n",
            "Epoch 16/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.6526 - accuracy: 0.4102 - val_loss: 1.6448 - val_accuracy: 0.4103\n",
            "Epoch 17/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.6390 - accuracy: 0.4119 - val_loss: 1.6398 - val_accuracy: 0.4124\n",
            "Epoch 18/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.6231 - accuracy: 0.4167 - val_loss: 1.6145 - val_accuracy: 0.4219\n",
            "Epoch 19/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.6115 - accuracy: 0.4211 - val_loss: 1.6122 - val_accuracy: 0.4245\n",
            "Epoch 20/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.6054 - accuracy: 0.4231 - val_loss: 1.5804 - val_accuracy: 0.4307\n",
            "Epoch 21/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.5911 - accuracy: 0.4280 - val_loss: 1.5883 - val_accuracy: 0.4314\n",
            "Epoch 22/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.5790 - accuracy: 0.4325 - val_loss: 1.5828 - val_accuracy: 0.4339\n",
            "Epoch 23/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.5722 - accuracy: 0.4331 - val_loss: 1.5366 - val_accuracy: 0.4440\n",
            "Epoch 24/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.5629 - accuracy: 0.4391 - val_loss: 1.5484 - val_accuracy: 0.4459\n",
            "Epoch 25/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.5567 - accuracy: 0.4379 - val_loss: 1.5431 - val_accuracy: 0.4483\n",
            "Epoch 26/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.5461 - accuracy: 0.4429 - val_loss: 1.5215 - val_accuracy: 0.4544\n",
            "Epoch 27/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 1.5392 - accuracy: 0.4476 - val_loss: 1.5119 - val_accuracy: 0.4561\n",
            "Epoch 28/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.5277 - accuracy: 0.4522 - val_loss: 1.5342 - val_accuracy: 0.4502\n",
            "Epoch 29/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.5230 - accuracy: 0.4527 - val_loss: 1.5299 - val_accuracy: 0.4532\n",
            "Epoch 30/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.5198 - accuracy: 0.4526 - val_loss: 1.5076 - val_accuracy: 0.4598\n",
            "Score for fold 4: loss of 1.5075923204421997; accuracy of 45.980000495910645%\n",
            "Model: \"sequential_34\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_102 (Conv2D)         (None, 32, 32, 32)        896       \n",
            "                                                                 \n",
            " max_pooling2d_34 (MaxPoolin  multiple                 0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_103 (Conv2D)         (None, 16, 16, 64)        18496     \n",
            "                                                                 \n",
            " dropout_34 (Dropout)        (None, 8, 8, 64)          0         \n",
            "                                                                 \n",
            " conv2d_104 (Conv2D)         (None, 8, 8, 128)         73856     \n",
            "                                                                 \n",
            " flatten_34 (Flatten)        (None, 2048)              0         \n",
            "                                                                 \n",
            " dense_34 (Dense)            (None, 10)                20490     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 113,738\n",
            "Trainable params: 113,738\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 2.2873 - accuracy: 0.1315 - val_loss: 2.2567 - val_accuracy: 0.2215\n",
            "Epoch 2/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 2.1865 - accuracy: 0.2216 - val_loss: 2.1033 - val_accuracy: 0.2605\n",
            "Epoch 3/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 2.0396 - accuracy: 0.2716 - val_loss: 1.9920 - val_accuracy: 0.2950\n",
            "Epoch 4/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.9749 - accuracy: 0.2910 - val_loss: 1.9362 - val_accuracy: 0.3161\n",
            "Epoch 5/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.9283 - accuracy: 0.3144 - val_loss: 1.8820 - val_accuracy: 0.3410\n",
            "Epoch 6/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 1.8835 - accuracy: 0.3345 - val_loss: 1.8372 - val_accuracy: 0.3580\n",
            "Epoch 7/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 1.8493 - accuracy: 0.3469 - val_loss: 1.7991 - val_accuracy: 0.3710\n",
            "Epoch 8/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.8191 - accuracy: 0.3579 - val_loss: 1.7840 - val_accuracy: 0.3731\n",
            "Epoch 9/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 1.7928 - accuracy: 0.3662 - val_loss: 1.7650 - val_accuracy: 0.3750\n",
            "Epoch 10/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 1.7691 - accuracy: 0.3733 - val_loss: 1.7372 - val_accuracy: 0.3887\n",
            "Epoch 11/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.7481 - accuracy: 0.3801 - val_loss: 1.7063 - val_accuracy: 0.3996\n",
            "Epoch 12/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.7283 - accuracy: 0.3871 - val_loss: 1.6843 - val_accuracy: 0.4015\n",
            "Epoch 13/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.7113 - accuracy: 0.3907 - val_loss: 1.6727 - val_accuracy: 0.4087\n",
            "Epoch 14/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.6920 - accuracy: 0.3987 - val_loss: 1.6618 - val_accuracy: 0.4112\n",
            "Epoch 15/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 1.6762 - accuracy: 0.4020 - val_loss: 1.6424 - val_accuracy: 0.4160\n",
            "Epoch 16/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.6633 - accuracy: 0.4087 - val_loss: 1.6468 - val_accuracy: 0.4175\n",
            "Epoch 17/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.6455 - accuracy: 0.4127 - val_loss: 1.6289 - val_accuracy: 0.4193\n",
            "Epoch 18/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.6345 - accuracy: 0.4135 - val_loss: 1.6073 - val_accuracy: 0.4247\n",
            "Epoch 19/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.6235 - accuracy: 0.4162 - val_loss: 1.6305 - val_accuracy: 0.4203\n",
            "Epoch 20/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.6051 - accuracy: 0.4252 - val_loss: 1.6101 - val_accuracy: 0.4265\n",
            "Epoch 21/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 1.5942 - accuracy: 0.4297 - val_loss: 1.5728 - val_accuracy: 0.4356\n",
            "Epoch 22/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.5831 - accuracy: 0.4297 - val_loss: 1.5739 - val_accuracy: 0.4375\n",
            "Epoch 23/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.5742 - accuracy: 0.4373 - val_loss: 1.5506 - val_accuracy: 0.4469\n",
            "Epoch 24/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.5629 - accuracy: 0.4401 - val_loss: 1.5509 - val_accuracy: 0.4422\n",
            "Epoch 25/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.5527 - accuracy: 0.4410 - val_loss: 1.5269 - val_accuracy: 0.4471\n",
            "Epoch 26/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 1.5440 - accuracy: 0.4455 - val_loss: 1.5028 - val_accuracy: 0.4593\n",
            "Epoch 27/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 1.5376 - accuracy: 0.4460 - val_loss: 1.5164 - val_accuracy: 0.4552\n",
            "Epoch 28/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.5295 - accuracy: 0.4518 - val_loss: 1.5183 - val_accuracy: 0.4538\n",
            "Epoch 29/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.5199 - accuracy: 0.4555 - val_loss: 1.4997 - val_accuracy: 0.4599\n",
            "Epoch 30/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.5120 - accuracy: 0.4588 - val_loss: 1.5174 - val_accuracy: 0.4544\n",
            "Score for fold 5: loss of 1.5173509120941162; accuracy of 45.44000029563904%\n",
            "------------------------------------------------------------------------\n",
            "Score per fold\n",
            "------------------------------------------------------------------------\n",
            "> Fold 1 - Loss: 1.5298523902893066 - Accuracy: 45.84999978542328%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 2 - Loss: 1.471041202545166 - Accuracy: 46.81000113487244%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 3 - Loss: 1.5290347337722778 - Accuracy: 45.46000063419342%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 4 - Loss: 1.5075923204421997 - Accuracy: 45.980000495910645%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 5 - Loss: 1.5173509120941162 - Accuracy: 45.44000029563904%\n",
            "------------------------------------------------------------------------\n",
            "Average scores for all folds:\n",
            "> Accuracy: 45.908000469207764 (+- 0.49837361460202373)\n",
            "> Loss: 1.5109743118286132\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Learning_rate=1e-5 & Step decay"
      ],
      "metadata": {
        "id": "dfUcXbYavpby"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# K-fold Cross Validation model evaluation\n",
        "acc_per_fold = []\n",
        "loss_per_fold = []\n",
        "\n",
        "with tf.device('/device:GPU:0'):\n",
        "  fold_no = 1\n",
        "  for train, valid in kfold.split(trainX, trainy):\n",
        "\n",
        "    model = CNN_Model(batch_size=32, depth=3, hidden_neurons_1=32, hidden_neurons_2=64, hidden_neurons_3=128, kernel_size_1=3, kernel_size_2=3, kernel_size_3=3, \n",
        "               dropout_rate=0.2, batch_norm=False, pooling='max', layer_activation_1='relu', layer_activation_2='relu', layer_activation_3='relu',\n",
        "               kernel_initialiser='glorot_uniform', bias_initialiser='zeros', optimiser='Adam', weight_decay=None, learning_rate=1e-5, output_activation='softmax', SGD_momentum=0) \n",
        "  \n",
        "    train_gen = tf.keras.preprocessing.image.ImageDataGenerator(width_shift_range=0.1,\n",
        "      height_shift_range=0.1,\n",
        "      horizontal_flip=True)\n",
        "    train_gen.fit(trainX[train])\n",
        "  \n",
        "    print('------------------------------------------------------------------------')\n",
        "    print(f'Training for fold {fold_no} ...')\n",
        "\n",
        "    history = model.train(\n",
        "              train_gen,\n",
        "                trainX[train],\n",
        "                  trainy[train],\n",
        "                trainX[valid],\n",
        "                  trainy[valid],\n",
        "                  lr_rate_type='step')\n",
        "\n",
        "    scores = model.evaluate(trainX[valid], trainy[valid])\n",
        "\n",
        "    print(f'Score for fold {fold_no}: loss of {scores[0]}; accuracy of {scores[1]*100}%')\n",
        "    acc_per_fold.append(scores[1] * 100)\n",
        "    loss_per_fold.append(scores[0])\n",
        "\n",
        "    fold_no+=1\n",
        "\n",
        "# == Provide average scores ==\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Score per fold')\n",
        "for i in range(0, len(acc_per_fold)):\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Average scores for all folds:')\n",
        "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
        "print(f'> Loss: {np.mean(loss_per_fold)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qCtlxQjnvq6K",
        "outputId": "7a32cbd6-f65e-4a59-9086-041c201616b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_35\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_105 (Conv2D)         (None, 32, 32, 32)        896       \n",
            "                                                                 \n",
            " max_pooling2d_35 (MaxPoolin  multiple                 0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_106 (Conv2D)         (None, 16, 16, 64)        18496     \n",
            "                                                                 \n",
            " dropout_35 (Dropout)        (None, 8, 8, 64)          0         \n",
            "                                                                 \n",
            " conv2d_107 (Conv2D)         (None, 8, 8, 128)         73856     \n",
            "                                                                 \n",
            " flatten_35 (Flatten)        (None, 2048)              0         \n",
            "                                                                 \n",
            " dense_35 (Dense)            (None, 10)                20490     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 113,738\n",
            "Trainable params: 113,738\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/30\n",
            "1250/1250 [==============================] - 20s 15ms/step - loss: 2.2677 - accuracy: 0.1669 - val_loss: 2.1952 - val_accuracy: 0.2242 - lr: 1.0000e-05\n",
            "Epoch 2/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 2.0942 - accuracy: 0.2537 - val_loss: 2.0161 - val_accuracy: 0.2972 - lr: 1.0000e-05\n",
            "Epoch 3/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.9849 - accuracy: 0.2978 - val_loss: 1.9323 - val_accuracy: 0.3341 - lr: 1.0000e-05\n",
            "Epoch 4/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.9180 - accuracy: 0.3221 - val_loss: 1.8633 - val_accuracy: 0.3617 - lr: 1.0000e-05\n",
            "Epoch 5/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.8636 - accuracy: 0.3449 - val_loss: 1.8215 - val_accuracy: 0.3713 - lr: 1.0000e-05\n",
            "Epoch 6/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.8225 - accuracy: 0.3580 - val_loss: 1.7813 - val_accuracy: 0.3796 - lr: 1.0000e-05\n",
            "Epoch 7/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.7906 - accuracy: 0.3690 - val_loss: 1.7485 - val_accuracy: 0.3899 - lr: 1.0000e-05\n",
            "Epoch 8/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.7622 - accuracy: 0.3760 - val_loss: 1.7301 - val_accuracy: 0.3941 - lr: 1.0000e-05\n",
            "Epoch 9/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.7407 - accuracy: 0.3845 - val_loss: 1.6948 - val_accuracy: 0.4050 - lr: 1.0000e-05\n",
            "Epoch 10/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.7199 - accuracy: 0.3890 - val_loss: 1.6846 - val_accuracy: 0.4048 - lr: 1.0000e-05\n",
            "Epoch 11/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.7047 - accuracy: 0.3936 - val_loss: 1.6726 - val_accuracy: 0.4118 - lr: 5.0000e-06\n",
            "Epoch 12/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.6948 - accuracy: 0.3994 - val_loss: 1.6658 - val_accuracy: 0.4084 - lr: 5.0000e-06\n",
            "Epoch 13/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.6882 - accuracy: 0.4008 - val_loss: 1.6475 - val_accuracy: 0.4176 - lr: 5.0000e-06\n",
            "Epoch 14/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 1.6779 - accuracy: 0.4043 - val_loss: 1.6545 - val_accuracy: 0.4139 - lr: 5.0000e-06\n",
            "Epoch 15/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 1.6709 - accuracy: 0.4068 - val_loss: 1.6383 - val_accuracy: 0.4173 - lr: 5.0000e-06\n",
            "Epoch 16/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.6637 - accuracy: 0.4083 - val_loss: 1.6278 - val_accuracy: 0.4223 - lr: 5.0000e-06\n",
            "Epoch 17/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.6571 - accuracy: 0.4100 - val_loss: 1.6285 - val_accuracy: 0.4214 - lr: 5.0000e-06\n",
            "Epoch 18/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.6488 - accuracy: 0.4101 - val_loss: 1.6123 - val_accuracy: 0.4277 - lr: 5.0000e-06\n",
            "Epoch 19/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.6418 - accuracy: 0.4130 - val_loss: 1.6111 - val_accuracy: 0.4262 - lr: 5.0000e-06\n",
            "Epoch 20/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.6355 - accuracy: 0.4166 - val_loss: 1.6159 - val_accuracy: 0.4267 - lr: 5.0000e-06\n",
            "Epoch 21/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.6346 - accuracy: 0.4178 - val_loss: 1.6061 - val_accuracy: 0.4286 - lr: 1.2500e-06\n",
            "Epoch 22/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.6290 - accuracy: 0.4175 - val_loss: 1.5991 - val_accuracy: 0.4324 - lr: 1.2500e-06\n",
            "Epoch 23/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.6278 - accuracy: 0.4173 - val_loss: 1.6017 - val_accuracy: 0.4317 - lr: 1.2500e-06\n",
            "Epoch 24/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.6283 - accuracy: 0.4178 - val_loss: 1.5991 - val_accuracy: 0.4316 - lr: 1.2500e-06\n",
            "Epoch 25/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.6251 - accuracy: 0.4169 - val_loss: 1.5991 - val_accuracy: 0.4310 - lr: 1.2500e-06\n",
            "Epoch 26/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.6234 - accuracy: 0.4217 - val_loss: 1.5957 - val_accuracy: 0.4318 - lr: 1.2500e-06\n",
            "Epoch 27/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.6215 - accuracy: 0.4214 - val_loss: 1.5898 - val_accuracy: 0.4362 - lr: 1.2500e-06\n",
            "Epoch 28/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.6223 - accuracy: 0.4196 - val_loss: 1.5877 - val_accuracy: 0.4359 - lr: 1.2500e-06\n",
            "Epoch 29/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.6212 - accuracy: 0.4213 - val_loss: 1.5901 - val_accuracy: 0.4357 - lr: 1.2500e-06\n",
            "Epoch 30/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.6164 - accuracy: 0.4216 - val_loss: 1.5852 - val_accuracy: 0.4362 - lr: 1.2500e-06\n",
            "Score for fold 1: loss of 1.5851900577545166; accuracy of 43.619999289512634%\n",
            "Model: \"sequential_36\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_108 (Conv2D)         (None, 32, 32, 32)        896       \n",
            "                                                                 \n",
            " max_pooling2d_36 (MaxPoolin  multiple                 0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_109 (Conv2D)         (None, 16, 16, 64)        18496     \n",
            "                                                                 \n",
            " dropout_36 (Dropout)        (None, 8, 8, 64)          0         \n",
            "                                                                 \n",
            " conv2d_110 (Conv2D)         (None, 8, 8, 128)         73856     \n",
            "                                                                 \n",
            " flatten_36 (Flatten)        (None, 2048)              0         \n",
            "                                                                 \n",
            " dense_36 (Dense)            (None, 10)                20490     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 113,738\n",
            "Trainable params: 113,738\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 2.2807 - accuracy: 0.1343 - val_loss: 2.2455 - val_accuracy: 0.2414 - lr: 1.0000e-05\n",
            "Epoch 2/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 2.1666 - accuracy: 0.2499 - val_loss: 2.0612 - val_accuracy: 0.2904 - lr: 1.0000e-05\n",
            "Epoch 3/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 2.0061 - accuracy: 0.2900 - val_loss: 1.9518 - val_accuracy: 0.3211 - lr: 1.0000e-05\n",
            "Epoch 4/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.9408 - accuracy: 0.3155 - val_loss: 1.8940 - val_accuracy: 0.3401 - lr: 1.0000e-05\n",
            "Epoch 5/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.8948 - accuracy: 0.3329 - val_loss: 1.8524 - val_accuracy: 0.3529 - lr: 1.0000e-05\n",
            "Epoch 6/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.8567 - accuracy: 0.3469 - val_loss: 1.8160 - val_accuracy: 0.3618 - lr: 1.0000e-05\n",
            "Epoch 7/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.8199 - accuracy: 0.3627 - val_loss: 1.7872 - val_accuracy: 0.3701 - lr: 1.0000e-05\n",
            "Epoch 8/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.7964 - accuracy: 0.3704 - val_loss: 1.7758 - val_accuracy: 0.3734 - lr: 1.0000e-05\n",
            "Epoch 9/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.7677 - accuracy: 0.3758 - val_loss: 1.7528 - val_accuracy: 0.3776 - lr: 1.0000e-05\n",
            "Epoch 10/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.7453 - accuracy: 0.3814 - val_loss: 1.7411 - val_accuracy: 0.3809 - lr: 1.0000e-05\n",
            "Epoch 11/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.7260 - accuracy: 0.3890 - val_loss: 1.7184 - val_accuracy: 0.3894 - lr: 5.0000e-06\n",
            "Epoch 12/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.7184 - accuracy: 0.3916 - val_loss: 1.7204 - val_accuracy: 0.3881 - lr: 5.0000e-06\n",
            "Epoch 13/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 1.7065 - accuracy: 0.3943 - val_loss: 1.6994 - val_accuracy: 0.3925 - lr: 5.0000e-06\n",
            "Epoch 14/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.6978 - accuracy: 0.3984 - val_loss: 1.6995 - val_accuracy: 0.3938 - lr: 5.0000e-06\n",
            "Epoch 15/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.6900 - accuracy: 0.3984 - val_loss: 1.6869 - val_accuracy: 0.4005 - lr: 5.0000e-06\n",
            "Epoch 16/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.6770 - accuracy: 0.4053 - val_loss: 1.6756 - val_accuracy: 0.4045 - lr: 5.0000e-06\n",
            "Epoch 17/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.6729 - accuracy: 0.4031 - val_loss: 1.6671 - val_accuracy: 0.4075 - lr: 5.0000e-06\n",
            "Epoch 18/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.6607 - accuracy: 0.4105 - val_loss: 1.6560 - val_accuracy: 0.4108 - lr: 5.0000e-06\n",
            "Epoch 19/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.6554 - accuracy: 0.4122 - val_loss: 1.6569 - val_accuracy: 0.4069 - lr: 5.0000e-06\n",
            "Epoch 20/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.6470 - accuracy: 0.4132 - val_loss: 1.6365 - val_accuracy: 0.4160 - lr: 5.0000e-06\n",
            "Epoch 21/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.6431 - accuracy: 0.4130 - val_loss: 1.6373 - val_accuracy: 0.4172 - lr: 1.2500e-06\n",
            "Epoch 22/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.6401 - accuracy: 0.4182 - val_loss: 1.6509 - val_accuracy: 0.4113 - lr: 1.2500e-06\n",
            "Epoch 23/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.6386 - accuracy: 0.4164 - val_loss: 1.6333 - val_accuracy: 0.4189 - lr: 1.2500e-06\n",
            "Epoch 24/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.6344 - accuracy: 0.4163 - val_loss: 1.6407 - val_accuracy: 0.4151 - lr: 1.2500e-06\n",
            "Epoch 25/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.6344 - accuracy: 0.4159 - val_loss: 1.6298 - val_accuracy: 0.4208 - lr: 1.2500e-06\n",
            "Epoch 26/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.6295 - accuracy: 0.4172 - val_loss: 1.6452 - val_accuracy: 0.4134 - lr: 1.2500e-06\n",
            "Epoch 27/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.6292 - accuracy: 0.4192 - val_loss: 1.6298 - val_accuracy: 0.4197 - lr: 1.2500e-06\n",
            "Epoch 28/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.6272 - accuracy: 0.4196 - val_loss: 1.6358 - val_accuracy: 0.4155 - lr: 1.2500e-06\n",
            "Epoch 29/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.6264 - accuracy: 0.4192 - val_loss: 1.6276 - val_accuracy: 0.4204 - lr: 1.2500e-06\n",
            "Epoch 30/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.6212 - accuracy: 0.4218 - val_loss: 1.6183 - val_accuracy: 0.4241 - lr: 1.2500e-06\n",
            "Score for fold 2: loss of 1.6183027029037476; accuracy of 42.410001158714294%\n",
            "Model: \"sequential_37\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_111 (Conv2D)         (None, 32, 32, 32)        896       \n",
            "                                                                 \n",
            " max_pooling2d_37 (MaxPoolin  multiple                 0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_112 (Conv2D)         (None, 16, 16, 64)        18496     \n",
            "                                                                 \n",
            " dropout_37 (Dropout)        (None, 8, 8, 64)          0         \n",
            "                                                                 \n",
            " conv2d_113 (Conv2D)         (None, 8, 8, 128)         73856     \n",
            "                                                                 \n",
            " flatten_37 (Flatten)        (None, 2048)              0         \n",
            "                                                                 \n",
            " dense_37 (Dense)            (None, 10)                20490     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 113,738\n",
            "Trainable params: 113,738\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 2.2655 - accuracy: 0.1614 - val_loss: 2.1960 - val_accuracy: 0.2273 - lr: 1.0000e-05\n",
            "Epoch 2/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 2.0998 - accuracy: 0.2501 - val_loss: 2.0075 - val_accuracy: 0.2876 - lr: 1.0000e-05\n",
            "Epoch 3/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.9845 - accuracy: 0.2935 - val_loss: 1.9195 - val_accuracy: 0.3283 - lr: 1.0000e-05\n",
            "Epoch 4/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.9185 - accuracy: 0.3213 - val_loss: 1.8490 - val_accuracy: 0.3481 - lr: 1.0000e-05\n",
            "Epoch 5/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.8613 - accuracy: 0.3417 - val_loss: 1.7985 - val_accuracy: 0.3679 - lr: 1.0000e-05\n",
            "Epoch 6/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.8215 - accuracy: 0.3561 - val_loss: 1.7634 - val_accuracy: 0.3761 - lr: 1.0000e-05\n",
            "Epoch 7/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.7912 - accuracy: 0.3683 - val_loss: 1.7475 - val_accuracy: 0.3798 - lr: 1.0000e-05\n",
            "Epoch 8/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 1.7681 - accuracy: 0.3734 - val_loss: 1.7225 - val_accuracy: 0.3870 - lr: 1.0000e-05\n",
            "Epoch 9/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 1.7457 - accuracy: 0.3785 - val_loss: 1.6921 - val_accuracy: 0.3933 - lr: 1.0000e-05\n",
            "Epoch 10/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 1.7268 - accuracy: 0.3848 - val_loss: 1.6796 - val_accuracy: 0.3947 - lr: 1.0000e-05\n",
            "Epoch 11/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.7115 - accuracy: 0.3900 - val_loss: 1.6717 - val_accuracy: 0.3971 - lr: 5.0000e-06\n",
            "Epoch 12/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.7025 - accuracy: 0.3928 - val_loss: 1.6665 - val_accuracy: 0.3993 - lr: 5.0000e-06\n",
            "Epoch 13/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 1.6942 - accuracy: 0.3986 - val_loss: 1.6547 - val_accuracy: 0.4045 - lr: 5.0000e-06\n",
            "Epoch 14/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 1.6877 - accuracy: 0.3999 - val_loss: 1.6505 - val_accuracy: 0.4034 - lr: 5.0000e-06\n",
            "Epoch 15/30\n",
            "1250/1250 [==============================] - 20s 16ms/step - loss: 1.6769 - accuracy: 0.4004 - val_loss: 1.6432 - val_accuracy: 0.4036 - lr: 5.0000e-06\n",
            "Epoch 16/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 1.6688 - accuracy: 0.4034 - val_loss: 1.6247 - val_accuracy: 0.4129 - lr: 5.0000e-06\n",
            "Epoch 17/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 1.6610 - accuracy: 0.4083 - val_loss: 1.6224 - val_accuracy: 0.4127 - lr: 5.0000e-06\n",
            "Epoch 18/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 1.6558 - accuracy: 0.4083 - val_loss: 1.6081 - val_accuracy: 0.4184 - lr: 5.0000e-06\n",
            "Epoch 19/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 1.6480 - accuracy: 0.4098 - val_loss: 1.6109 - val_accuracy: 0.4155 - lr: 5.0000e-06\n",
            "Epoch 20/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.6444 - accuracy: 0.4109 - val_loss: 1.6009 - val_accuracy: 0.4190 - lr: 5.0000e-06\n",
            "Epoch 21/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.6399 - accuracy: 0.4132 - val_loss: 1.6000 - val_accuracy: 0.4192 - lr: 1.2500e-06\n",
            "Epoch 22/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 1.6367 - accuracy: 0.4151 - val_loss: 1.6008 - val_accuracy: 0.4186 - lr: 1.2500e-06\n",
            "Epoch 23/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.6344 - accuracy: 0.4155 - val_loss: 1.5994 - val_accuracy: 0.4200 - lr: 1.2500e-06\n",
            "Epoch 24/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 1.6311 - accuracy: 0.4153 - val_loss: 1.6012 - val_accuracy: 0.4171 - lr: 1.2500e-06\n",
            "Epoch 25/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.6303 - accuracy: 0.4135 - val_loss: 1.5970 - val_accuracy: 0.4208 - lr: 1.2500e-06\n",
            "Epoch 26/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.6327 - accuracy: 0.4146 - val_loss: 1.5994 - val_accuracy: 0.4198 - lr: 1.2500e-06\n",
            "Epoch 27/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.6270 - accuracy: 0.4164 - val_loss: 1.5949 - val_accuracy: 0.4204 - lr: 1.2500e-06\n",
            "Epoch 28/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.6290 - accuracy: 0.4166 - val_loss: 1.5936 - val_accuracy: 0.4205 - lr: 1.2500e-06\n",
            "Epoch 29/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.6254 - accuracy: 0.4166 - val_loss: 1.5878 - val_accuracy: 0.4242 - lr: 1.2500e-06\n",
            "Epoch 30/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 1.6263 - accuracy: 0.4160 - val_loss: 1.5851 - val_accuracy: 0.4254 - lr: 1.2500e-06\n",
            "Score for fold 3: loss of 1.5850588083267212; accuracy of 42.53999888896942%\n",
            "Model: \"sequential_38\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_114 (Conv2D)         (None, 32, 32, 32)        896       \n",
            "                                                                 \n",
            " max_pooling2d_38 (MaxPoolin  multiple                 0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_115 (Conv2D)         (None, 16, 16, 64)        18496     \n",
            "                                                                 \n",
            " dropout_38 (Dropout)        (None, 8, 8, 64)          0         \n",
            "                                                                 \n",
            " conv2d_116 (Conv2D)         (None, 8, 8, 128)         73856     \n",
            "                                                                 \n",
            " flatten_38 (Flatten)        (None, 2048)              0         \n",
            "                                                                 \n",
            " dense_38 (Dense)            (None, 10)                20490     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 113,738\n",
            "Trainable params: 113,738\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 2.2523 - accuracy: 0.1704 - val_loss: 2.1500 - val_accuracy: 0.2760 - lr: 1.0000e-05\n",
            "Epoch 2/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 2.0559 - accuracy: 0.2708 - val_loss: 1.9778 - val_accuracy: 0.3136 - lr: 1.0000e-05\n",
            "Epoch 3/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.9638 - accuracy: 0.3036 - val_loss: 1.9107 - val_accuracy: 0.3384 - lr: 1.0000e-05\n",
            "Epoch 4/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.9033 - accuracy: 0.3291 - val_loss: 1.8416 - val_accuracy: 0.3630 - lr: 1.0000e-05\n",
            "Epoch 5/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 1.8528 - accuracy: 0.3509 - val_loss: 1.7984 - val_accuracy: 0.3746 - lr: 1.0000e-05\n",
            "Epoch 6/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.8094 - accuracy: 0.3645 - val_loss: 1.7636 - val_accuracy: 0.3811 - lr: 1.0000e-05\n",
            "Epoch 7/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 1.7775 - accuracy: 0.3726 - val_loss: 1.7343 - val_accuracy: 0.3849 - lr: 1.0000e-05\n",
            "Epoch 8/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.7534 - accuracy: 0.3801 - val_loss: 1.7107 - val_accuracy: 0.3934 - lr: 1.0000e-05\n",
            "Epoch 9/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.7270 - accuracy: 0.3892 - val_loss: 1.6858 - val_accuracy: 0.4054 - lr: 1.0000e-05\n",
            "Epoch 10/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 1.7050 - accuracy: 0.3957 - val_loss: 1.6701 - val_accuracy: 0.4079 - lr: 1.0000e-05\n",
            "Epoch 11/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 1.6900 - accuracy: 0.4004 - val_loss: 1.6591 - val_accuracy: 0.4086 - lr: 5.0000e-06\n",
            "Epoch 12/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.6808 - accuracy: 0.4049 - val_loss: 1.6519 - val_accuracy: 0.4080 - lr: 5.0000e-06\n",
            "Epoch 13/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.6723 - accuracy: 0.4089 - val_loss: 1.6461 - val_accuracy: 0.4100 - lr: 5.0000e-06\n",
            "Epoch 14/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.6624 - accuracy: 0.4101 - val_loss: 1.6445 - val_accuracy: 0.4118 - lr: 5.0000e-06\n",
            "Epoch 15/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.6493 - accuracy: 0.4119 - val_loss: 1.6167 - val_accuracy: 0.4204 - lr: 5.0000e-06\n",
            "Epoch 16/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.6444 - accuracy: 0.4159 - val_loss: 1.6094 - val_accuracy: 0.4273 - lr: 5.0000e-06\n",
            "Epoch 17/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 1.6373 - accuracy: 0.4177 - val_loss: 1.5915 - val_accuracy: 0.4310 - lr: 5.0000e-06\n",
            "Epoch 18/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.6291 - accuracy: 0.4188 - val_loss: 1.6014 - val_accuracy: 0.4261 - lr: 5.0000e-06\n",
            "Epoch 19/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 1.6195 - accuracy: 0.4241 - val_loss: 1.6020 - val_accuracy: 0.4258 - lr: 5.0000e-06\n",
            "Epoch 20/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 1.6109 - accuracy: 0.4261 - val_loss: 1.5951 - val_accuracy: 0.4272 - lr: 5.0000e-06\n",
            "Epoch 21/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 1.6091 - accuracy: 0.4276 - val_loss: 1.5773 - val_accuracy: 0.4358 - lr: 1.2500e-06\n",
            "Epoch 22/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.6054 - accuracy: 0.4295 - val_loss: 1.5805 - val_accuracy: 0.4351 - lr: 1.2500e-06\n",
            "Epoch 23/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.6039 - accuracy: 0.4303 - val_loss: 1.5792 - val_accuracy: 0.4337 - lr: 1.2500e-06\n",
            "Epoch 24/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.6039 - accuracy: 0.4284 - val_loss: 1.5819 - val_accuracy: 0.4327 - lr: 1.2500e-06\n",
            "Epoch 25/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.6004 - accuracy: 0.4295 - val_loss: 1.5779 - val_accuracy: 0.4363 - lr: 1.2500e-06\n",
            "Epoch 26/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.5970 - accuracy: 0.4315 - val_loss: 1.5764 - val_accuracy: 0.4356 - lr: 1.2500e-06\n",
            "Epoch 27/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.5957 - accuracy: 0.4315 - val_loss: 1.5779 - val_accuracy: 0.4356 - lr: 1.2500e-06\n",
            "Epoch 28/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.5990 - accuracy: 0.4299 - val_loss: 1.5700 - val_accuracy: 0.4389 - lr: 1.2500e-06\n",
            "Epoch 29/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.5972 - accuracy: 0.4313 - val_loss: 1.5690 - val_accuracy: 0.4380 - lr: 1.2500e-06\n",
            "Epoch 30/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.5920 - accuracy: 0.4315 - val_loss: 1.5634 - val_accuracy: 0.4412 - lr: 1.2500e-06\n",
            "Score for fold 4: loss of 1.5633537769317627; accuracy of 44.119998812675476%\n",
            "Model: \"sequential_39\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_117 (Conv2D)         (None, 32, 32, 32)        896       \n",
            "                                                                 \n",
            " max_pooling2d_39 (MaxPoolin  multiple                 0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_118 (Conv2D)         (None, 16, 16, 64)        18496     \n",
            "                                                                 \n",
            " dropout_39 (Dropout)        (None, 8, 8, 64)          0         \n",
            "                                                                 \n",
            " conv2d_119 (Conv2D)         (None, 8, 8, 128)         73856     \n",
            "                                                                 \n",
            " flatten_39 (Flatten)        (None, 2048)              0         \n",
            "                                                                 \n",
            " dense_39 (Dense)            (None, 10)                20490     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 113,738\n",
            "Trainable params: 113,738\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 2.2748 - accuracy: 0.1453 - val_loss: 2.2269 - val_accuracy: 0.2344 - lr: 1.0000e-05\n",
            "Epoch 2/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 2.1425 - accuracy: 0.2388 - val_loss: 2.0333 - val_accuracy: 0.3019 - lr: 1.0000e-05\n",
            "Epoch 3/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 2.0127 - accuracy: 0.2822 - val_loss: 1.9504 - val_accuracy: 0.3240 - lr: 1.0000e-05\n",
            "Epoch 4/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.9583 - accuracy: 0.3034 - val_loss: 1.8962 - val_accuracy: 0.3395 - lr: 1.0000e-05\n",
            "Epoch 5/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.9164 - accuracy: 0.3211 - val_loss: 1.8502 - val_accuracy: 0.3478 - lr: 1.0000e-05\n",
            "Epoch 6/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.8750 - accuracy: 0.3374 - val_loss: 1.8043 - val_accuracy: 0.3653 - lr: 1.0000e-05\n",
            "Epoch 7/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.8363 - accuracy: 0.3493 - val_loss: 1.7652 - val_accuracy: 0.3781 - lr: 1.0000e-05\n",
            "Epoch 8/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.7999 - accuracy: 0.3641 - val_loss: 1.7337 - val_accuracy: 0.3901 - lr: 1.0000e-05\n",
            "Epoch 9/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.7667 - accuracy: 0.3754 - val_loss: 1.7016 - val_accuracy: 0.3996 - lr: 1.0000e-05\n",
            "Epoch 10/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.7352 - accuracy: 0.3853 - val_loss: 1.6742 - val_accuracy: 0.4137 - lr: 1.0000e-05\n",
            "Epoch 11/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.7175 - accuracy: 0.3914 - val_loss: 1.6581 - val_accuracy: 0.4167 - lr: 5.0000e-06\n",
            "Epoch 12/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.7052 - accuracy: 0.3951 - val_loss: 1.6490 - val_accuracy: 0.4215 - lr: 5.0000e-06\n",
            "Epoch 13/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.6920 - accuracy: 0.3997 - val_loss: 1.6433 - val_accuracy: 0.4212 - lr: 5.0000e-06\n",
            "Epoch 14/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.6838 - accuracy: 0.3981 - val_loss: 1.6304 - val_accuracy: 0.4276 - lr: 5.0000e-06\n",
            "Epoch 15/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.6721 - accuracy: 0.4038 - val_loss: 1.6225 - val_accuracy: 0.4287 - lr: 5.0000e-06\n",
            "Epoch 16/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.6597 - accuracy: 0.4105 - val_loss: 1.6177 - val_accuracy: 0.4277 - lr: 5.0000e-06\n",
            "Epoch 17/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.6538 - accuracy: 0.4119 - val_loss: 1.6123 - val_accuracy: 0.4324 - lr: 5.0000e-06\n",
            "Epoch 18/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.6435 - accuracy: 0.4125 - val_loss: 1.6041 - val_accuracy: 0.4357 - lr: 5.0000e-06\n",
            "Epoch 19/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 1.6348 - accuracy: 0.4161 - val_loss: 1.5983 - val_accuracy: 0.4349 - lr: 5.0000e-06\n",
            "Epoch 20/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.6237 - accuracy: 0.4188 - val_loss: 1.5801 - val_accuracy: 0.4431 - lr: 5.0000e-06\n",
            "Epoch 21/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.6211 - accuracy: 0.4222 - val_loss: 1.5778 - val_accuracy: 0.4418 - lr: 1.2500e-06\n",
            "Epoch 22/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.6204 - accuracy: 0.4223 - val_loss: 1.5733 - val_accuracy: 0.4437 - lr: 1.2500e-06\n",
            "Epoch 23/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.6201 - accuracy: 0.4205 - val_loss: 1.5733 - val_accuracy: 0.4440 - lr: 1.2500e-06\n",
            "Epoch 24/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.6154 - accuracy: 0.4230 - val_loss: 1.5824 - val_accuracy: 0.4409 - lr: 1.2500e-06\n",
            "Epoch 25/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 1.6157 - accuracy: 0.4212 - val_loss: 1.5699 - val_accuracy: 0.4450 - lr: 1.2500e-06\n",
            "Epoch 26/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.6127 - accuracy: 0.4236 - val_loss: 1.5692 - val_accuracy: 0.4456 - lr: 1.2500e-06\n",
            "Epoch 27/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.6116 - accuracy: 0.4212 - val_loss: 1.5664 - val_accuracy: 0.4457 - lr: 1.2500e-06\n",
            "Epoch 28/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.6080 - accuracy: 0.4256 - val_loss: 1.5706 - val_accuracy: 0.4431 - lr: 1.2500e-06\n",
            "Epoch 29/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.6051 - accuracy: 0.4280 - val_loss: 1.5634 - val_accuracy: 0.4472 - lr: 1.2500e-06\n",
            "Epoch 30/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 1.6042 - accuracy: 0.4259 - val_loss: 1.5681 - val_accuracy: 0.4445 - lr: 1.2500e-06\n",
            "Score for fold 5: loss of 1.568050742149353; accuracy of 44.449999928474426%\n",
            "------------------------------------------------------------------------\n",
            "Score per fold\n",
            "------------------------------------------------------------------------\n",
            "> Fold 1 - Loss: 1.5851900577545166 - Accuracy: 43.619999289512634%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 2 - Loss: 1.6183027029037476 - Accuracy: 42.410001158714294%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 3 - Loss: 1.5850588083267212 - Accuracy: 42.53999888896942%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 4 - Loss: 1.5633537769317627 - Accuracy: 44.119998812675476%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 5 - Loss: 1.568050742149353 - Accuracy: 44.449999928474426%\n",
            "------------------------------------------------------------------------\n",
            "Average scores for all folds:\n",
            "> Accuracy: 43.42799961566925 (+- 0.8228095225225823)\n",
            "> Loss: 1.5839912176132203\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Network depth, width and maxpool"
      ],
      "metadata": {
        "id": "iUKvtk6lYzHL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2 hidden layers and Wide Structure"
      ],
      "metadata": {
        "id": "_f5pVoSg6sgW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "acc_per_fold = []\n",
        "loss_per_fold = []\n",
        "\n",
        "with tf.device('/device:GPU:0'):\n",
        "  fold_no = 1\n",
        "  for train, valid in kfold.split(trainX, trainy):\n",
        "\n",
        "    model = CNN_Model(batch_size=32, depth=2, hidden_neurons_1=64, hidden_neurons_2=128, hidden_neurons_3=256, kernel_size_1=3, kernel_size_2=3, kernel_size_3=3, \n",
        "               dropout_rate=0.2, batch_norm=False, pooling='max', layer_activation_1='relu', layer_activation_2='relu', layer_activation_3='relu',\n",
        "               kernel_initialiser='glorot_uniform', bias_initialiser='zeros', optimiser='Adam', weight_decay=None, learning_rate=1e-3, output_activation='softmax', SGD_momentum=0) \n",
        "  \n",
        "    train_gen = tf.keras.preprocessing.image.ImageDataGenerator(width_shift_range=0.1,\n",
        "      height_shift_range=0.1,\n",
        "      horizontal_flip=True)\n",
        "    train_gen.fit(trainX[train])      #Data Augmentation\n",
        "  \n",
        "    print('------------------------------------------------------------------------')\n",
        "    print(f'Training for fold {fold_no} ...')\n",
        "\n",
        "    history = model.train(\n",
        "              train_gen,\n",
        "                trainX[train],\n",
        "                  trainy[train],\n",
        "                trainX[valid],\n",
        "                  trainy[valid],\n",
        "                  lr_rate_type='step') #Step decay\n",
        "\n",
        "    scores = model.evaluate(trainX[valid], trainy[valid])\n",
        "\n",
        "    print(f'Score for fold {fold_no}: loss of {scores[0]}; accuracy of {scores[1]*100}%')\n",
        "    acc_per_fold.append(scores[1] * 100)\n",
        "    loss_per_fold.append(scores[0])\n",
        "\n",
        "    fold_no+=1\n",
        "\n",
        "# == Provide average scores ==\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Score per fold')\n",
        "for i in range(0, len(acc_per_fold)):\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Average scores for all folds:')\n",
        "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
        "print(f'> Loss: {np.mean(loss_per_fold)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qLulIEh26uSp",
        "outputId": "2208755e-8e60-4a40-90a2-8490ec7ed395"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_40\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_120 (Conv2D)         (None, 32, 32, 64)        1792      \n",
            "                                                                 \n",
            " max_pooling2d_40 (MaxPoolin  multiple                 0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_121 (Conv2D)         (None, 16, 16, 128)       73856     \n",
            "                                                                 \n",
            " dropout_40 (Dropout)        (None, 8, 8, 128)         0         \n",
            "                                                                 \n",
            " flatten_40 (Flatten)        (None, 8192)              0         \n",
            "                                                                 \n",
            " dense_40 (Dense)            (None, 10)                81930     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 157,578\n",
            "Trainable params: 157,578\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 1.5634 - accuracy: 0.4390 - val_loss: 1.2709 - val_accuracy: 0.5480 - lr: 0.0010\n",
            "Epoch 2/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 1.2436 - accuracy: 0.5618 - val_loss: 1.0769 - val_accuracy: 0.6294 - lr: 0.0010\n",
            "Epoch 3/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.1309 - accuracy: 0.6028 - val_loss: 0.9971 - val_accuracy: 0.6556 - lr: 0.0010\n",
            "Epoch 4/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.0676 - accuracy: 0.6271 - val_loss: 0.9327 - val_accuracy: 0.6738 - lr: 0.0010\n",
            "Epoch 5/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 1.0221 - accuracy: 0.6450 - val_loss: 0.9345 - val_accuracy: 0.6750 - lr: 0.0010\n",
            "Epoch 6/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.9883 - accuracy: 0.6575 - val_loss: 0.9399 - val_accuracy: 0.6834 - lr: 0.0010\n",
            "Epoch 7/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.9615 - accuracy: 0.6650 - val_loss: 0.8906 - val_accuracy: 0.6990 - lr: 0.0010\n",
            "Epoch 8/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.9326 - accuracy: 0.6770 - val_loss: 0.8541 - val_accuracy: 0.7064 - lr: 0.0010\n",
            "Epoch 9/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.9251 - accuracy: 0.6768 - val_loss: 0.8424 - val_accuracy: 0.7095 - lr: 0.0010\n",
            "Epoch 10/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.8935 - accuracy: 0.6894 - val_loss: 0.8466 - val_accuracy: 0.7097 - lr: 0.0010\n",
            "Epoch 11/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.8320 - accuracy: 0.7099 - val_loss: 0.7872 - val_accuracy: 0.7295 - lr: 5.0000e-04\n",
            "Epoch 12/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.8163 - accuracy: 0.7170 - val_loss: 0.7946 - val_accuracy: 0.7261 - lr: 5.0000e-04\n",
            "Epoch 13/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.8057 - accuracy: 0.7196 - val_loss: 0.7838 - val_accuracy: 0.7337 - lr: 5.0000e-04\n",
            "Epoch 14/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.7969 - accuracy: 0.7222 - val_loss: 0.7737 - val_accuracy: 0.7283 - lr: 5.0000e-04\n",
            "Epoch 15/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.7847 - accuracy: 0.7279 - val_loss: 0.7799 - val_accuracy: 0.7315 - lr: 5.0000e-04\n",
            "Epoch 16/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.7851 - accuracy: 0.7291 - val_loss: 0.7669 - val_accuracy: 0.7369 - lr: 5.0000e-04\n",
            "Epoch 17/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.7775 - accuracy: 0.7299 - val_loss: 0.7499 - val_accuracy: 0.7424 - lr: 5.0000e-04\n",
            "Epoch 18/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 0.7670 - accuracy: 0.7335 - val_loss: 0.7573 - val_accuracy: 0.7392 - lr: 5.0000e-04\n",
            "Epoch 19/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.7577 - accuracy: 0.7373 - val_loss: 0.7725 - val_accuracy: 0.7367 - lr: 5.0000e-04\n",
            "Epoch 20/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.7550 - accuracy: 0.7390 - val_loss: 0.7494 - val_accuracy: 0.7451 - lr: 5.0000e-04\n",
            "Epoch 21/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.7120 - accuracy: 0.7546 - val_loss: 0.7070 - val_accuracy: 0.7577 - lr: 1.2500e-04\n",
            "Epoch 22/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 0.7013 - accuracy: 0.7569 - val_loss: 0.7065 - val_accuracy: 0.7595 - lr: 1.2500e-04\n",
            "Epoch 23/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.7032 - accuracy: 0.7572 - val_loss: 0.7294 - val_accuracy: 0.7498 - lr: 1.2500e-04\n",
            "Epoch 24/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.6956 - accuracy: 0.7579 - val_loss: 0.7252 - val_accuracy: 0.7508 - lr: 1.2500e-04\n",
            "Epoch 25/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.6933 - accuracy: 0.7593 - val_loss: 0.7447 - val_accuracy: 0.7474 - lr: 1.2500e-04\n",
            "Epoch 26/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.6917 - accuracy: 0.7622 - val_loss: 0.7317 - val_accuracy: 0.7489 - lr: 1.2500e-04\n",
            "Epoch 27/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 0.6860 - accuracy: 0.7632 - val_loss: 0.6946 - val_accuracy: 0.7616 - lr: 1.2500e-04\n",
            "Epoch 28/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 0.6886 - accuracy: 0.7609 - val_loss: 0.7490 - val_accuracy: 0.7427 - lr: 1.2500e-04\n",
            "Epoch 29/30\n",
            "1250/1250 [==============================] - 23s 18ms/step - loss: 0.6813 - accuracy: 0.7617 - val_loss: 0.7328 - val_accuracy: 0.7515 - lr: 1.2500e-04\n",
            "Epoch 30/30\n",
            "1250/1250 [==============================] - 20s 16ms/step - loss: 0.6825 - accuracy: 0.7621 - val_loss: 0.7136 - val_accuracy: 0.7566 - lr: 1.2500e-04\n",
            "Score for fold 1: loss of 0.7135555148124695; accuracy of 75.6600022315979%\n",
            "Model: \"sequential_41\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_122 (Conv2D)         (None, 32, 32, 64)        1792      \n",
            "                                                                 \n",
            " max_pooling2d_41 (MaxPoolin  multiple                 0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_123 (Conv2D)         (None, 16, 16, 128)       73856     \n",
            "                                                                 \n",
            " dropout_41 (Dropout)        (None, 8, 8, 128)         0         \n",
            "                                                                 \n",
            " flatten_41 (Flatten)        (None, 8192)              0         \n",
            "                                                                 \n",
            " dense_41 (Dense)            (None, 10)                81930     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 157,578\n",
            "Trainable params: 157,578\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/30\n",
            "1250/1250 [==============================] - 21s 16ms/step - loss: 1.5726 - accuracy: 0.4352 - val_loss: 1.3014 - val_accuracy: 0.5458 - lr: 0.0010\n",
            "Epoch 2/30\n",
            "1250/1250 [==============================] - 22s 17ms/step - loss: 1.2463 - accuracy: 0.5611 - val_loss: 1.0885 - val_accuracy: 0.6275 - lr: 0.0010\n",
            "Epoch 3/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 1.1384 - accuracy: 0.6017 - val_loss: 1.0264 - val_accuracy: 0.6487 - lr: 0.0010\n",
            "Epoch 4/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.0674 - accuracy: 0.6278 - val_loss: 0.9598 - val_accuracy: 0.6686 - lr: 0.0010\n",
            "Epoch 5/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.0262 - accuracy: 0.6417 - val_loss: 0.9073 - val_accuracy: 0.6917 - lr: 0.0010\n",
            "Epoch 6/30\n",
            "1250/1250 [==============================] - 21s 17ms/step - loss: 0.9837 - accuracy: 0.6556 - val_loss: 0.9973 - val_accuracy: 0.6633 - lr: 0.0010\n",
            "Epoch 7/30\n",
            "1250/1250 [==============================] - 20s 16ms/step - loss: 0.9526 - accuracy: 0.6682 - val_loss: 0.8411 - val_accuracy: 0.7145 - lr: 0.0010\n",
            "Epoch 8/30\n",
            "1250/1250 [==============================] - 21s 16ms/step - loss: 0.9384 - accuracy: 0.6737 - val_loss: 0.8718 - val_accuracy: 0.7040 - lr: 0.0010\n",
            "Epoch 9/30\n",
            "1250/1250 [==============================] - 22s 17ms/step - loss: 0.9131 - accuracy: 0.6812 - val_loss: 0.8062 - val_accuracy: 0.7246 - lr: 0.0010\n",
            "Epoch 10/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.8956 - accuracy: 0.6877 - val_loss: 0.7925 - val_accuracy: 0.7322 - lr: 0.0010\n",
            "Epoch 11/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.8353 - accuracy: 0.7085 - val_loss: 0.8046 - val_accuracy: 0.7275 - lr: 5.0000e-04\n",
            "Epoch 12/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.8257 - accuracy: 0.7128 - val_loss: 0.7581 - val_accuracy: 0.7416 - lr: 5.0000e-04\n",
            "Epoch 13/30\n",
            "1250/1250 [==============================] - 20s 16ms/step - loss: 0.8093 - accuracy: 0.7179 - val_loss: 0.7687 - val_accuracy: 0.7413 - lr: 5.0000e-04\n",
            "Epoch 14/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.7953 - accuracy: 0.7253 - val_loss: 0.7687 - val_accuracy: 0.7433 - lr: 5.0000e-04\n",
            "Epoch 15/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 0.7895 - accuracy: 0.7264 - val_loss: 0.7350 - val_accuracy: 0.7478 - lr: 5.0000e-04\n",
            "Epoch 16/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.7876 - accuracy: 0.7261 - val_loss: 0.7388 - val_accuracy: 0.7482 - lr: 5.0000e-04\n",
            "Epoch 17/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.7761 - accuracy: 0.7297 - val_loss: 0.7360 - val_accuracy: 0.7495 - lr: 5.0000e-04\n",
            "Epoch 18/30\n",
            "1250/1250 [==============================] - 20s 16ms/step - loss: 0.7705 - accuracy: 0.7325 - val_loss: 0.7334 - val_accuracy: 0.7488 - lr: 5.0000e-04\n",
            "Epoch 19/30\n",
            "1250/1250 [==============================] - 20s 16ms/step - loss: 0.7605 - accuracy: 0.7332 - val_loss: 0.7494 - val_accuracy: 0.7464 - lr: 5.0000e-04\n",
            "Epoch 20/30\n",
            "1250/1250 [==============================] - 20s 16ms/step - loss: 0.7574 - accuracy: 0.7357 - val_loss: 0.7522 - val_accuracy: 0.7496 - lr: 5.0000e-04\n",
            "Epoch 21/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.7186 - accuracy: 0.7510 - val_loss: 0.7161 - val_accuracy: 0.7606 - lr: 1.2500e-04\n",
            "Epoch 22/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.7040 - accuracy: 0.7570 - val_loss: 0.7239 - val_accuracy: 0.7568 - lr: 1.2500e-04\n",
            "Epoch 23/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.7029 - accuracy: 0.7538 - val_loss: 0.7059 - val_accuracy: 0.7612 - lr: 1.2500e-04\n",
            "Epoch 24/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.6998 - accuracy: 0.7578 - val_loss: 0.7197 - val_accuracy: 0.7574 - lr: 1.2500e-04\n",
            "Epoch 25/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 0.6994 - accuracy: 0.7562 - val_loss: 0.6996 - val_accuracy: 0.7641 - lr: 1.2500e-04\n",
            "Epoch 26/30\n",
            "1250/1250 [==============================] - 19s 16ms/step - loss: 0.6995 - accuracy: 0.7579 - val_loss: 0.6864 - val_accuracy: 0.7700 - lr: 1.2500e-04\n",
            "Epoch 27/30\n",
            "1250/1250 [==============================] - 20s 16ms/step - loss: 0.6943 - accuracy: 0.7596 - val_loss: 0.6851 - val_accuracy: 0.7702 - lr: 1.2500e-04\n",
            "Epoch 28/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.6900 - accuracy: 0.7598 - val_loss: 0.6885 - val_accuracy: 0.7648 - lr: 1.2500e-04\n",
            "Epoch 29/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 0.6904 - accuracy: 0.7611 - val_loss: 0.7229 - val_accuracy: 0.7592 - lr: 1.2500e-04\n",
            "Epoch 30/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.6833 - accuracy: 0.7645 - val_loss: 0.6920 - val_accuracy: 0.7640 - lr: 1.2500e-04\n",
            "Score for fold 2: loss of 0.6920393705368042; accuracy of 76.39999985694885%\n",
            "Model: \"sequential_42\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_124 (Conv2D)         (None, 32, 32, 64)        1792      \n",
            "                                                                 \n",
            " max_pooling2d_42 (MaxPoolin  multiple                 0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_125 (Conv2D)         (None, 16, 16, 128)       73856     \n",
            "                                                                 \n",
            " dropout_42 (Dropout)        (None, 8, 8, 128)         0         \n",
            "                                                                 \n",
            " flatten_42 (Flatten)        (None, 8192)              0         \n",
            "                                                                 \n",
            " dense_42 (Dense)            (None, 10)                81930     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 157,578\n",
            "Trainable params: 157,578\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 1.5497 - accuracy: 0.4447 - val_loss: 1.2494 - val_accuracy: 0.5726 - lr: 0.0010\n",
            "Epoch 2/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 1.2505 - accuracy: 0.5590 - val_loss: 1.0675 - val_accuracy: 0.6290 - lr: 0.0010\n",
            "Epoch 3/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 1.1452 - accuracy: 0.5969 - val_loss: 1.0235 - val_accuracy: 0.6430 - lr: 0.0010\n",
            "Epoch 4/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 1.0768 - accuracy: 0.6253 - val_loss: 1.0078 - val_accuracy: 0.6521 - lr: 0.0010\n",
            "Epoch 5/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 1.0389 - accuracy: 0.6374 - val_loss: 0.9493 - val_accuracy: 0.6764 - lr: 0.0010\n",
            "Epoch 6/30\n",
            "1250/1250 [==============================] - 20s 16ms/step - loss: 0.9999 - accuracy: 0.6519 - val_loss: 0.8960 - val_accuracy: 0.6913 - lr: 0.0010\n",
            "Epoch 7/30\n",
            "1250/1250 [==============================] - 20s 16ms/step - loss: 0.9722 - accuracy: 0.6605 - val_loss: 0.8442 - val_accuracy: 0.7070 - lr: 0.0010\n",
            "Epoch 8/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.9490 - accuracy: 0.6704 - val_loss: 0.8510 - val_accuracy: 0.7070 - lr: 0.0010\n",
            "Epoch 9/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.9259 - accuracy: 0.6775 - val_loss: 0.8944 - val_accuracy: 0.6893 - lr: 0.0010\n",
            "Epoch 10/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 0.9131 - accuracy: 0.6843 - val_loss: 0.8582 - val_accuracy: 0.7052 - lr: 0.0010\n",
            "Epoch 11/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.8396 - accuracy: 0.7093 - val_loss: 0.7833 - val_accuracy: 0.7303 - lr: 5.0000e-04\n",
            "Epoch 12/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 0.8307 - accuracy: 0.7118 - val_loss: 0.7422 - val_accuracy: 0.7437 - lr: 5.0000e-04\n",
            "Epoch 13/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.8160 - accuracy: 0.7167 - val_loss: 0.7560 - val_accuracy: 0.7435 - lr: 5.0000e-04\n",
            "Epoch 14/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.8037 - accuracy: 0.7226 - val_loss: 0.7640 - val_accuracy: 0.7381 - lr: 5.0000e-04\n",
            "Epoch 15/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.7981 - accuracy: 0.7225 - val_loss: 0.7453 - val_accuracy: 0.7434 - lr: 5.0000e-04\n",
            "Epoch 16/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 0.7920 - accuracy: 0.7254 - val_loss: 0.7489 - val_accuracy: 0.7414 - lr: 5.0000e-04\n",
            "Epoch 17/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.7806 - accuracy: 0.7278 - val_loss: 0.7200 - val_accuracy: 0.7576 - lr: 5.0000e-04\n",
            "Epoch 18/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.7756 - accuracy: 0.7298 - val_loss: 0.7379 - val_accuracy: 0.7522 - lr: 5.0000e-04\n",
            "Epoch 19/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.7664 - accuracy: 0.7349 - val_loss: 0.7292 - val_accuracy: 0.7497 - lr: 5.0000e-04\n",
            "Epoch 20/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.7658 - accuracy: 0.7335 - val_loss: 0.7251 - val_accuracy: 0.7515 - lr: 5.0000e-04\n",
            "Epoch 21/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.7156 - accuracy: 0.7523 - val_loss: 0.6943 - val_accuracy: 0.7658 - lr: 1.2500e-04\n",
            "Epoch 22/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.7121 - accuracy: 0.7505 - val_loss: 0.7114 - val_accuracy: 0.7612 - lr: 1.2500e-04\n",
            "Epoch 23/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.7060 - accuracy: 0.7569 - val_loss: 0.6872 - val_accuracy: 0.7660 - lr: 1.2500e-04\n",
            "Epoch 24/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 0.7017 - accuracy: 0.7584 - val_loss: 0.6961 - val_accuracy: 0.7651 - lr: 1.2500e-04\n",
            "Epoch 25/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.7023 - accuracy: 0.7567 - val_loss: 0.6935 - val_accuracy: 0.7664 - lr: 1.2500e-04\n",
            "Epoch 26/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 0.6978 - accuracy: 0.7589 - val_loss: 0.7130 - val_accuracy: 0.7595 - lr: 1.2500e-04\n",
            "Epoch 27/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.6916 - accuracy: 0.7602 - val_loss: 0.6951 - val_accuracy: 0.7656 - lr: 1.2500e-04\n",
            "Epoch 28/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 0.6972 - accuracy: 0.7586 - val_loss: 0.6785 - val_accuracy: 0.7715 - lr: 1.2500e-04\n",
            "Epoch 29/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.6871 - accuracy: 0.7637 - val_loss: 0.6788 - val_accuracy: 0.7735 - lr: 1.2500e-04\n",
            "Epoch 30/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 0.6830 - accuracy: 0.7663 - val_loss: 0.6919 - val_accuracy: 0.7675 - lr: 1.2500e-04\n",
            "Score for fold 3: loss of 0.6919037699699402; accuracy of 76.74999833106995%\n",
            "Model: \"sequential_43\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_126 (Conv2D)         (None, 32, 32, 64)        1792      \n",
            "                                                                 \n",
            " max_pooling2d_43 (MaxPoolin  multiple                 0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_127 (Conv2D)         (None, 16, 16, 128)       73856     \n",
            "                                                                 \n",
            " dropout_43 (Dropout)        (None, 8, 8, 128)         0         \n",
            "                                                                 \n",
            " flatten_43 (Flatten)        (None, 8192)              0         \n",
            "                                                                 \n",
            " dense_43 (Dense)            (None, 10)                81930     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 157,578\n",
            "Trainable params: 157,578\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.5555 - accuracy: 0.4420 - val_loss: 1.2826 - val_accuracy: 0.5559 - lr: 0.0010\n",
            "Epoch 2/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.2462 - accuracy: 0.5633 - val_loss: 1.0655 - val_accuracy: 0.6378 - lr: 0.0010\n",
            "Epoch 3/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.1347 - accuracy: 0.6004 - val_loss: 0.9639 - val_accuracy: 0.6720 - lr: 0.0010\n",
            "Epoch 4/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.0775 - accuracy: 0.6235 - val_loss: 0.9327 - val_accuracy: 0.6799 - lr: 0.0010\n",
            "Epoch 5/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 1.0259 - accuracy: 0.6446 - val_loss: 0.8724 - val_accuracy: 0.6973 - lr: 0.0010\n",
            "Epoch 6/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.9960 - accuracy: 0.6528 - val_loss: 0.8876 - val_accuracy: 0.6925 - lr: 0.0010\n",
            "Epoch 7/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 0.9626 - accuracy: 0.6658 - val_loss: 0.8908 - val_accuracy: 0.6945 - lr: 0.0010\n",
            "Epoch 8/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 0.9351 - accuracy: 0.6720 - val_loss: 0.7819 - val_accuracy: 0.7325 - lr: 0.0010\n",
            "Epoch 9/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.9150 - accuracy: 0.6813 - val_loss: 0.8165 - val_accuracy: 0.7216 - lr: 0.0010\n",
            "Epoch 10/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.8987 - accuracy: 0.6872 - val_loss: 0.8220 - val_accuracy: 0.7174 - lr: 0.0010\n",
            "Epoch 11/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 0.8359 - accuracy: 0.7111 - val_loss: 0.7831 - val_accuracy: 0.7308 - lr: 5.0000e-04\n",
            "Epoch 12/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.8177 - accuracy: 0.7171 - val_loss: 0.7550 - val_accuracy: 0.7430 - lr: 5.0000e-04\n",
            "Epoch 13/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 0.8017 - accuracy: 0.7213 - val_loss: 0.7461 - val_accuracy: 0.7498 - lr: 5.0000e-04\n",
            "Epoch 14/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.7946 - accuracy: 0.7261 - val_loss: 0.7369 - val_accuracy: 0.7486 - lr: 5.0000e-04\n",
            "Epoch 15/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.7786 - accuracy: 0.7314 - val_loss: 0.7153 - val_accuracy: 0.7545 - lr: 5.0000e-04\n",
            "Epoch 16/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.7768 - accuracy: 0.7316 - val_loss: 0.7065 - val_accuracy: 0.7600 - lr: 5.0000e-04\n",
            "Epoch 17/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 0.7705 - accuracy: 0.7338 - val_loss: 0.7263 - val_accuracy: 0.7562 - lr: 5.0000e-04\n",
            "Epoch 18/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.7626 - accuracy: 0.7355 - val_loss: 0.7525 - val_accuracy: 0.7465 - lr: 5.0000e-04\n",
            "Epoch 19/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.7572 - accuracy: 0.7374 - val_loss: 0.7126 - val_accuracy: 0.7583 - lr: 5.0000e-04\n",
            "Epoch 20/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.7512 - accuracy: 0.7404 - val_loss: 0.7599 - val_accuracy: 0.7433 - lr: 5.0000e-04\n",
            "Epoch 21/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.7073 - accuracy: 0.7556 - val_loss: 0.6731 - val_accuracy: 0.7724 - lr: 1.2500e-04\n",
            "Epoch 22/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.6995 - accuracy: 0.7580 - val_loss: 0.6872 - val_accuracy: 0.7687 - lr: 1.2500e-04\n",
            "Epoch 23/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.6959 - accuracy: 0.7588 - val_loss: 0.6727 - val_accuracy: 0.7719 - lr: 1.2500e-04\n",
            "Epoch 24/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.6913 - accuracy: 0.7627 - val_loss: 0.6746 - val_accuracy: 0.7705 - lr: 1.2500e-04\n",
            "Epoch 25/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.6884 - accuracy: 0.7614 - val_loss: 0.6791 - val_accuracy: 0.7717 - lr: 1.2500e-04\n",
            "Epoch 26/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.6865 - accuracy: 0.7632 - val_loss: 0.6804 - val_accuracy: 0.7691 - lr: 1.2500e-04\n",
            "Epoch 27/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.6847 - accuracy: 0.7608 - val_loss: 0.6604 - val_accuracy: 0.7767 - lr: 1.2500e-04\n",
            "Epoch 28/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.6824 - accuracy: 0.7647 - val_loss: 0.6771 - val_accuracy: 0.7704 - lr: 1.2500e-04\n",
            "Epoch 29/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.6772 - accuracy: 0.7669 - val_loss: 0.6802 - val_accuracy: 0.7694 - lr: 1.2500e-04\n",
            "Epoch 30/30\n",
            "1250/1250 [==============================] - 20s 16ms/step - loss: 0.6790 - accuracy: 0.7659 - val_loss: 0.6719 - val_accuracy: 0.7723 - lr: 1.2500e-04\n",
            "Score for fold 4: loss of 0.6719434261322021; accuracy of 77.23000049591064%\n",
            "Model: \"sequential_44\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_128 (Conv2D)         (None, 32, 32, 64)        1792      \n",
            "                                                                 \n",
            " max_pooling2d_44 (MaxPoolin  multiple                 0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_129 (Conv2D)         (None, 16, 16, 128)       73856     \n",
            "                                                                 \n",
            " dropout_44 (Dropout)        (None, 8, 8, 128)         0         \n",
            "                                                                 \n",
            " flatten_44 (Flatten)        (None, 8192)              0         \n",
            "                                                                 \n",
            " dense_44 (Dense)            (None, 10)                81930     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 157,578\n",
            "Trainable params: 157,578\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/30\n",
            "1250/1250 [==============================] - 21s 16ms/step - loss: 1.5524 - accuracy: 0.4437 - val_loss: 1.2169 - val_accuracy: 0.5757 - lr: 0.0010\n",
            "Epoch 2/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 1.2567 - accuracy: 0.5572 - val_loss: 1.0817 - val_accuracy: 0.6242 - lr: 0.0010\n",
            "Epoch 3/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 1.1364 - accuracy: 0.5990 - val_loss: 1.0470 - val_accuracy: 0.6370 - lr: 0.0010\n",
            "Epoch 4/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 1.0667 - accuracy: 0.6282 - val_loss: 0.9402 - val_accuracy: 0.6732 - lr: 0.0010\n",
            "Epoch 5/30\n",
            "1250/1250 [==============================] - 20s 16ms/step - loss: 1.0272 - accuracy: 0.6413 - val_loss: 0.9039 - val_accuracy: 0.6920 - lr: 0.0010\n",
            "Epoch 6/30\n",
            "1250/1250 [==============================] - 20s 16ms/step - loss: 0.9856 - accuracy: 0.6549 - val_loss: 0.9634 - val_accuracy: 0.6708 - lr: 0.0010\n",
            "Epoch 7/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.9589 - accuracy: 0.6648 - val_loss: 0.8535 - val_accuracy: 0.7069 - lr: 0.0010\n",
            "Epoch 8/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.9320 - accuracy: 0.6750 - val_loss: 0.8351 - val_accuracy: 0.7112 - lr: 0.0010\n",
            "Epoch 9/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 0.9085 - accuracy: 0.6844 - val_loss: 0.8022 - val_accuracy: 0.7246 - lr: 0.0010\n",
            "Epoch 10/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.8884 - accuracy: 0.6914 - val_loss: 0.8496 - val_accuracy: 0.7095 - lr: 0.0010\n",
            "Epoch 11/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.8241 - accuracy: 0.7142 - val_loss: 0.7758 - val_accuracy: 0.7303 - lr: 5.0000e-04\n",
            "Epoch 12/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.8084 - accuracy: 0.7201 - val_loss: 0.7472 - val_accuracy: 0.7412 - lr: 5.0000e-04\n",
            "Epoch 13/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.7955 - accuracy: 0.7238 - val_loss: 0.7827 - val_accuracy: 0.7314 - lr: 5.0000e-04\n",
            "Epoch 14/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 0.7872 - accuracy: 0.7257 - val_loss: 0.7700 - val_accuracy: 0.7369 - lr: 5.0000e-04\n",
            "Epoch 15/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.7768 - accuracy: 0.7306 - val_loss: 0.7655 - val_accuracy: 0.7406 - lr: 5.0000e-04\n",
            "Epoch 16/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.7614 - accuracy: 0.7353 - val_loss: 0.7553 - val_accuracy: 0.7373 - lr: 5.0000e-04\n",
            "Epoch 17/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.7619 - accuracy: 0.7349 - val_loss: 0.7055 - val_accuracy: 0.7552 - lr: 5.0000e-04\n",
            "Epoch 18/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.7507 - accuracy: 0.7385 - val_loss: 0.7746 - val_accuracy: 0.7378 - lr: 5.0000e-04\n",
            "Epoch 19/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 0.7533 - accuracy: 0.7383 - val_loss: 0.6933 - val_accuracy: 0.7628 - lr: 5.0000e-04\n",
            "Epoch 20/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.7371 - accuracy: 0.7455 - val_loss: 0.7174 - val_accuracy: 0.7585 - lr: 5.0000e-04\n",
            "Epoch 21/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.6956 - accuracy: 0.7581 - val_loss: 0.6976 - val_accuracy: 0.7620 - lr: 1.2500e-04\n",
            "Epoch 22/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.6855 - accuracy: 0.7625 - val_loss: 0.6854 - val_accuracy: 0.7685 - lr: 1.2500e-04\n",
            "Epoch 23/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.6919 - accuracy: 0.7608 - val_loss: 0.6812 - val_accuracy: 0.7678 - lr: 1.2500e-04\n",
            "Epoch 24/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.6799 - accuracy: 0.7681 - val_loss: 0.6773 - val_accuracy: 0.7703 - lr: 1.2500e-04\n",
            "Epoch 25/30\n",
            "1250/1250 [==============================] - 20s 16ms/step - loss: 0.6789 - accuracy: 0.7642 - val_loss: 0.6766 - val_accuracy: 0.7735 - lr: 1.2500e-04\n",
            "Epoch 26/30\n",
            "1250/1250 [==============================] - 22s 17ms/step - loss: 0.6781 - accuracy: 0.7655 - val_loss: 0.6806 - val_accuracy: 0.7710 - lr: 1.2500e-04\n",
            "Epoch 27/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.6748 - accuracy: 0.7688 - val_loss: 0.6875 - val_accuracy: 0.7682 - lr: 1.2500e-04\n",
            "Epoch 28/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.6695 - accuracy: 0.7676 - val_loss: 0.6726 - val_accuracy: 0.7724 - lr: 1.2500e-04\n",
            "Epoch 29/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 0.6674 - accuracy: 0.7713 - val_loss: 0.6737 - val_accuracy: 0.7730 - lr: 1.2500e-04\n",
            "Epoch 30/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.6646 - accuracy: 0.7687 - val_loss: 0.6827 - val_accuracy: 0.7701 - lr: 1.2500e-04\n",
            "Score for fold 5: loss of 0.6827021837234497; accuracy of 77.00999975204468%\n",
            "------------------------------------------------------------------------\n",
            "Score per fold\n",
            "------------------------------------------------------------------------\n",
            "> Fold 1 - Loss: 0.7135555148124695 - Accuracy: 75.6600022315979%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 2 - Loss: 0.6920393705368042 - Accuracy: 76.39999985694885%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 3 - Loss: 0.6919037699699402 - Accuracy: 76.74999833106995%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 4 - Loss: 0.6719434261322021 - Accuracy: 77.23000049591064%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 5 - Loss: 0.6827021837234497 - Accuracy: 77.00999975204468%\n",
            "------------------------------------------------------------------------\n",
            "Average scores for all folds:\n",
            "> Accuracy: 76.6100001335144 (+- 0.5496536671974457)\n",
            "> Loss: 0.6904288530349731\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2 hidden layers and Middle Structure"
      ],
      "metadata": {
        "id": "RYbtbA0G6u6H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "acc_per_fold = []\n",
        "loss_per_fold = []\n",
        "\n",
        "with tf.device('/device:GPU:0'):\n",
        "  fold_no = 1\n",
        "  for train, valid in kfold.split(trainX, trainy):\n",
        "\n",
        "    model = CNN_Model(batch_size=32, depth=2, hidden_neurons_1=32, hidden_neurons_2=64, hidden_neurons_3=128, kernel_size_1=3, kernel_size_2=3, kernel_size_3=3, \n",
        "               dropout_rate=0.2, batch_norm=False, pooling='max', layer_activation_1='relu', layer_activation_2='relu', layer_activation_3='relu',\n",
        "               kernel_initialiser='glorot_uniform', bias_initialiser='zeros', optimiser='Adam', weight_decay=None, learning_rate=1e-3, output_activation='softmax', SGD_momentum=0) \n",
        "  \n",
        "    train_gen = tf.keras.preprocessing.image.ImageDataGenerator(width_shift_range=0.1,\n",
        "      height_shift_range=0.1,\n",
        "      horizontal_flip=True)\n",
        "    train_gen.fit(trainX[train])\n",
        "  \n",
        "    print('------------------------------------------------------------------------')\n",
        "    print(f'Training for fold {fold_no} ...')\n",
        "\n",
        "    history = model.train(\n",
        "              train_gen,\n",
        "                trainX[train],\n",
        "                  trainy[train],\n",
        "                trainX[valid],\n",
        "                  trainy[valid],\n",
        "                  lr_rate_type='step')\n",
        "\n",
        "    scores = model.evaluate(trainX[valid], trainy[valid])\n",
        "\n",
        "    print(f'Score for fold {fold_no}: loss of {scores[0]}; accuracy of {scores[1]*100}%')\n",
        "    acc_per_fold.append(scores[1] * 100)\n",
        "    loss_per_fold.append(scores[0])\n",
        "\n",
        "    fold_no+=1\n",
        "\n",
        "# == Provide average scores ==\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Score per fold')\n",
        "for i in range(0, len(acc_per_fold)):\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Average scores for all folds:')\n",
        "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
        "print(f'> Loss: {np.mean(loss_per_fold)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WV7ugawI6wv3",
        "outputId": "13e80a06-d05e-4a9d-b6e4-8cf1c40bcdeb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_45\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_130 (Conv2D)         (None, 32, 32, 32)        896       \n",
            "                                                                 \n",
            " max_pooling2d_45 (MaxPoolin  multiple                 0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_131 (Conv2D)         (None, 16, 16, 64)        18496     \n",
            "                                                                 \n",
            " dropout_45 (Dropout)        (None, 8, 8, 64)          0         \n",
            "                                                                 \n",
            " flatten_45 (Flatten)        (None, 4096)              0         \n",
            "                                                                 \n",
            " dense_45 (Dense)            (None, 10)                40970     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 60,362\n",
            "Trainable params: 60,362\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 1.5664 - accuracy: 0.4373 - val_loss: 1.3238 - val_accuracy: 0.5405 - lr: 0.0010\n",
            "Epoch 2/30\n",
            "1250/1250 [==============================] - 20s 16ms/step - loss: 1.2716 - accuracy: 0.5536 - val_loss: 1.1746 - val_accuracy: 0.5907 - lr: 0.0010\n",
            "Epoch 3/30\n",
            "1250/1250 [==============================] - 20s 16ms/step - loss: 1.1647 - accuracy: 0.5929 - val_loss: 1.0584 - val_accuracy: 0.6357 - lr: 0.0010\n",
            "Epoch 4/30\n",
            "1250/1250 [==============================] - 20s 16ms/step - loss: 1.1022 - accuracy: 0.6174 - val_loss: 1.0087 - val_accuracy: 0.6476 - lr: 0.0010\n",
            "Epoch 5/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 1.0641 - accuracy: 0.6271 - val_loss: 0.9478 - val_accuracy: 0.6735 - lr: 0.0010\n",
            "Epoch 6/30\n",
            "1250/1250 [==============================] - 21s 16ms/step - loss: 1.0264 - accuracy: 0.6430 - val_loss: 0.9727 - val_accuracy: 0.6658 - lr: 0.0010\n",
            "Epoch 7/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 0.9938 - accuracy: 0.6556 - val_loss: 0.9515 - val_accuracy: 0.6729 - lr: 0.0010\n",
            "Epoch 8/30\n",
            "1250/1250 [==============================] - 23s 18ms/step - loss: 0.9775 - accuracy: 0.6605 - val_loss: 0.9034 - val_accuracy: 0.6924 - lr: 0.0010\n",
            "Epoch 9/30\n",
            "1250/1250 [==============================] - 22s 18ms/step - loss: 0.9571 - accuracy: 0.6680 - val_loss: 0.8732 - val_accuracy: 0.6934 - lr: 0.0010\n",
            "Epoch 10/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.9441 - accuracy: 0.6719 - val_loss: 0.8893 - val_accuracy: 0.6950 - lr: 0.0010\n",
            "Epoch 11/30\n",
            "1250/1250 [==============================] - 21s 17ms/step - loss: 0.8860 - accuracy: 0.6925 - val_loss: 0.8103 - val_accuracy: 0.7218 - lr: 5.0000e-04\n",
            "Epoch 12/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.8693 - accuracy: 0.6992 - val_loss: 0.8444 - val_accuracy: 0.7132 - lr: 5.0000e-04\n",
            "Epoch 13/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.8593 - accuracy: 0.7033 - val_loss: 0.8438 - val_accuracy: 0.7127 - lr: 5.0000e-04\n",
            "Epoch 14/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.8508 - accuracy: 0.7059 - val_loss: 0.7963 - val_accuracy: 0.7277 - lr: 5.0000e-04\n",
            "Epoch 15/30\n",
            "1250/1250 [==============================] - 20s 16ms/step - loss: 0.8458 - accuracy: 0.7080 - val_loss: 0.7921 - val_accuracy: 0.7278 - lr: 5.0000e-04\n",
            "Epoch 16/30\n",
            "1250/1250 [==============================] - 21s 17ms/step - loss: 0.8386 - accuracy: 0.7086 - val_loss: 0.8041 - val_accuracy: 0.7210 - lr: 5.0000e-04\n",
            "Epoch 17/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.8291 - accuracy: 0.7136 - val_loss: 0.7845 - val_accuracy: 0.7302 - lr: 5.0000e-04\n",
            "Epoch 18/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.8287 - accuracy: 0.7146 - val_loss: 0.7888 - val_accuracy: 0.7284 - lr: 5.0000e-04\n",
            "Epoch 19/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.8132 - accuracy: 0.7195 - val_loss: 0.7587 - val_accuracy: 0.7413 - lr: 5.0000e-04\n",
            "Epoch 20/30\n",
            "1250/1250 [==============================] - 20s 16ms/step - loss: 0.8137 - accuracy: 0.7195 - val_loss: 0.7736 - val_accuracy: 0.7390 - lr: 5.0000e-04\n",
            "Epoch 21/30\n",
            "1250/1250 [==============================] - 20s 16ms/step - loss: 0.7797 - accuracy: 0.7279 - val_loss: 0.7637 - val_accuracy: 0.7389 - lr: 1.2500e-04\n",
            "Epoch 22/30\n",
            "1250/1250 [==============================] - 19s 16ms/step - loss: 0.7703 - accuracy: 0.7338 - val_loss: 0.7445 - val_accuracy: 0.7475 - lr: 1.2500e-04\n",
            "Epoch 23/30\n",
            "1250/1250 [==============================] - 20s 16ms/step - loss: 0.7635 - accuracy: 0.7338 - val_loss: 0.7627 - val_accuracy: 0.7439 - lr: 1.2500e-04\n",
            "Epoch 24/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 0.7658 - accuracy: 0.7379 - val_loss: 0.7528 - val_accuracy: 0.7460 - lr: 1.2500e-04\n",
            "Epoch 25/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.7630 - accuracy: 0.7358 - val_loss: 0.7529 - val_accuracy: 0.7413 - lr: 1.2500e-04\n",
            "Epoch 26/30\n",
            "1250/1250 [==============================] - 20s 16ms/step - loss: 0.7660 - accuracy: 0.7372 - val_loss: 0.7453 - val_accuracy: 0.7464 - lr: 1.2500e-04\n",
            "Epoch 27/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.7564 - accuracy: 0.7368 - val_loss: 0.7335 - val_accuracy: 0.7546 - lr: 1.2500e-04\n",
            "Epoch 28/30\n",
            "1250/1250 [==============================] - 20s 16ms/step - loss: 0.7575 - accuracy: 0.7390 - val_loss: 0.7483 - val_accuracy: 0.7479 - lr: 1.2500e-04\n",
            "Epoch 29/30\n",
            "1250/1250 [==============================] - 20s 16ms/step - loss: 0.7564 - accuracy: 0.7393 - val_loss: 0.7249 - val_accuracy: 0.7552 - lr: 1.2500e-04\n",
            "Epoch 30/30\n",
            "1250/1250 [==============================] - 20s 16ms/step - loss: 0.7479 - accuracy: 0.7403 - val_loss: 0.7110 - val_accuracy: 0.7564 - lr: 1.2500e-04\n",
            "Score for fold 1: loss of 0.7110388875007629; accuracy of 75.63999891281128%\n",
            "Model: \"sequential_46\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_132 (Conv2D)         (None, 32, 32, 32)        896       \n",
            "                                                                 \n",
            " max_pooling2d_46 (MaxPoolin  multiple                 0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_133 (Conv2D)         (None, 16, 16, 64)        18496     \n",
            "                                                                 \n",
            " dropout_46 (Dropout)        (None, 8, 8, 64)          0         \n",
            "                                                                 \n",
            " flatten_46 (Flatten)        (None, 4096)              0         \n",
            "                                                                 \n",
            " dense_46 (Dense)            (None, 10)                40970     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 60,362\n",
            "Trainable params: 60,362\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/30\n",
            "1250/1250 [==============================] - 20s 15ms/step - loss: 1.6120 - accuracy: 0.4242 - val_loss: 1.3266 - val_accuracy: 0.5390 - lr: 0.0010\n",
            "Epoch 2/30\n",
            "1250/1250 [==============================] - 20s 16ms/step - loss: 1.3247 - accuracy: 0.5283 - val_loss: 1.2089 - val_accuracy: 0.5827 - lr: 0.0010\n",
            "Epoch 3/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 1.2177 - accuracy: 0.5709 - val_loss: 1.0775 - val_accuracy: 0.6269 - lr: 0.0010\n",
            "Epoch 4/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 1.1460 - accuracy: 0.5996 - val_loss: 1.0402 - val_accuracy: 0.6445 - lr: 0.0010\n",
            "Epoch 5/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 1.1019 - accuracy: 0.6146 - val_loss: 0.9743 - val_accuracy: 0.6609 - lr: 0.0010\n",
            "Epoch 6/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 1.0708 - accuracy: 0.6266 - val_loss: 0.9739 - val_accuracy: 0.6672 - lr: 0.0010\n",
            "Epoch 7/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.0375 - accuracy: 0.6372 - val_loss: 0.9365 - val_accuracy: 0.6764 - lr: 0.0010\n",
            "Epoch 8/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 1.0090 - accuracy: 0.6481 - val_loss: 0.9292 - val_accuracy: 0.6764 - lr: 0.0010\n",
            "Epoch 9/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 1.0008 - accuracy: 0.6520 - val_loss: 0.8979 - val_accuracy: 0.6972 - lr: 0.0010\n",
            "Epoch 10/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.9790 - accuracy: 0.6605 - val_loss: 0.8874 - val_accuracy: 0.6930 - lr: 0.0010\n",
            "Epoch 11/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.9280 - accuracy: 0.6787 - val_loss: 0.8427 - val_accuracy: 0.7079 - lr: 5.0000e-04\n",
            "Epoch 12/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.9129 - accuracy: 0.6819 - val_loss: 0.8178 - val_accuracy: 0.7191 - lr: 5.0000e-04\n",
            "Epoch 13/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.9027 - accuracy: 0.6878 - val_loss: 0.8460 - val_accuracy: 0.7071 - lr: 5.0000e-04\n",
            "Epoch 14/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 0.8930 - accuracy: 0.6917 - val_loss: 0.8149 - val_accuracy: 0.7202 - lr: 5.0000e-04\n",
            "Epoch 15/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.8892 - accuracy: 0.6917 - val_loss: 0.7785 - val_accuracy: 0.7330 - lr: 5.0000e-04\n",
            "Epoch 16/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.8801 - accuracy: 0.6902 - val_loss: 0.8244 - val_accuracy: 0.7168 - lr: 5.0000e-04\n",
            "Epoch 17/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.8727 - accuracy: 0.6975 - val_loss: 0.8019 - val_accuracy: 0.7210 - lr: 5.0000e-04\n",
            "Epoch 18/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.8615 - accuracy: 0.6994 - val_loss: 0.7793 - val_accuracy: 0.7328 - lr: 5.0000e-04\n",
            "Epoch 19/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 0.8556 - accuracy: 0.7017 - val_loss: 0.7929 - val_accuracy: 0.7271 - lr: 5.0000e-04\n",
            "Epoch 20/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.8556 - accuracy: 0.7013 - val_loss: 0.7869 - val_accuracy: 0.7290 - lr: 5.0000e-04\n",
            "Epoch 21/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.8233 - accuracy: 0.7133 - val_loss: 0.7536 - val_accuracy: 0.7376 - lr: 1.2500e-04\n",
            "Epoch 22/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.8114 - accuracy: 0.7181 - val_loss: 0.7656 - val_accuracy: 0.7339 - lr: 1.2500e-04\n",
            "Epoch 23/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.8043 - accuracy: 0.7206 - val_loss: 0.7683 - val_accuracy: 0.7362 - lr: 1.2500e-04\n",
            "Epoch 24/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.8062 - accuracy: 0.7200 - val_loss: 0.7663 - val_accuracy: 0.7347 - lr: 1.2500e-04\n",
            "Epoch 25/30\n",
            "1250/1250 [==============================] - 20s 16ms/step - loss: 0.8066 - accuracy: 0.7220 - val_loss: 0.7594 - val_accuracy: 0.7384 - lr: 1.2500e-04\n",
            "Epoch 26/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.8033 - accuracy: 0.7223 - val_loss: 0.7552 - val_accuracy: 0.7411 - lr: 1.2500e-04\n",
            "Epoch 27/30\n",
            "1250/1250 [==============================] - 19s 16ms/step - loss: 0.8054 - accuracy: 0.7244 - val_loss: 0.7412 - val_accuracy: 0.7447 - lr: 1.2500e-04\n",
            "Epoch 28/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.8000 - accuracy: 0.7215 - val_loss: 0.7524 - val_accuracy: 0.7408 - lr: 1.2500e-04\n",
            "Epoch 29/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.7977 - accuracy: 0.7235 - val_loss: 0.7423 - val_accuracy: 0.7435 - lr: 1.2500e-04\n",
            "Epoch 30/30\n",
            "1250/1250 [==============================] - 19s 16ms/step - loss: 0.7983 - accuracy: 0.7241 - val_loss: 0.7421 - val_accuracy: 0.7447 - lr: 1.2500e-04\n",
            "Score for fold 2: loss of 0.7420842051506042; accuracy of 74.4700014591217%\n",
            "Model: \"sequential_47\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_134 (Conv2D)         (None, 32, 32, 32)        896       \n",
            "                                                                 \n",
            " max_pooling2d_47 (MaxPoolin  multiple                 0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_135 (Conv2D)         (None, 16, 16, 64)        18496     \n",
            "                                                                 \n",
            " dropout_47 (Dropout)        (None, 8, 8, 64)          0         \n",
            "                                                                 \n",
            " flatten_47 (Flatten)        (None, 4096)              0         \n",
            "                                                                 \n",
            " dense_47 (Dense)            (None, 10)                40970     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 60,362\n",
            "Trainable params: 60,362\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/30\n",
            "1250/1250 [==============================] - 22s 17ms/step - loss: 1.5770 - accuracy: 0.4368 - val_loss: 1.2531 - val_accuracy: 0.5639 - lr: 0.0010\n",
            "Epoch 2/30\n",
            "1250/1250 [==============================] - 23s 19ms/step - loss: 1.3019 - accuracy: 0.5396 - val_loss: 1.1353 - val_accuracy: 0.6057 - lr: 0.0010\n",
            "Epoch 3/30\n",
            "1250/1250 [==============================] - 20s 16ms/step - loss: 1.1982 - accuracy: 0.5783 - val_loss: 1.0344 - val_accuracy: 0.6419 - lr: 0.0010\n",
            "Epoch 4/30\n",
            "1250/1250 [==============================] - 20s 16ms/step - loss: 1.1299 - accuracy: 0.6058 - val_loss: 0.9953 - val_accuracy: 0.6590 - lr: 0.0010\n",
            "Epoch 5/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 1.0879 - accuracy: 0.6213 - val_loss: 0.9888 - val_accuracy: 0.6611 - lr: 0.0010\n",
            "Epoch 6/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 1.0461 - accuracy: 0.6359 - val_loss: 0.9514 - val_accuracy: 0.6740 - lr: 0.0010\n",
            "Epoch 7/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 1.0224 - accuracy: 0.6443 - val_loss: 0.8738 - val_accuracy: 0.7000 - lr: 0.0010\n",
            "Epoch 8/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 1.0035 - accuracy: 0.6503 - val_loss: 0.9325 - val_accuracy: 0.6717 - lr: 0.0010\n",
            "Epoch 9/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.9769 - accuracy: 0.6586 - val_loss: 0.8720 - val_accuracy: 0.6989 - lr: 0.0010\n",
            "Epoch 10/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.9583 - accuracy: 0.6673 - val_loss: 0.8917 - val_accuracy: 0.7015 - lr: 0.0010\n",
            "Epoch 11/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.9077 - accuracy: 0.6858 - val_loss: 0.8123 - val_accuracy: 0.7261 - lr: 5.0000e-04\n",
            "Epoch 12/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.8890 - accuracy: 0.6902 - val_loss: 0.7795 - val_accuracy: 0.7344 - lr: 5.0000e-04\n",
            "Epoch 13/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.8788 - accuracy: 0.6936 - val_loss: 0.8379 - val_accuracy: 0.7159 - lr: 5.0000e-04\n",
            "Epoch 14/30\n",
            "1250/1250 [==============================] - 20s 16ms/step - loss: 0.8704 - accuracy: 0.6979 - val_loss: 0.8184 - val_accuracy: 0.7191 - lr: 5.0000e-04\n",
            "Epoch 15/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 0.8627 - accuracy: 0.7004 - val_loss: 0.8123 - val_accuracy: 0.7234 - lr: 5.0000e-04\n",
            "Epoch 16/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.8513 - accuracy: 0.7057 - val_loss: 0.7576 - val_accuracy: 0.7414 - lr: 5.0000e-04\n",
            "Epoch 17/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.8492 - accuracy: 0.7052 - val_loss: 0.7928 - val_accuracy: 0.7287 - lr: 5.0000e-04\n",
            "Epoch 18/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.8439 - accuracy: 0.7066 - val_loss: 0.7806 - val_accuracy: 0.7362 - lr: 5.0000e-04\n",
            "Epoch 19/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.8368 - accuracy: 0.7114 - val_loss: 0.7891 - val_accuracy: 0.7351 - lr: 5.0000e-04\n",
            "Epoch 20/30\n",
            "1250/1250 [==============================] - 23s 18ms/step - loss: 0.8303 - accuracy: 0.7122 - val_loss: 0.7821 - val_accuracy: 0.7369 - lr: 5.0000e-04\n",
            "Epoch 21/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.7906 - accuracy: 0.7254 - val_loss: 0.7551 - val_accuracy: 0.7413 - lr: 1.2500e-04\n",
            "Epoch 22/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.7833 - accuracy: 0.7277 - val_loss: 0.7208 - val_accuracy: 0.7587 - lr: 1.2500e-04\n",
            "Epoch 23/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.7802 - accuracy: 0.7307 - val_loss: 0.7431 - val_accuracy: 0.7501 - lr: 1.2500e-04\n",
            "Epoch 24/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.7791 - accuracy: 0.7299 - val_loss: 0.7348 - val_accuracy: 0.7523 - lr: 1.2500e-04\n",
            "Epoch 25/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.7733 - accuracy: 0.7321 - val_loss: 0.7412 - val_accuracy: 0.7494 - lr: 1.2500e-04\n",
            "Epoch 26/30\n",
            "1250/1250 [==============================] - 20s 16ms/step - loss: 0.7735 - accuracy: 0.7339 - val_loss: 0.7356 - val_accuracy: 0.7532 - lr: 1.2500e-04\n",
            "Epoch 27/30\n",
            "1250/1250 [==============================] - 20s 16ms/step - loss: 0.7739 - accuracy: 0.7325 - val_loss: 0.7280 - val_accuracy: 0.7540 - lr: 1.2500e-04\n",
            "Epoch 28/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 0.7725 - accuracy: 0.7328 - val_loss: 0.7207 - val_accuracy: 0.7572 - lr: 1.2500e-04\n",
            "Epoch 29/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.7715 - accuracy: 0.7314 - val_loss: 0.7326 - val_accuracy: 0.7546 - lr: 1.2500e-04\n",
            "Epoch 30/30\n",
            "1250/1250 [==============================] - 22s 17ms/step - loss: 0.7642 - accuracy: 0.7352 - val_loss: 0.7257 - val_accuracy: 0.7592 - lr: 1.2500e-04\n",
            "Score for fold 3: loss of 0.7256683111190796; accuracy of 75.91999769210815%\n",
            "Model: \"sequential_48\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_136 (Conv2D)         (None, 32, 32, 32)        896       \n",
            "                                                                 \n",
            " max_pooling2d_48 (MaxPoolin  multiple                 0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_137 (Conv2D)         (None, 16, 16, 64)        18496     \n",
            "                                                                 \n",
            " dropout_48 (Dropout)        (None, 8, 8, 64)          0         \n",
            "                                                                 \n",
            " flatten_48 (Flatten)        (None, 4096)              0         \n",
            "                                                                 \n",
            " dense_48 (Dense)            (None, 10)                40970     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 60,362\n",
            "Trainable params: 60,362\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/30\n",
            "1250/1250 [==============================] - 22s 17ms/step - loss: 1.5799 - accuracy: 0.4351 - val_loss: 1.2495 - val_accuracy: 0.5613 - lr: 0.0010\n",
            "Epoch 2/30\n",
            "1250/1250 [==============================] - 22s 18ms/step - loss: 1.2872 - accuracy: 0.5483 - val_loss: 1.1646 - val_accuracy: 0.5943 - lr: 0.0010\n",
            "Epoch 3/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 1.1818 - accuracy: 0.5854 - val_loss: 1.0615 - val_accuracy: 0.6326 - lr: 0.0010\n",
            "Epoch 4/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 1.1264 - accuracy: 0.6051 - val_loss: 1.0146 - val_accuracy: 0.6528 - lr: 0.0010\n",
            "Epoch 5/30\n",
            "1250/1250 [==============================] - 21s 17ms/step - loss: 1.0793 - accuracy: 0.6244 - val_loss: 0.9839 - val_accuracy: 0.6604 - lr: 0.0010\n",
            "Epoch 6/30\n",
            "1250/1250 [==============================] - 20s 16ms/step - loss: 1.0441 - accuracy: 0.6343 - val_loss: 0.9193 - val_accuracy: 0.6816 - lr: 0.0010\n",
            "Epoch 7/30\n",
            "1250/1250 [==============================] - 20s 16ms/step - loss: 1.0198 - accuracy: 0.6417 - val_loss: 0.9367 - val_accuracy: 0.6804 - lr: 0.0010\n",
            "Epoch 8/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.9976 - accuracy: 0.6529 - val_loss: 0.9173 - val_accuracy: 0.6836 - lr: 0.0010\n",
            "Epoch 9/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.9824 - accuracy: 0.6568 - val_loss: 0.8832 - val_accuracy: 0.6978 - lr: 0.0010\n",
            "Epoch 10/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.9677 - accuracy: 0.6629 - val_loss: 0.8794 - val_accuracy: 0.6949 - lr: 0.0010\n",
            "Epoch 11/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.9066 - accuracy: 0.6848 - val_loss: 0.8504 - val_accuracy: 0.7057 - lr: 5.0000e-04\n",
            "Epoch 12/30\n",
            "1250/1250 [==============================] - 22s 18ms/step - loss: 0.9000 - accuracy: 0.6849 - val_loss: 0.8638 - val_accuracy: 0.7069 - lr: 5.0000e-04\n",
            "Epoch 13/30\n",
            "1250/1250 [==============================] - 19s 16ms/step - loss: 0.8836 - accuracy: 0.6933 - val_loss: 0.8213 - val_accuracy: 0.7194 - lr: 5.0000e-04\n",
            "Epoch 14/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.8764 - accuracy: 0.6946 - val_loss: 0.8163 - val_accuracy: 0.7227 - lr: 5.0000e-04\n",
            "Epoch 15/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.8732 - accuracy: 0.6960 - val_loss: 0.8254 - val_accuracy: 0.7209 - lr: 5.0000e-04\n",
            "Epoch 16/30\n",
            "1250/1250 [==============================] - 20s 16ms/step - loss: 0.8624 - accuracy: 0.7017 - val_loss: 0.8216 - val_accuracy: 0.7217 - lr: 5.0000e-04\n",
            "Epoch 17/30\n",
            "1250/1250 [==============================] - 21s 16ms/step - loss: 0.8544 - accuracy: 0.7024 - val_loss: 0.8125 - val_accuracy: 0.7199 - lr: 5.0000e-04\n",
            "Epoch 18/30\n",
            "1250/1250 [==============================] - 20s 16ms/step - loss: 0.8475 - accuracy: 0.7048 - val_loss: 0.7902 - val_accuracy: 0.7309 - lr: 5.0000e-04\n",
            "Epoch 19/30\n",
            "1250/1250 [==============================] - 20s 16ms/step - loss: 0.8422 - accuracy: 0.7063 - val_loss: 0.8120 - val_accuracy: 0.7217 - lr: 5.0000e-04\n",
            "Epoch 20/30\n",
            "1250/1250 [==============================] - 20s 16ms/step - loss: 0.8321 - accuracy: 0.7108 - val_loss: 0.8478 - val_accuracy: 0.7098 - lr: 5.0000e-04\n",
            "Epoch 21/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 0.8016 - accuracy: 0.7199 - val_loss: 0.7726 - val_accuracy: 0.7397 - lr: 1.2500e-04\n",
            "Epoch 22/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.7948 - accuracy: 0.7234 - val_loss: 0.7939 - val_accuracy: 0.7284 - lr: 1.2500e-04\n",
            "Epoch 23/30\n",
            "1250/1250 [==============================] - 22s 17ms/step - loss: 0.7971 - accuracy: 0.7234 - val_loss: 0.7710 - val_accuracy: 0.7401 - lr: 1.2500e-04\n",
            "Epoch 24/30\n",
            "1250/1250 [==============================] - 20s 16ms/step - loss: 0.7874 - accuracy: 0.7260 - val_loss: 0.7667 - val_accuracy: 0.7398 - lr: 1.2500e-04\n",
            "Epoch 25/30\n",
            "1250/1250 [==============================] - 20s 16ms/step - loss: 0.7860 - accuracy: 0.7278 - val_loss: 0.7969 - val_accuracy: 0.7316 - lr: 1.2500e-04\n",
            "Epoch 26/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.7868 - accuracy: 0.7284 - val_loss: 0.7622 - val_accuracy: 0.7411 - lr: 1.2500e-04\n",
            "Epoch 27/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.7850 - accuracy: 0.7286 - val_loss: 0.7635 - val_accuracy: 0.7405 - lr: 1.2500e-04\n",
            "Epoch 28/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.7803 - accuracy: 0.7293 - val_loss: 0.7829 - val_accuracy: 0.7354 - lr: 1.2500e-04\n",
            "Epoch 29/30\n",
            "1250/1250 [==============================] - 19s 16ms/step - loss: 0.7812 - accuracy: 0.7321 - val_loss: 0.7705 - val_accuracy: 0.7395 - lr: 1.2500e-04\n",
            "Epoch 30/30\n",
            "1250/1250 [==============================] - 20s 16ms/step - loss: 0.7726 - accuracy: 0.7310 - val_loss: 0.7636 - val_accuracy: 0.7406 - lr: 1.2500e-04\n",
            "Score for fold 4: loss of 0.7636081576347351; accuracy of 74.05999898910522%\n",
            "Model: \"sequential_49\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_138 (Conv2D)         (None, 32, 32, 32)        896       \n",
            "                                                                 \n",
            " max_pooling2d_49 (MaxPoolin  multiple                 0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_139 (Conv2D)         (None, 16, 16, 64)        18496     \n",
            "                                                                 \n",
            " dropout_49 (Dropout)        (None, 8, 8, 64)          0         \n",
            "                                                                 \n",
            " flatten_49 (Flatten)        (None, 4096)              0         \n",
            "                                                                 \n",
            " dense_49 (Dense)            (None, 10)                40970     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 60,362\n",
            "Trainable params: 60,362\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/30\n",
            "1250/1250 [==============================] - 21s 17ms/step - loss: 1.5628 - accuracy: 0.4410 - val_loss: 1.2556 - val_accuracy: 0.5559 - lr: 0.0010\n",
            "Epoch 2/30\n",
            "1250/1250 [==============================] - 22s 17ms/step - loss: 1.2735 - accuracy: 0.5515 - val_loss: 1.1525 - val_accuracy: 0.5972 - lr: 0.0010\n",
            "Epoch 3/30\n",
            "1250/1250 [==============================] - 23s 18ms/step - loss: 1.1686 - accuracy: 0.5908 - val_loss: 1.0759 - val_accuracy: 0.6330 - lr: 0.0010\n",
            "Epoch 4/30\n",
            "1250/1250 [==============================] - 20s 16ms/step - loss: 1.1102 - accuracy: 0.6116 - val_loss: 1.0207 - val_accuracy: 0.6527 - lr: 0.0010\n",
            "Epoch 5/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 1.0665 - accuracy: 0.6282 - val_loss: 0.9807 - val_accuracy: 0.6597 - lr: 0.0010\n",
            "Epoch 6/30\n",
            "1250/1250 [==============================] - 21s 17ms/step - loss: 1.0353 - accuracy: 0.6396 - val_loss: 1.0085 - val_accuracy: 0.6548 - lr: 0.0010\n",
            "Epoch 7/30\n",
            "1250/1250 [==============================] - 20s 16ms/step - loss: 1.0065 - accuracy: 0.6521 - val_loss: 0.9135 - val_accuracy: 0.6834 - lr: 0.0010\n",
            "Epoch 8/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 0.9891 - accuracy: 0.6577 - val_loss: 0.9176 - val_accuracy: 0.6884 - lr: 0.0010\n",
            "Epoch 9/30\n",
            "1250/1250 [==============================] - 20s 16ms/step - loss: 0.9716 - accuracy: 0.6612 - val_loss: 0.9447 - val_accuracy: 0.6802 - lr: 0.0010\n",
            "Epoch 10/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.9515 - accuracy: 0.6684 - val_loss: 0.8730 - val_accuracy: 0.6993 - lr: 0.0010\n",
            "Epoch 11/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.8971 - accuracy: 0.6856 - val_loss: 0.8663 - val_accuracy: 0.7061 - lr: 5.0000e-04\n",
            "Epoch 12/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.8840 - accuracy: 0.6916 - val_loss: 0.8408 - val_accuracy: 0.7111 - lr: 5.0000e-04\n",
            "Epoch 13/30\n",
            "1250/1250 [==============================] - 20s 16ms/step - loss: 0.8734 - accuracy: 0.6961 - val_loss: 0.8277 - val_accuracy: 0.7176 - lr: 5.0000e-04\n",
            "Epoch 14/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.8680 - accuracy: 0.7006 - val_loss: 0.8146 - val_accuracy: 0.7208 - lr: 5.0000e-04\n",
            "Epoch 15/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.8609 - accuracy: 0.7033 - val_loss: 0.8276 - val_accuracy: 0.7138 - lr: 5.0000e-04\n",
            "Epoch 16/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.8562 - accuracy: 0.7026 - val_loss: 0.7998 - val_accuracy: 0.7263 - lr: 5.0000e-04\n",
            "Epoch 17/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.8481 - accuracy: 0.7052 - val_loss: 0.8014 - val_accuracy: 0.7266 - lr: 5.0000e-04\n",
            "Epoch 18/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.8393 - accuracy: 0.7077 - val_loss: 0.7921 - val_accuracy: 0.7305 - lr: 5.0000e-04\n",
            "Epoch 19/30\n",
            "1250/1250 [==============================] - 21s 16ms/step - loss: 0.8354 - accuracy: 0.7096 - val_loss: 0.7971 - val_accuracy: 0.7274 - lr: 5.0000e-04\n",
            "Epoch 20/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.8264 - accuracy: 0.7125 - val_loss: 0.8031 - val_accuracy: 0.7232 - lr: 5.0000e-04\n",
            "Epoch 21/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.7909 - accuracy: 0.7263 - val_loss: 0.7567 - val_accuracy: 0.7430 - lr: 1.2500e-04\n",
            "Epoch 22/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.7868 - accuracy: 0.7276 - val_loss: 0.7695 - val_accuracy: 0.7390 - lr: 1.2500e-04\n",
            "Epoch 23/30\n",
            "1250/1250 [==============================] - 19s 16ms/step - loss: 0.7851 - accuracy: 0.7267 - val_loss: 0.7623 - val_accuracy: 0.7411 - lr: 1.2500e-04\n",
            "Epoch 24/30\n",
            "1250/1250 [==============================] - 22s 17ms/step - loss: 0.7801 - accuracy: 0.7299 - val_loss: 0.7633 - val_accuracy: 0.7394 - lr: 1.2500e-04\n",
            "Epoch 25/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 0.7813 - accuracy: 0.7287 - val_loss: 0.7493 - val_accuracy: 0.7466 - lr: 1.2500e-04\n",
            "Epoch 26/30\n",
            "1250/1250 [==============================] - 21s 17ms/step - loss: 0.7738 - accuracy: 0.7302 - val_loss: 0.7605 - val_accuracy: 0.7428 - lr: 1.2500e-04\n",
            "Epoch 27/30\n",
            "1250/1250 [==============================] - 20s 16ms/step - loss: 0.7746 - accuracy: 0.7325 - val_loss: 0.7471 - val_accuracy: 0.7450 - lr: 1.2500e-04\n",
            "Epoch 28/30\n",
            "1250/1250 [==============================] - 20s 16ms/step - loss: 0.7754 - accuracy: 0.7329 - val_loss: 0.7424 - val_accuracy: 0.7474 - lr: 1.2500e-04\n",
            "Epoch 29/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 0.7802 - accuracy: 0.7322 - val_loss: 0.7536 - val_accuracy: 0.7451 - lr: 1.2500e-04\n",
            "Epoch 30/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.7654 - accuracy: 0.7355 - val_loss: 0.7597 - val_accuracy: 0.7409 - lr: 1.2500e-04\n",
            "Score for fold 5: loss of 0.7596668601036072; accuracy of 74.08999800682068%\n",
            "------------------------------------------------------------------------\n",
            "Score per fold\n",
            "------------------------------------------------------------------------\n",
            "> Fold 1 - Loss: 0.7110388875007629 - Accuracy: 75.63999891281128%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 2 - Loss: 0.7420842051506042 - Accuracy: 74.4700014591217%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 3 - Loss: 0.7256683111190796 - Accuracy: 75.91999769210815%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 4 - Loss: 0.7636081576347351 - Accuracy: 74.05999898910522%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 5 - Loss: 0.7596668601036072 - Accuracy: 74.08999800682068%\n",
            "------------------------------------------------------------------------\n",
            "Average scores for all folds:\n",
            "> Accuracy: 74.83599901199341 (+- 0.7891915766092437)\n",
            "> Loss: 0.7404132843017578\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2 hidden layers and Narrow Structure"
      ],
      "metadata": {
        "id": "S978HaJf6xKO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "acc_per_fold = []\n",
        "loss_per_fold = []\n",
        "\n",
        "with tf.device('/device:GPU:0'):\n",
        "  fold_no = 1\n",
        "  for train, valid in kfold.split(trainX, trainy):\n",
        "\n",
        "    model = CNN_Model(batch_size=32, depth=2, hidden_neurons_1=4, hidden_neurons_2=32, hidden_neurons_3=64, kernel_size_1=3, kernel_size_2=3, kernel_size_3=3, \n",
        "               dropout_rate=0.2, batch_norm=False, pooling='max', layer_activation_1='relu', layer_activation_2='relu', layer_activation_3='relu',\n",
        "               kernel_initialiser='glorot_uniform', bias_initialiser='zeros', optimiser='Adam', weight_decay=None, learning_rate=1e-3, output_activation='softmax', SGD_momentum=0) \n",
        "  \n",
        "    train_gen = tf.keras.preprocessing.image.ImageDataGenerator(width_shift_range=0.1,\n",
        "      height_shift_range=0.1,\n",
        "      horizontal_flip=True)\n",
        "    train_gen.fit(trainX[train])\n",
        "  \n",
        "    print('------------------------------------------------------------------------')\n",
        "    print(f'Training for fold {fold_no} ...')\n",
        "\n",
        "    history = model.train(\n",
        "              train_gen,\n",
        "                trainX[train],\n",
        "                  trainy[train],\n",
        "                trainX[valid],\n",
        "                  trainy[valid],\n",
        "                  lr_rate_type='step')\n",
        "\n",
        "    scores = model.evaluate(trainX[valid], trainy[valid])\n",
        "\n",
        "    print(f'Score for fold {fold_no}: loss of {scores[0]}; accuracy of {scores[1]*100}%')\n",
        "    acc_per_fold.append(scores[1] * 100)\n",
        "    loss_per_fold.append(scores[0])\n",
        "\n",
        "    fold_no+=1\n",
        "\n",
        "# == Provide average scores ==\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Score per fold')\n",
        "for i in range(0, len(acc_per_fold)):\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Average scores for all folds:')\n",
        "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
        "print(f'> Loss: {np.mean(loss_per_fold)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t3KX5xzS6zKW",
        "outputId": "8390cc0d-1cdb-440a-cfc4-a084d6d59849"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_50\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_140 (Conv2D)         (None, 32, 32, 4)         112       \n",
            "                                                                 \n",
            " max_pooling2d_50 (MaxPoolin  multiple                 0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_141 (Conv2D)         (None, 16, 16, 32)        1184      \n",
            "                                                                 \n",
            " dropout_50 (Dropout)        (None, 8, 8, 32)          0         \n",
            "                                                                 \n",
            " flatten_50 (Flatten)        (None, 2048)              0         \n",
            "                                                                 \n",
            " dense_50 (Dense)            (None, 10)                20490     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 21,786\n",
            "Trainable params: 21,786\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/30\n",
            "1250/1250 [==============================] - 25s 19ms/step - loss: 1.7291 - accuracy: 0.3803 - val_loss: 1.5215 - val_accuracy: 0.4725 - lr: 0.0010\n",
            "Epoch 2/30\n",
            "1250/1250 [==============================] - 25s 20ms/step - loss: 1.4879 - accuracy: 0.4710 - val_loss: 1.3054 - val_accuracy: 0.5536 - lr: 0.0010\n",
            "Epoch 3/30\n",
            "1250/1250 [==============================] - 20s 16ms/step - loss: 1.4022 - accuracy: 0.5068 - val_loss: 1.2479 - val_accuracy: 0.5777 - lr: 0.0010\n",
            "Epoch 4/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 1.3436 - accuracy: 0.5238 - val_loss: 1.1900 - val_accuracy: 0.5926 - lr: 0.0010\n",
            "Epoch 5/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 1.3108 - accuracy: 0.5393 - val_loss: 1.2069 - val_accuracy: 0.5892 - lr: 0.0010\n",
            "Epoch 6/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 1.2822 - accuracy: 0.5537 - val_loss: 1.1989 - val_accuracy: 0.5938 - lr: 0.0010\n",
            "Epoch 7/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 1.2620 - accuracy: 0.5594 - val_loss: 1.1430 - val_accuracy: 0.6082 - lr: 0.0010\n",
            "Epoch 8/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 1.2373 - accuracy: 0.5695 - val_loss: 1.1193 - val_accuracy: 0.6168 - lr: 0.0010\n",
            "Epoch 9/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 1.2288 - accuracy: 0.5700 - val_loss: 1.1317 - val_accuracy: 0.6122 - lr: 0.0010\n",
            "Epoch 10/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.2118 - accuracy: 0.5783 - val_loss: 1.1743 - val_accuracy: 0.5942 - lr: 0.0010\n",
            "Epoch 11/30\n",
            "1250/1250 [==============================] - 20s 16ms/step - loss: 1.1864 - accuracy: 0.5867 - val_loss: 1.0907 - val_accuracy: 0.6278 - lr: 5.0000e-04\n",
            "Epoch 12/30\n",
            "1250/1250 [==============================] - 19s 16ms/step - loss: 1.1741 - accuracy: 0.5941 - val_loss: 1.0773 - val_accuracy: 0.6287 - lr: 5.0000e-04\n",
            "Epoch 13/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 1.1707 - accuracy: 0.5933 - val_loss: 1.0545 - val_accuracy: 0.6378 - lr: 5.0000e-04\n",
            "Epoch 14/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.1677 - accuracy: 0.5920 - val_loss: 1.0756 - val_accuracy: 0.6320 - lr: 5.0000e-04\n",
            "Epoch 15/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 1.1688 - accuracy: 0.5946 - val_loss: 1.0691 - val_accuracy: 0.6346 - lr: 5.0000e-04\n",
            "Epoch 16/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 1.1546 - accuracy: 0.5990 - val_loss: 1.0862 - val_accuracy: 0.6335 - lr: 5.0000e-04\n",
            "Epoch 17/30\n",
            "1250/1250 [==============================] - 20s 16ms/step - loss: 1.1488 - accuracy: 0.6008 - val_loss: 1.0539 - val_accuracy: 0.6356 - lr: 5.0000e-04\n",
            "Epoch 18/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 1.1567 - accuracy: 0.5990 - val_loss: 1.0506 - val_accuracy: 0.6391 - lr: 5.0000e-04\n",
            "Epoch 19/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 1.1449 - accuracy: 0.6007 - val_loss: 1.0540 - val_accuracy: 0.6419 - lr: 5.0000e-04\n",
            "Epoch 20/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.1483 - accuracy: 0.6014 - val_loss: 1.0808 - val_accuracy: 0.6329 - lr: 5.0000e-04\n",
            "Epoch 21/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 1.1225 - accuracy: 0.6109 - val_loss: 1.0353 - val_accuracy: 0.6525 - lr: 1.2500e-04\n",
            "Epoch 22/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 1.1237 - accuracy: 0.6119 - val_loss: 1.0313 - val_accuracy: 0.6521 - lr: 1.2500e-04\n",
            "Epoch 23/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.1231 - accuracy: 0.6145 - val_loss: 1.0292 - val_accuracy: 0.6501 - lr: 1.2500e-04\n",
            "Epoch 24/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 1.1195 - accuracy: 0.6125 - val_loss: 1.0270 - val_accuracy: 0.6528 - lr: 1.2500e-04\n",
            "Epoch 25/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 1.1246 - accuracy: 0.6081 - val_loss: 1.0403 - val_accuracy: 0.6474 - lr: 1.2500e-04\n",
            "Epoch 26/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 1.1175 - accuracy: 0.6165 - val_loss: 1.0380 - val_accuracy: 0.6463 - lr: 1.2500e-04\n",
            "Epoch 27/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 1.1140 - accuracy: 0.6160 - val_loss: 1.0266 - val_accuracy: 0.6502 - lr: 1.2500e-04\n",
            "Epoch 28/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 1.1156 - accuracy: 0.6131 - val_loss: 1.0271 - val_accuracy: 0.6477 - lr: 1.2500e-04\n",
            "Epoch 29/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 1.1186 - accuracy: 0.6130 - val_loss: 1.0106 - val_accuracy: 0.6584 - lr: 1.2500e-04\n",
            "Epoch 30/30\n",
            "1250/1250 [==============================] - 19s 16ms/step - loss: 1.1197 - accuracy: 0.6138 - val_loss: 1.0215 - val_accuracy: 0.6565 - lr: 1.2500e-04\n",
            "Score for fold 1: loss of 1.021470546722412; accuracy of 65.6499981880188%\n",
            "Model: \"sequential_51\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_142 (Conv2D)         (None, 32, 32, 4)         112       \n",
            "                                                                 \n",
            " max_pooling2d_51 (MaxPoolin  multiple                 0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_143 (Conv2D)         (None, 16, 16, 32)        1184      \n",
            "                                                                 \n",
            " dropout_51 (Dropout)        (None, 8, 8, 32)          0         \n",
            "                                                                 \n",
            " flatten_51 (Flatten)        (None, 2048)              0         \n",
            "                                                                 \n",
            " dense_51 (Dense)            (None, 10)                20490     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 21,786\n",
            "Trainable params: 21,786\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/30\n",
            "1250/1250 [==============================] - 20s 16ms/step - loss: 1.7000 - accuracy: 0.3914 - val_loss: 1.5550 - val_accuracy: 0.4653 - lr: 0.0010\n",
            "Epoch 2/30\n",
            "1250/1250 [==============================] - 22s 18ms/step - loss: 1.4869 - accuracy: 0.4732 - val_loss: 1.3490 - val_accuracy: 0.5322 - lr: 0.0010\n",
            "Epoch 3/30\n",
            "1250/1250 [==============================] - 22s 18ms/step - loss: 1.3861 - accuracy: 0.5097 - val_loss: 1.2772 - val_accuracy: 0.5622 - lr: 0.0010\n",
            "Epoch 4/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 1.3266 - accuracy: 0.5323 - val_loss: 1.2257 - val_accuracy: 0.5770 - lr: 0.0010\n",
            "Epoch 5/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 1.2823 - accuracy: 0.5485 - val_loss: 1.1814 - val_accuracy: 0.5961 - lr: 0.0010\n",
            "Epoch 6/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.2543 - accuracy: 0.5581 - val_loss: 1.1660 - val_accuracy: 0.6006 - lr: 0.0010\n",
            "Epoch 7/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.2267 - accuracy: 0.5691 - val_loss: 1.1338 - val_accuracy: 0.6093 - lr: 0.0010\n",
            "Epoch 8/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 1.2056 - accuracy: 0.5784 - val_loss: 1.1715 - val_accuracy: 0.5961 - lr: 0.0010\n",
            "Epoch 9/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 1.1915 - accuracy: 0.5819 - val_loss: 1.0985 - val_accuracy: 0.6257 - lr: 0.0010\n",
            "Epoch 10/30\n",
            "1250/1250 [==============================] - 22s 17ms/step - loss: 1.1865 - accuracy: 0.5836 - val_loss: 1.1117 - val_accuracy: 0.6175 - lr: 0.0010\n",
            "Epoch 11/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 1.1574 - accuracy: 0.5960 - val_loss: 1.0884 - val_accuracy: 0.6270 - lr: 5.0000e-04\n",
            "Epoch 12/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 1.1511 - accuracy: 0.5990 - val_loss: 1.0680 - val_accuracy: 0.6388 - lr: 5.0000e-04\n",
            "Epoch 13/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 1.1500 - accuracy: 0.5981 - val_loss: 1.0537 - val_accuracy: 0.6416 - lr: 5.0000e-04\n",
            "Epoch 14/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 1.1383 - accuracy: 0.6042 - val_loss: 1.0586 - val_accuracy: 0.6422 - lr: 5.0000e-04\n",
            "Epoch 15/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 1.1312 - accuracy: 0.6062 - val_loss: 1.0443 - val_accuracy: 0.6465 - lr: 5.0000e-04\n",
            "Epoch 16/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 1.1308 - accuracy: 0.6061 - val_loss: 1.0403 - val_accuracy: 0.6468 - lr: 5.0000e-04\n",
            "Epoch 17/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 1.1319 - accuracy: 0.6060 - val_loss: 1.0739 - val_accuracy: 0.6298 - lr: 5.0000e-04\n",
            "Epoch 18/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 1.1226 - accuracy: 0.6103 - val_loss: 1.0734 - val_accuracy: 0.6332 - lr: 5.0000e-04\n",
            "Epoch 19/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 1.1204 - accuracy: 0.6073 - val_loss: 1.0560 - val_accuracy: 0.6409 - lr: 5.0000e-04\n",
            "Epoch 20/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 1.1232 - accuracy: 0.6079 - val_loss: 1.0423 - val_accuracy: 0.6420 - lr: 5.0000e-04\n",
            "Epoch 21/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 1.1020 - accuracy: 0.6169 - val_loss: 1.0365 - val_accuracy: 0.6423 - lr: 1.2500e-04\n",
            "Epoch 22/30\n",
            "1250/1250 [==============================] - 21s 16ms/step - loss: 1.0977 - accuracy: 0.6185 - val_loss: 1.0383 - val_accuracy: 0.6406 - lr: 1.2500e-04\n",
            "Epoch 23/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 1.0993 - accuracy: 0.6172 - val_loss: 1.0626 - val_accuracy: 0.6342 - lr: 1.2500e-04\n",
            "Epoch 24/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 1.0977 - accuracy: 0.6170 - val_loss: 1.0240 - val_accuracy: 0.6520 - lr: 1.2500e-04\n",
            "Epoch 25/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 1.0924 - accuracy: 0.6195 - val_loss: 1.0282 - val_accuracy: 0.6500 - lr: 1.2500e-04\n",
            "Epoch 26/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 1.0899 - accuracy: 0.6204 - val_loss: 1.0242 - val_accuracy: 0.6508 - lr: 1.2500e-04\n",
            "Epoch 27/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 1.0950 - accuracy: 0.6212 - val_loss: 1.0539 - val_accuracy: 0.6401 - lr: 1.2500e-04\n",
            "Epoch 28/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 1.0906 - accuracy: 0.6191 - val_loss: 1.0360 - val_accuracy: 0.6472 - lr: 1.2500e-04\n",
            "Epoch 29/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 1.0920 - accuracy: 0.6208 - val_loss: 1.0409 - val_accuracy: 0.6411 - lr: 1.2500e-04\n",
            "Epoch 30/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 1.0854 - accuracy: 0.6239 - val_loss: 1.0225 - val_accuracy: 0.6515 - lr: 1.2500e-04\n",
            "Score for fold 2: loss of 1.0224931240081787; accuracy of 65.14999866485596%\n",
            "Model: \"sequential_52\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_144 (Conv2D)         (None, 32, 32, 4)         112       \n",
            "                                                                 \n",
            " max_pooling2d_52 (MaxPoolin  multiple                 0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_145 (Conv2D)         (None, 16, 16, 32)        1184      \n",
            "                                                                 \n",
            " dropout_52 (Dropout)        (None, 8, 8, 32)          0         \n",
            "                                                                 \n",
            " flatten_52 (Flatten)        (None, 2048)              0         \n",
            "                                                                 \n",
            " dense_52 (Dense)            (None, 10)                20490     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 21,786\n",
            "Trainable params: 21,786\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/30\n",
            "1250/1250 [==============================] - 20s 15ms/step - loss: 1.7393 - accuracy: 0.3775 - val_loss: 1.5754 - val_accuracy: 0.4532 - lr: 0.0010\n",
            "Epoch 2/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 1.5305 - accuracy: 0.4545 - val_loss: 1.3999 - val_accuracy: 0.5084 - lr: 0.0010\n",
            "Epoch 3/30\n",
            "1250/1250 [==============================] - 23s 18ms/step - loss: 1.4348 - accuracy: 0.4893 - val_loss: 1.4198 - val_accuracy: 0.5124 - lr: 0.0010\n",
            "Epoch 4/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 1.3754 - accuracy: 0.5125 - val_loss: 1.2691 - val_accuracy: 0.5517 - lr: 0.0010\n",
            "Epoch 5/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 1.3326 - accuracy: 0.5278 - val_loss: 1.2470 - val_accuracy: 0.5679 - lr: 0.0010\n",
            "Epoch 6/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 1.2989 - accuracy: 0.5404 - val_loss: 1.1973 - val_accuracy: 0.5833 - lr: 0.0010\n",
            "Epoch 7/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 1.2715 - accuracy: 0.5497 - val_loss: 1.1471 - val_accuracy: 0.6001 - lr: 0.0010\n",
            "Epoch 8/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 1.2496 - accuracy: 0.5605 - val_loss: 1.1673 - val_accuracy: 0.5974 - lr: 0.0010\n",
            "Epoch 9/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 1.2297 - accuracy: 0.5679 - val_loss: 1.1383 - val_accuracy: 0.6082 - lr: 0.0010\n",
            "Epoch 10/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 1.2123 - accuracy: 0.5743 - val_loss: 1.1125 - val_accuracy: 0.6180 - lr: 0.0010\n",
            "Epoch 11/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 1.1835 - accuracy: 0.5837 - val_loss: 1.0651 - val_accuracy: 0.6406 - lr: 5.0000e-04\n",
            "Epoch 12/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.1694 - accuracy: 0.5906 - val_loss: 1.0798 - val_accuracy: 0.6287 - lr: 5.0000e-04\n",
            "Epoch 13/30\n",
            "1250/1250 [==============================] - 20s 16ms/step - loss: 1.1652 - accuracy: 0.5914 - val_loss: 1.0891 - val_accuracy: 0.6260 - lr: 5.0000e-04\n",
            "Epoch 14/30\n",
            "1250/1250 [==============================] - 20s 16ms/step - loss: 1.1506 - accuracy: 0.5978 - val_loss: 1.1066 - val_accuracy: 0.6297 - lr: 5.0000e-04\n",
            "Epoch 15/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 1.1541 - accuracy: 0.5960 - val_loss: 1.1082 - val_accuracy: 0.6258 - lr: 5.0000e-04\n",
            "Epoch 16/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 1.1514 - accuracy: 0.5974 - val_loss: 1.0632 - val_accuracy: 0.6359 - lr: 5.0000e-04\n",
            "Epoch 17/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.1460 - accuracy: 0.5975 - val_loss: 1.0742 - val_accuracy: 0.6316 - lr: 5.0000e-04\n",
            "Epoch 18/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 1.1413 - accuracy: 0.6015 - val_loss: 1.0322 - val_accuracy: 0.6469 - lr: 5.0000e-04\n",
            "Epoch 19/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 1.1346 - accuracy: 0.6042 - val_loss: 1.0756 - val_accuracy: 0.6327 - lr: 5.0000e-04\n",
            "Epoch 20/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 1.1315 - accuracy: 0.6028 - val_loss: 1.1020 - val_accuracy: 0.6266 - lr: 5.0000e-04\n",
            "Epoch 21/30\n",
            "1250/1250 [==============================] - 20s 16ms/step - loss: 1.1072 - accuracy: 0.6127 - val_loss: 1.0269 - val_accuracy: 0.6517 - lr: 1.2500e-04\n",
            "Epoch 22/30\n",
            "1250/1250 [==============================] - 22s 17ms/step - loss: 1.1080 - accuracy: 0.6126 - val_loss: 1.0202 - val_accuracy: 0.6547 - lr: 1.2500e-04\n",
            "Epoch 23/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 1.1085 - accuracy: 0.6137 - val_loss: 1.0231 - val_accuracy: 0.6524 - lr: 1.2500e-04\n",
            "Epoch 24/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 1.1069 - accuracy: 0.6160 - val_loss: 1.0396 - val_accuracy: 0.6477 - lr: 1.2500e-04\n",
            "Epoch 25/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 1.0957 - accuracy: 0.6191 - val_loss: 1.0359 - val_accuracy: 0.6507 - lr: 1.2500e-04\n",
            "Epoch 26/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 1.1020 - accuracy: 0.6176 - val_loss: 1.0342 - val_accuracy: 0.6493 - lr: 1.2500e-04\n",
            "Epoch 27/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 1.0993 - accuracy: 0.6141 - val_loss: 1.0381 - val_accuracy: 0.6488 - lr: 1.2500e-04\n",
            "Epoch 28/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 1.1009 - accuracy: 0.6155 - val_loss: 1.0249 - val_accuracy: 0.6529 - lr: 1.2500e-04\n",
            "Epoch 29/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 1.0970 - accuracy: 0.6173 - val_loss: 1.0105 - val_accuracy: 0.6577 - lr: 1.2500e-04\n",
            "Epoch 30/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 1.0967 - accuracy: 0.6181 - val_loss: 1.0201 - val_accuracy: 0.6556 - lr: 1.2500e-04\n",
            "Score for fold 3: loss of 1.02006995677948; accuracy of 65.56000113487244%\n",
            "Model: \"sequential_53\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_146 (Conv2D)         (None, 32, 32, 4)         112       \n",
            "                                                                 \n",
            " max_pooling2d_53 (MaxPoolin  multiple                 0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_147 (Conv2D)         (None, 16, 16, 32)        1184      \n",
            "                                                                 \n",
            " dropout_53 (Dropout)        (None, 8, 8, 32)          0         \n",
            "                                                                 \n",
            " flatten_53 (Flatten)        (None, 2048)              0         \n",
            "                                                                 \n",
            " dense_53 (Dense)            (None, 10)                20490     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 21,786\n",
            "Trainable params: 21,786\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/30\n",
            "1250/1250 [==============================] - 20s 15ms/step - loss: 1.7326 - accuracy: 0.3784 - val_loss: 1.5340 - val_accuracy: 0.4658 - lr: 0.0010\n",
            "Epoch 2/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 1.5057 - accuracy: 0.4644 - val_loss: 1.3344 - val_accuracy: 0.5323 - lr: 0.0010\n",
            "Epoch 3/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 1.4108 - accuracy: 0.4971 - val_loss: 1.2632 - val_accuracy: 0.5590 - lr: 0.0010\n",
            "Epoch 4/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.3548 - accuracy: 0.5202 - val_loss: 1.2260 - val_accuracy: 0.5719 - lr: 0.0010\n",
            "Epoch 5/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 1.3093 - accuracy: 0.5384 - val_loss: 1.2205 - val_accuracy: 0.5599 - lr: 0.0010\n",
            "Epoch 6/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.2849 - accuracy: 0.5479 - val_loss: 1.1762 - val_accuracy: 0.5946 - lr: 0.0010\n",
            "Epoch 7/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 1.2614 - accuracy: 0.5584 - val_loss: 1.1441 - val_accuracy: 0.6037 - lr: 0.0010\n",
            "Epoch 8/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.2384 - accuracy: 0.5656 - val_loss: 1.1164 - val_accuracy: 0.6056 - lr: 0.0010\n",
            "Epoch 9/30\n",
            "1250/1250 [==============================] - 21s 17ms/step - loss: 1.2254 - accuracy: 0.5704 - val_loss: 1.1025 - val_accuracy: 0.6176 - lr: 0.0010\n",
            "Epoch 10/30\n",
            "1250/1250 [==============================] - 22s 17ms/step - loss: 1.2143 - accuracy: 0.5742 - val_loss: 1.1060 - val_accuracy: 0.6173 - lr: 0.0010\n",
            "Epoch 11/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 1.1775 - accuracy: 0.5887 - val_loss: 1.1082 - val_accuracy: 0.6142 - lr: 5.0000e-04\n",
            "Epoch 12/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 1.1773 - accuracy: 0.5878 - val_loss: 1.0715 - val_accuracy: 0.6287 - lr: 5.0000e-04\n",
            "Epoch 13/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 1.1715 - accuracy: 0.5909 - val_loss: 1.0528 - val_accuracy: 0.6371 - lr: 5.0000e-04\n",
            "Epoch 14/30\n",
            "1250/1250 [==============================] - 20s 16ms/step - loss: 1.1563 - accuracy: 0.5950 - val_loss: 1.0583 - val_accuracy: 0.6349 - lr: 5.0000e-04\n",
            "Epoch 15/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 1.1528 - accuracy: 0.5973 - val_loss: 1.0822 - val_accuracy: 0.6237 - lr: 5.0000e-04\n",
            "Epoch 16/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 1.1471 - accuracy: 0.6020 - val_loss: 1.0763 - val_accuracy: 0.6236 - lr: 5.0000e-04\n",
            "Epoch 17/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 1.1495 - accuracy: 0.5968 - val_loss: 1.0785 - val_accuracy: 0.6251 - lr: 5.0000e-04\n",
            "Epoch 18/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 1.1469 - accuracy: 0.6010 - val_loss: 1.0608 - val_accuracy: 0.6329 - lr: 5.0000e-04\n",
            "Epoch 19/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 1.1386 - accuracy: 0.6014 - val_loss: 1.0618 - val_accuracy: 0.6335 - lr: 5.0000e-04\n",
            "Epoch 20/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 1.1311 - accuracy: 0.6073 - val_loss: 1.0456 - val_accuracy: 0.6356 - lr: 5.0000e-04\n",
            "Epoch 21/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 1.1205 - accuracy: 0.6105 - val_loss: 1.0249 - val_accuracy: 0.6442 - lr: 1.2500e-04\n",
            "Epoch 22/30\n",
            "1250/1250 [==============================] - 19s 16ms/step - loss: 1.1183 - accuracy: 0.6124 - val_loss: 1.0252 - val_accuracy: 0.6486 - lr: 1.2500e-04\n",
            "Epoch 23/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 1.1157 - accuracy: 0.6141 - val_loss: 1.0261 - val_accuracy: 0.6445 - lr: 1.2500e-04\n",
            "Epoch 24/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 1.1147 - accuracy: 0.6113 - val_loss: 1.0336 - val_accuracy: 0.6429 - lr: 1.2500e-04\n",
            "Epoch 25/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.1090 - accuracy: 0.6135 - val_loss: 1.0290 - val_accuracy: 0.6471 - lr: 1.2500e-04\n",
            "Epoch 26/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 1.1135 - accuracy: 0.6149 - val_loss: 1.0349 - val_accuracy: 0.6410 - lr: 1.2500e-04\n",
            "Epoch 27/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 1.1143 - accuracy: 0.6097 - val_loss: 1.0200 - val_accuracy: 0.6456 - lr: 1.2500e-04\n",
            "Epoch 28/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 1.1098 - accuracy: 0.6144 - val_loss: 1.0260 - val_accuracy: 0.6435 - lr: 1.2500e-04\n",
            "Epoch 29/30\n",
            "1250/1250 [==============================] - 21s 17ms/step - loss: 1.1085 - accuracy: 0.6133 - val_loss: 1.0169 - val_accuracy: 0.6472 - lr: 1.2500e-04\n",
            "Epoch 30/30\n",
            "1250/1250 [==============================] - 19s 16ms/step - loss: 1.1079 - accuracy: 0.6161 - val_loss: 1.0196 - val_accuracy: 0.6503 - lr: 1.2500e-04\n",
            "Score for fold 4: loss of 1.0195727348327637; accuracy of 65.03000259399414%\n",
            "Model: \"sequential_54\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_148 (Conv2D)         (None, 32, 32, 4)         112       \n",
            "                                                                 \n",
            " max_pooling2d_54 (MaxPoolin  multiple                 0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_149 (Conv2D)         (None, 16, 16, 32)        1184      \n",
            "                                                                 \n",
            " dropout_54 (Dropout)        (None, 8, 8, 32)          0         \n",
            "                                                                 \n",
            " flatten_54 (Flatten)        (None, 2048)              0         \n",
            "                                                                 \n",
            " dense_54 (Dense)            (None, 10)                20490     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 21,786\n",
            "Trainable params: 21,786\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/30\n",
            "1250/1250 [==============================] - 23s 18ms/step - loss: 1.6945 - accuracy: 0.3853 - val_loss: 1.4772 - val_accuracy: 0.4734 - lr: 0.0010\n",
            "Epoch 2/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 1.4909 - accuracy: 0.4643 - val_loss: 1.4309 - val_accuracy: 0.4938 - lr: 0.0010\n",
            "Epoch 3/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.4211 - accuracy: 0.4949 - val_loss: 1.3287 - val_accuracy: 0.5279 - lr: 0.0010\n",
            "Epoch 4/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.3519 - accuracy: 0.5240 - val_loss: 1.2539 - val_accuracy: 0.5516 - lr: 0.0010\n",
            "Epoch 5/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.3158 - accuracy: 0.5371 - val_loss: 1.2259 - val_accuracy: 0.5626 - lr: 0.0010\n",
            "Epoch 6/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.2833 - accuracy: 0.5488 - val_loss: 1.1973 - val_accuracy: 0.5774 - lr: 0.0010\n",
            "Epoch 7/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.2565 - accuracy: 0.5600 - val_loss: 1.1636 - val_accuracy: 0.5896 - lr: 0.0010\n",
            "Epoch 8/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.2333 - accuracy: 0.5670 - val_loss: 1.1328 - val_accuracy: 0.6052 - lr: 0.0010\n",
            "Epoch 9/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 1.2200 - accuracy: 0.5742 - val_loss: 1.1371 - val_accuracy: 0.5994 - lr: 0.0010\n",
            "Epoch 10/30\n",
            "1250/1250 [==============================] - 20s 16ms/step - loss: 1.2054 - accuracy: 0.5801 - val_loss: 1.1228 - val_accuracy: 0.6158 - lr: 0.0010\n",
            "Epoch 11/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.1786 - accuracy: 0.5899 - val_loss: 1.1203 - val_accuracy: 0.6102 - lr: 5.0000e-04\n",
            "Epoch 12/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.1660 - accuracy: 0.5955 - val_loss: 1.1555 - val_accuracy: 0.6081 - lr: 5.0000e-04\n",
            "Epoch 13/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 1.1596 - accuracy: 0.5947 - val_loss: 1.0827 - val_accuracy: 0.6235 - lr: 5.0000e-04\n",
            "Epoch 14/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 1.1612 - accuracy: 0.5997 - val_loss: 1.0962 - val_accuracy: 0.6146 - lr: 5.0000e-04\n",
            "Epoch 15/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 1.1519 - accuracy: 0.5998 - val_loss: 1.0645 - val_accuracy: 0.6364 - lr: 5.0000e-04\n",
            "Epoch 16/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.1447 - accuracy: 0.6022 - val_loss: 1.0497 - val_accuracy: 0.6324 - lr: 5.0000e-04\n",
            "Epoch 17/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.1428 - accuracy: 0.6021 - val_loss: 1.0542 - val_accuracy: 0.6366 - lr: 5.0000e-04\n",
            "Epoch 18/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.1414 - accuracy: 0.6040 - val_loss: 1.0494 - val_accuracy: 0.6408 - lr: 5.0000e-04\n",
            "Epoch 19/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.1337 - accuracy: 0.6075 - val_loss: 1.0602 - val_accuracy: 0.6353 - lr: 5.0000e-04\n",
            "Epoch 20/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 1.1355 - accuracy: 0.6040 - val_loss: 1.0287 - val_accuracy: 0.6443 - lr: 5.0000e-04\n",
            "Epoch 21/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.1186 - accuracy: 0.6130 - val_loss: 1.0354 - val_accuracy: 0.6471 - lr: 1.2500e-04\n",
            "Epoch 22/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.1120 - accuracy: 0.6141 - val_loss: 1.0395 - val_accuracy: 0.6438 - lr: 1.2500e-04\n",
            "Epoch 23/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.1069 - accuracy: 0.6181 - val_loss: 1.0428 - val_accuracy: 0.6425 - lr: 1.2500e-04\n",
            "Epoch 24/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.1092 - accuracy: 0.6135 - val_loss: 1.0411 - val_accuracy: 0.6429 - lr: 1.2500e-04\n",
            "Epoch 25/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.1082 - accuracy: 0.6178 - val_loss: 1.0401 - val_accuracy: 0.6435 - lr: 1.2500e-04\n",
            "Epoch 26/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.1034 - accuracy: 0.6166 - val_loss: 1.0288 - val_accuracy: 0.6477 - lr: 1.2500e-04\n",
            "Epoch 27/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.1008 - accuracy: 0.6169 - val_loss: 1.0208 - val_accuracy: 0.6498 - lr: 1.2500e-04\n",
            "Epoch 28/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.1019 - accuracy: 0.6187 - val_loss: 1.0388 - val_accuracy: 0.6443 - lr: 1.2500e-04\n",
            "Epoch 29/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 1.1017 - accuracy: 0.6201 - val_loss: 1.0239 - val_accuracy: 0.6497 - lr: 1.2500e-04\n",
            "Epoch 30/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 1.0932 - accuracy: 0.6211 - val_loss: 1.0234 - val_accuracy: 0.6469 - lr: 1.2500e-04\n",
            "Score for fold 5: loss of 1.023415207862854; accuracy of 64.68999981880188%\n",
            "------------------------------------------------------------------------\n",
            "Score per fold\n",
            "------------------------------------------------------------------------\n",
            "> Fold 1 - Loss: 1.021470546722412 - Accuracy: 65.6499981880188%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 2 - Loss: 1.0224931240081787 - Accuracy: 65.14999866485596%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 3 - Loss: 1.02006995677948 - Accuracy: 65.56000113487244%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 4 - Loss: 1.0195727348327637 - Accuracy: 65.03000259399414%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 5 - Loss: 1.023415207862854 - Accuracy: 64.68999981880188%\n",
            "------------------------------------------------------------------------\n",
            "Average scores for all folds:\n",
            "> Accuracy: 65.21600008010864 (+- 0.35279416375536965)\n",
            "> Loss: 1.0214043140411377\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3 hidden layers and Wide Structure"
      ],
      "metadata": {
        "id": "W08AK6Qx6FSA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "acc_per_fold = []\n",
        "loss_per_fold = []\n",
        "\n",
        "with tf.device('/device:GPU:0'):\n",
        "  fold_no = 1\n",
        "  for train, valid in kfold.split(trainX, trainy):\n",
        "\n",
        "    model = CNN_Model(batch_size=32, depth=3, hidden_neurons_1=64, hidden_neurons_2=128, hidden_neurons_3=256, kernel_size_1=3, kernel_size_2=3, kernel_size_3=3, \n",
        "               dropout_rate=0.2, batch_norm=False, pooling='max', layer_activation_1='relu', layer_activation_2='relu', layer_activation_3='relu',\n",
        "               kernel_initialiser='glorot_uniform', bias_initialiser='zeros', optimiser='Adam', weight_decay=None, learning_rate=1e-3, output_activation='softmax', SGD_momentum=0) \n",
        "    train_gen = tf.keras.preprocessing.image.ImageDataGenerator(width_shift_range=0.1,\n",
        "      height_shift_range=0.1,\n",
        "      horizontal_flip=True)\n",
        "    train_gen.fit(trainX[train])\n",
        "  \n",
        "    print('------------------------------------------------------------------------')\n",
        "    print(f'Training for fold {fold_no} ...')\n",
        "\n",
        "    history = model.train(\n",
        "              train_gen,\n",
        "                trainX[train],\n",
        "                  trainy[train],\n",
        "                trainX[valid],\n",
        "                  trainy[valid],\n",
        "                  lr_rate_type='step')\n",
        "\n",
        "    scores = model.evaluate(trainX[valid], trainy[valid])\n",
        "\n",
        "    print(f'Score for fold {fold_no}: loss of {scores[0]}; accuracy of {scores[1]*100}%')\n",
        "    acc_per_fold.append(scores[1] * 100)\n",
        "    loss_per_fold.append(scores[0])\n",
        "\n",
        "    fold_no+=1\n",
        "\n",
        "# == Provide average scores ==\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Score per fold')\n",
        "for i in range(0, len(acc_per_fold)):\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Average scores for all folds:')\n",
        "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
        "print(f'> Loss: {np.mean(loss_per_fold)}')"
      ],
      "metadata": {
        "id": "YZVQRNRrY4xf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50db0aff-ce99-4b39-c898-19bebc951ff8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_55\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_150 (Conv2D)         (None, 32, 32, 64)        1792      \n",
            "                                                                 \n",
            " max_pooling2d_55 (MaxPoolin  multiple                 0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_151 (Conv2D)         (None, 16, 16, 128)       73856     \n",
            "                                                                 \n",
            " dropout_55 (Dropout)        (None, 8, 8, 128)         0         \n",
            "                                                                 \n",
            " conv2d_152 (Conv2D)         (None, 8, 8, 256)         295168    \n",
            "                                                                 \n",
            " flatten_55 (Flatten)        (None, 4096)              0         \n",
            "                                                                 \n",
            " dense_55 (Dense)            (None, 10)                40970     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 411,786\n",
            "Trainable params: 411,786\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/30\n",
            "1250/1250 [==============================] - 20s 16ms/step - loss: 1.5485 - accuracy: 0.4385 - val_loss: 1.1735 - val_accuracy: 0.5836 - lr: 0.0010\n",
            "Epoch 2/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 1.1598 - accuracy: 0.5906 - val_loss: 0.9440 - val_accuracy: 0.6750 - lr: 0.0010\n",
            "Epoch 3/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 1.0025 - accuracy: 0.6493 - val_loss: 0.9080 - val_accuracy: 0.6890 - lr: 0.0010\n",
            "Epoch 4/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.9054 - accuracy: 0.6847 - val_loss: 0.8721 - val_accuracy: 0.7016 - lr: 0.0010\n",
            "Epoch 5/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.8559 - accuracy: 0.7014 - val_loss: 0.7672 - val_accuracy: 0.7348 - lr: 0.0010\n",
            "Epoch 6/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.8149 - accuracy: 0.7183 - val_loss: 0.8416 - val_accuracy: 0.7206 - lr: 0.0010\n",
            "Epoch 7/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 0.7749 - accuracy: 0.7301 - val_loss: 0.7076 - val_accuracy: 0.7607 - lr: 0.0010\n",
            "Epoch 8/30\n",
            "1250/1250 [==============================] - 19s 16ms/step - loss: 0.7515 - accuracy: 0.7412 - val_loss: 0.7073 - val_accuracy: 0.7543 - lr: 0.0010\n",
            "Epoch 9/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.7290 - accuracy: 0.7461 - val_loss: 0.7059 - val_accuracy: 0.7621 - lr: 0.0010\n",
            "Epoch 10/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.7093 - accuracy: 0.7545 - val_loss: 0.6496 - val_accuracy: 0.7761 - lr: 0.0010\n",
            "Epoch 11/30\n",
            "1250/1250 [==============================] - 22s 18ms/step - loss: 0.6281 - accuracy: 0.7816 - val_loss: 0.6196 - val_accuracy: 0.7900 - lr: 5.0000e-04\n",
            "Epoch 12/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.5995 - accuracy: 0.7913 - val_loss: 0.5930 - val_accuracy: 0.7999 - lr: 5.0000e-04\n",
            "Epoch 13/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.5845 - accuracy: 0.7964 - val_loss: 0.6104 - val_accuracy: 0.7931 - lr: 5.0000e-04\n",
            "Epoch 14/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.5766 - accuracy: 0.8001 - val_loss: 0.5773 - val_accuracy: 0.8035 - lr: 5.0000e-04\n",
            "Epoch 15/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 0.5665 - accuracy: 0.8034 - val_loss: 0.5885 - val_accuracy: 0.8017 - lr: 5.0000e-04\n",
            "Epoch 16/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.5557 - accuracy: 0.8054 - val_loss: 0.5885 - val_accuracy: 0.8023 - lr: 5.0000e-04\n",
            "Epoch 17/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.5467 - accuracy: 0.8079 - val_loss: 0.5986 - val_accuracy: 0.7975 - lr: 5.0000e-04\n",
            "Epoch 18/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 0.5310 - accuracy: 0.8155 - val_loss: 0.5900 - val_accuracy: 0.8038 - lr: 5.0000e-04\n",
            "Epoch 19/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.5270 - accuracy: 0.8137 - val_loss: 0.5839 - val_accuracy: 0.8020 - lr: 5.0000e-04\n",
            "Epoch 20/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.5225 - accuracy: 0.8159 - val_loss: 0.5964 - val_accuracy: 0.8013 - lr: 5.0000e-04\n",
            "Epoch 21/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.4690 - accuracy: 0.8367 - val_loss: 0.5516 - val_accuracy: 0.8124 - lr: 1.2500e-04\n",
            "Epoch 22/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.4603 - accuracy: 0.8413 - val_loss: 0.5417 - val_accuracy: 0.8165 - lr: 1.2500e-04\n",
            "Epoch 23/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.4529 - accuracy: 0.8431 - val_loss: 0.5482 - val_accuracy: 0.8161 - lr: 1.2500e-04\n",
            "Epoch 24/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.4436 - accuracy: 0.8446 - val_loss: 0.5663 - val_accuracy: 0.8098 - lr: 1.2500e-04\n",
            "Epoch 25/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.4441 - accuracy: 0.8457 - val_loss: 0.5529 - val_accuracy: 0.8138 - lr: 1.2500e-04\n",
            "Epoch 26/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.4364 - accuracy: 0.8480 - val_loss: 0.5537 - val_accuracy: 0.8183 - lr: 1.2500e-04\n",
            "Epoch 27/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.4379 - accuracy: 0.8462 - val_loss: 0.5417 - val_accuracy: 0.8187 - lr: 1.2500e-04\n",
            "Epoch 28/30\n",
            "1250/1250 [==============================] - 20s 16ms/step - loss: 0.4336 - accuracy: 0.8487 - val_loss: 0.5434 - val_accuracy: 0.8175 - lr: 1.2500e-04\n",
            "Epoch 29/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.4266 - accuracy: 0.8504 - val_loss: 0.5404 - val_accuracy: 0.8200 - lr: 1.2500e-04\n",
            "Epoch 30/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.4262 - accuracy: 0.8502 - val_loss: 0.5496 - val_accuracy: 0.8173 - lr: 1.2500e-04\n",
            "Score for fold 1: loss of 0.5495588779449463; accuracy of 81.7300021648407%\n",
            "Model: \"sequential_56\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_153 (Conv2D)         (None, 32, 32, 64)        1792      \n",
            "                                                                 \n",
            " max_pooling2d_56 (MaxPoolin  multiple                 0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_154 (Conv2D)         (None, 16, 16, 128)       73856     \n",
            "                                                                 \n",
            " dropout_56 (Dropout)        (None, 8, 8, 128)         0         \n",
            "                                                                 \n",
            " conv2d_155 (Conv2D)         (None, 8, 8, 256)         295168    \n",
            "                                                                 \n",
            " flatten_56 (Flatten)        (None, 4096)              0         \n",
            "                                                                 \n",
            " dense_56 (Dense)            (None, 10)                40970     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 411,786\n",
            "Trainable params: 411,786\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/30\n",
            "1250/1250 [==============================] - 19s 14ms/step - loss: 1.5337 - accuracy: 0.4461 - val_loss: 1.1594 - val_accuracy: 0.5899 - lr: 0.0010\n",
            "Epoch 2/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 1.1418 - accuracy: 0.5990 - val_loss: 0.9704 - val_accuracy: 0.6619 - lr: 0.0010\n",
            "Epoch 3/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.9962 - accuracy: 0.6515 - val_loss: 0.8529 - val_accuracy: 0.7048 - lr: 0.0010\n",
            "Epoch 4/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 0.9107 - accuracy: 0.6822 - val_loss: 0.8685 - val_accuracy: 0.6992 - lr: 0.0010\n",
            "Epoch 5/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.8566 - accuracy: 0.7035 - val_loss: 0.7860 - val_accuracy: 0.7267 - lr: 0.0010\n",
            "Epoch 6/30\n",
            "1250/1250 [==============================] - 19s 16ms/step - loss: 0.8064 - accuracy: 0.7182 - val_loss: 0.7448 - val_accuracy: 0.7465 - lr: 0.0010\n",
            "Epoch 7/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.7771 - accuracy: 0.7300 - val_loss: 0.7152 - val_accuracy: 0.7582 - lr: 0.0010\n",
            "Epoch 8/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 0.7423 - accuracy: 0.7422 - val_loss: 0.7215 - val_accuracy: 0.7564 - lr: 0.0010\n",
            "Epoch 9/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.7179 - accuracy: 0.7490 - val_loss: 0.7327 - val_accuracy: 0.7522 - lr: 0.0010\n",
            "Epoch 10/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.7022 - accuracy: 0.7555 - val_loss: 0.7045 - val_accuracy: 0.7618 - lr: 0.0010\n",
            "Epoch 11/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.6157 - accuracy: 0.7846 - val_loss: 0.6155 - val_accuracy: 0.7897 - lr: 5.0000e-04\n",
            "Epoch 12/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.5931 - accuracy: 0.7951 - val_loss: 0.6314 - val_accuracy: 0.7868 - lr: 5.0000e-04\n",
            "Epoch 13/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.5782 - accuracy: 0.7993 - val_loss: 0.6413 - val_accuracy: 0.7836 - lr: 5.0000e-04\n",
            "Epoch 14/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.5693 - accuracy: 0.8023 - val_loss: 0.6556 - val_accuracy: 0.7842 - lr: 5.0000e-04\n",
            "Epoch 15/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.5548 - accuracy: 0.8048 - val_loss: 0.5729 - val_accuracy: 0.8103 - lr: 5.0000e-04\n",
            "Epoch 16/30\n",
            "1250/1250 [==============================] - 20s 16ms/step - loss: 0.5479 - accuracy: 0.8095 - val_loss: 0.6149 - val_accuracy: 0.7958 - lr: 5.0000e-04\n",
            "Epoch 17/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.5325 - accuracy: 0.8140 - val_loss: 0.6157 - val_accuracy: 0.7941 - lr: 5.0000e-04\n",
            "Epoch 18/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.5238 - accuracy: 0.8164 - val_loss: 0.6021 - val_accuracy: 0.8046 - lr: 5.0000e-04\n",
            "Epoch 19/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.5194 - accuracy: 0.8182 - val_loss: 0.5783 - val_accuracy: 0.8069 - lr: 5.0000e-04\n",
            "Epoch 20/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.5133 - accuracy: 0.8224 - val_loss: 0.6435 - val_accuracy: 0.7910 - lr: 5.0000e-04\n",
            "Epoch 21/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 0.4569 - accuracy: 0.8403 - val_loss: 0.5648 - val_accuracy: 0.8130 - lr: 1.2500e-04\n",
            "Epoch 22/30\n",
            "1250/1250 [==============================] - 20s 16ms/step - loss: 0.4410 - accuracy: 0.8468 - val_loss: 0.5475 - val_accuracy: 0.8210 - lr: 1.2500e-04\n",
            "Epoch 23/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.4315 - accuracy: 0.8472 - val_loss: 0.5593 - val_accuracy: 0.8156 - lr: 1.2500e-04\n",
            "Epoch 24/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.4304 - accuracy: 0.8503 - val_loss: 0.5455 - val_accuracy: 0.8192 - lr: 1.2500e-04\n",
            "Epoch 25/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.4217 - accuracy: 0.8528 - val_loss: 0.5586 - val_accuracy: 0.8151 - lr: 1.2500e-04\n",
            "Epoch 26/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 0.4237 - accuracy: 0.8519 - val_loss: 0.5422 - val_accuracy: 0.8224 - lr: 1.2500e-04\n",
            "Epoch 27/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.4209 - accuracy: 0.8516 - val_loss: 0.5362 - val_accuracy: 0.8225 - lr: 1.2500e-04\n",
            "Epoch 28/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.4198 - accuracy: 0.8525 - val_loss: 0.5468 - val_accuracy: 0.8201 - lr: 1.2500e-04\n",
            "Epoch 29/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.4111 - accuracy: 0.8560 - val_loss: 0.5387 - val_accuracy: 0.8194 - lr: 1.2500e-04\n",
            "Epoch 30/30\n",
            "1250/1250 [==============================] - 20s 16ms/step - loss: 0.4105 - accuracy: 0.8561 - val_loss: 0.5391 - val_accuracy: 0.8221 - lr: 1.2500e-04\n",
            "Score for fold 2: loss of 0.5390692949295044; accuracy of 82.20999836921692%\n",
            "Model: \"sequential_57\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_156 (Conv2D)         (None, 32, 32, 64)        1792      \n",
            "                                                                 \n",
            " max_pooling2d_57 (MaxPoolin  multiple                 0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_157 (Conv2D)         (None, 16, 16, 128)       73856     \n",
            "                                                                 \n",
            " dropout_57 (Dropout)        (None, 8, 8, 128)         0         \n",
            "                                                                 \n",
            " conv2d_158 (Conv2D)         (None, 8, 8, 256)         295168    \n",
            "                                                                 \n",
            " flatten_57 (Flatten)        (None, 4096)              0         \n",
            "                                                                 \n",
            " dense_57 (Dense)            (None, 10)                40970     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 411,786\n",
            "Trainable params: 411,786\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/30\n",
            "1250/1250 [==============================] - 20s 16ms/step - loss: 1.5543 - accuracy: 0.4371 - val_loss: 1.2201 - val_accuracy: 0.5677 - lr: 0.0010\n",
            "Epoch 2/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 1.1924 - accuracy: 0.5760 - val_loss: 1.0409 - val_accuracy: 0.6444 - lr: 0.0010\n",
            "Epoch 3/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 1.0352 - accuracy: 0.6385 - val_loss: 0.9249 - val_accuracy: 0.6792 - lr: 0.0010\n",
            "Epoch 4/30\n",
            "1250/1250 [==============================] - 20s 16ms/step - loss: 0.9385 - accuracy: 0.6718 - val_loss: 0.8896 - val_accuracy: 0.6997 - lr: 0.0010\n",
            "Epoch 5/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 0.8800 - accuracy: 0.6942 - val_loss: 0.8557 - val_accuracy: 0.7023 - lr: 0.0010\n",
            "Epoch 6/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 0.8393 - accuracy: 0.7095 - val_loss: 0.8154 - val_accuracy: 0.7197 - lr: 0.0010\n",
            "Epoch 7/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 0.7922 - accuracy: 0.7245 - val_loss: 0.7870 - val_accuracy: 0.7282 - lr: 0.0010\n",
            "Epoch 8/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.7719 - accuracy: 0.7307 - val_loss: 0.7765 - val_accuracy: 0.7318 - lr: 0.0010\n",
            "Epoch 9/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.7370 - accuracy: 0.7440 - val_loss: 0.7156 - val_accuracy: 0.7547 - lr: 0.0010\n",
            "Epoch 10/30\n",
            "1250/1250 [==============================] - 22s 17ms/step - loss: 0.7245 - accuracy: 0.7498 - val_loss: 0.7216 - val_accuracy: 0.7480 - lr: 0.0010\n",
            "Epoch 11/30\n",
            "1250/1250 [==============================] - 20s 16ms/step - loss: 0.6380 - accuracy: 0.7773 - val_loss: 0.6613 - val_accuracy: 0.7720 - lr: 5.0000e-04\n",
            "Epoch 12/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 0.6182 - accuracy: 0.7848 - val_loss: 0.6580 - val_accuracy: 0.7729 - lr: 5.0000e-04\n",
            "Epoch 13/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.6054 - accuracy: 0.7905 - val_loss: 0.6278 - val_accuracy: 0.7864 - lr: 5.0000e-04\n",
            "Epoch 14/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.6006 - accuracy: 0.7904 - val_loss: 0.6782 - val_accuracy: 0.7681 - lr: 5.0000e-04\n",
            "Epoch 15/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.5793 - accuracy: 0.7983 - val_loss: 0.6233 - val_accuracy: 0.7884 - lr: 5.0000e-04\n",
            "Epoch 16/30\n",
            "1250/1250 [==============================] - 20s 16ms/step - loss: 0.5770 - accuracy: 0.7980 - val_loss: 0.6138 - val_accuracy: 0.7865 - lr: 5.0000e-04\n",
            "Epoch 17/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.5555 - accuracy: 0.8045 - val_loss: 0.6245 - val_accuracy: 0.7881 - lr: 5.0000e-04\n",
            "Epoch 18/30\n",
            "1250/1250 [==============================] - 19s 16ms/step - loss: 0.5524 - accuracy: 0.8075 - val_loss: 0.6282 - val_accuracy: 0.7882 - lr: 5.0000e-04\n",
            "Epoch 19/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 0.5466 - accuracy: 0.8102 - val_loss: 0.6307 - val_accuracy: 0.7830 - lr: 5.0000e-04\n",
            "Epoch 20/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.5388 - accuracy: 0.8113 - val_loss: 0.5988 - val_accuracy: 0.8002 - lr: 5.0000e-04\n",
            "Epoch 21/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 0.4864 - accuracy: 0.8301 - val_loss: 0.5908 - val_accuracy: 0.8017 - lr: 1.2500e-04\n",
            "Epoch 22/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.4725 - accuracy: 0.8356 - val_loss: 0.5762 - val_accuracy: 0.8040 - lr: 1.2500e-04\n",
            "Epoch 23/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.4718 - accuracy: 0.8365 - val_loss: 0.5788 - val_accuracy: 0.8045 - lr: 1.2500e-04\n",
            "Epoch 24/30\n",
            "1250/1250 [==============================] - 21s 17ms/step - loss: 0.4629 - accuracy: 0.8384 - val_loss: 0.5757 - val_accuracy: 0.8055 - lr: 1.2500e-04\n",
            "Epoch 25/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.4587 - accuracy: 0.8400 - val_loss: 0.5740 - val_accuracy: 0.8051 - lr: 1.2500e-04\n",
            "Epoch 26/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 0.4485 - accuracy: 0.8454 - val_loss: 0.5769 - val_accuracy: 0.8058 - lr: 1.2500e-04\n",
            "Epoch 27/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.4513 - accuracy: 0.8419 - val_loss: 0.5909 - val_accuracy: 0.8030 - lr: 1.2500e-04\n",
            "Epoch 28/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.4485 - accuracy: 0.8432 - val_loss: 0.5703 - val_accuracy: 0.8066 - lr: 1.2500e-04\n",
            "Epoch 29/30\n",
            "1250/1250 [==============================] - 19s 16ms/step - loss: 0.4462 - accuracy: 0.8434 - val_loss: 0.5640 - val_accuracy: 0.8109 - lr: 1.2500e-04\n",
            "Epoch 30/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.4416 - accuracy: 0.8466 - val_loss: 0.5636 - val_accuracy: 0.8121 - lr: 1.2500e-04\n",
            "Score for fold 3: loss of 0.5636122822761536; accuracy of 81.20999932289124%\n",
            "Model: \"sequential_58\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_159 (Conv2D)         (None, 32, 32, 64)        1792      \n",
            "                                                                 \n",
            " max_pooling2d_58 (MaxPoolin  multiple                 0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_160 (Conv2D)         (None, 16, 16, 128)       73856     \n",
            "                                                                 \n",
            " dropout_58 (Dropout)        (None, 8, 8, 128)         0         \n",
            "                                                                 \n",
            " conv2d_161 (Conv2D)         (None, 8, 8, 256)         295168    \n",
            "                                                                 \n",
            " flatten_58 (Flatten)        (None, 4096)              0         \n",
            "                                                                 \n",
            " dense_58 (Dense)            (None, 10)                40970     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 411,786\n",
            "Trainable params: 411,786\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/30\n",
            "1250/1250 [==============================] - 21s 16ms/step - loss: 1.5416 - accuracy: 0.4436 - val_loss: 1.2396 - val_accuracy: 0.5642 - lr: 0.0010\n",
            "Epoch 2/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 1.1485 - accuracy: 0.5944 - val_loss: 0.9909 - val_accuracy: 0.6530 - lr: 0.0010\n",
            "Epoch 3/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 1.0124 - accuracy: 0.6489 - val_loss: 0.8811 - val_accuracy: 0.6981 - lr: 0.0010\n",
            "Epoch 4/30\n",
            "1250/1250 [==============================] - 20s 16ms/step - loss: 0.9169 - accuracy: 0.6794 - val_loss: 0.8422 - val_accuracy: 0.7069 - lr: 0.0010\n",
            "Epoch 5/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 0.8691 - accuracy: 0.6972 - val_loss: 0.7972 - val_accuracy: 0.7268 - lr: 0.0010\n",
            "Epoch 6/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.8178 - accuracy: 0.7152 - val_loss: 0.7474 - val_accuracy: 0.7411 - lr: 0.0010\n",
            "Epoch 7/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.7890 - accuracy: 0.7267 - val_loss: 0.7742 - val_accuracy: 0.7306 - lr: 0.0010\n",
            "Epoch 8/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.7608 - accuracy: 0.7365 - val_loss: 0.7255 - val_accuracy: 0.7546 - lr: 0.0010\n",
            "Epoch 9/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 0.7353 - accuracy: 0.7454 - val_loss: 0.7130 - val_accuracy: 0.7578 - lr: 0.0010\n",
            "Epoch 10/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.7137 - accuracy: 0.7515 - val_loss: 0.6869 - val_accuracy: 0.7616 - lr: 0.0010\n",
            "Epoch 11/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.6317 - accuracy: 0.7798 - val_loss: 0.6430 - val_accuracy: 0.7768 - lr: 5.0000e-04\n",
            "Epoch 12/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.6067 - accuracy: 0.7903 - val_loss: 0.6495 - val_accuracy: 0.7803 - lr: 5.0000e-04\n",
            "Epoch 13/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.5977 - accuracy: 0.7910 - val_loss: 0.6423 - val_accuracy: 0.7816 - lr: 5.0000e-04\n",
            "Epoch 14/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.5802 - accuracy: 0.7981 - val_loss: 0.6254 - val_accuracy: 0.7896 - lr: 5.0000e-04\n",
            "Epoch 15/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.5703 - accuracy: 0.8012 - val_loss: 0.6205 - val_accuracy: 0.7893 - lr: 5.0000e-04\n",
            "Epoch 16/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.5652 - accuracy: 0.8037 - val_loss: 0.6090 - val_accuracy: 0.7907 - lr: 5.0000e-04\n",
            "Epoch 17/30\n",
            "1250/1250 [==============================] - 19s 16ms/step - loss: 0.5538 - accuracy: 0.8067 - val_loss: 0.5990 - val_accuracy: 0.7985 - lr: 5.0000e-04\n",
            "Epoch 18/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.5418 - accuracy: 0.8124 - val_loss: 0.6091 - val_accuracy: 0.7954 - lr: 5.0000e-04\n",
            "Epoch 19/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 0.5365 - accuracy: 0.8131 - val_loss: 0.5795 - val_accuracy: 0.8040 - lr: 5.0000e-04\n",
            "Epoch 20/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.5272 - accuracy: 0.8163 - val_loss: 0.6104 - val_accuracy: 0.7955 - lr: 5.0000e-04\n",
            "Epoch 21/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.4720 - accuracy: 0.8356 - val_loss: 0.5614 - val_accuracy: 0.8083 - lr: 1.2500e-04\n",
            "Epoch 22/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.4620 - accuracy: 0.8377 - val_loss: 0.5526 - val_accuracy: 0.8125 - lr: 1.2500e-04\n",
            "Epoch 23/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.4526 - accuracy: 0.8424 - val_loss: 0.5646 - val_accuracy: 0.8122 - lr: 1.2500e-04\n",
            "Epoch 24/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.4483 - accuracy: 0.8439 - val_loss: 0.5577 - val_accuracy: 0.8135 - lr: 1.2500e-04\n",
            "Epoch 25/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.4439 - accuracy: 0.8444 - val_loss: 0.5489 - val_accuracy: 0.8190 - lr: 1.2500e-04\n",
            "Epoch 26/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.4391 - accuracy: 0.8451 - val_loss: 0.5496 - val_accuracy: 0.8181 - lr: 1.2500e-04\n",
            "Epoch 27/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 0.4373 - accuracy: 0.8465 - val_loss: 0.5530 - val_accuracy: 0.8162 - lr: 1.2500e-04\n",
            "Epoch 28/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.4325 - accuracy: 0.8487 - val_loss: 0.5379 - val_accuracy: 0.8207 - lr: 1.2500e-04\n",
            "Epoch 29/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.4335 - accuracy: 0.8502 - val_loss: 0.5518 - val_accuracy: 0.8162 - lr: 1.2500e-04\n",
            "Epoch 30/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.4300 - accuracy: 0.8510 - val_loss: 0.5428 - val_accuracy: 0.8172 - lr: 1.2500e-04\n",
            "Score for fold 4: loss of 0.542830228805542; accuracy of 81.72000050544739%\n",
            "Model: \"sequential_59\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_162 (Conv2D)         (None, 32, 32, 64)        1792      \n",
            "                                                                 \n",
            " max_pooling2d_59 (MaxPoolin  multiple                 0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_163 (Conv2D)         (None, 16, 16, 128)       73856     \n",
            "                                                                 \n",
            " dropout_59 (Dropout)        (None, 8, 8, 128)         0         \n",
            "                                                                 \n",
            " conv2d_164 (Conv2D)         (None, 8, 8, 256)         295168    \n",
            "                                                                 \n",
            " flatten_59 (Flatten)        (None, 4096)              0         \n",
            "                                                                 \n",
            " dense_59 (Dense)            (None, 10)                40970     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 411,786\n",
            "Trainable params: 411,786\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 1.5455 - accuracy: 0.4408 - val_loss: 1.1676 - val_accuracy: 0.5848 - lr: 0.0010\n",
            "Epoch 2/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.1569 - accuracy: 0.5926 - val_loss: 0.9419 - val_accuracy: 0.6659 - lr: 0.0010\n",
            "Epoch 3/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 1.0113 - accuracy: 0.6445 - val_loss: 0.8618 - val_accuracy: 0.7042 - lr: 0.0010\n",
            "Epoch 4/30\n",
            "1250/1250 [==============================] - 20s 16ms/step - loss: 0.9207 - accuracy: 0.6794 - val_loss: 0.7984 - val_accuracy: 0.7219 - lr: 0.0010\n",
            "Epoch 5/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.8576 - accuracy: 0.7005 - val_loss: 0.8357 - val_accuracy: 0.7143 - lr: 0.0010\n",
            "Epoch 6/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.8191 - accuracy: 0.7152 - val_loss: 0.7394 - val_accuracy: 0.7415 - lr: 0.0010\n",
            "Epoch 7/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.7905 - accuracy: 0.7247 - val_loss: 0.7345 - val_accuracy: 0.7452 - lr: 0.0010\n",
            "Epoch 8/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.7602 - accuracy: 0.7355 - val_loss: 0.6993 - val_accuracy: 0.7569 - lr: 0.0010\n",
            "Epoch 9/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 0.7374 - accuracy: 0.7423 - val_loss: 0.6695 - val_accuracy: 0.7680 - lr: 0.0010\n",
            "Epoch 10/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.7108 - accuracy: 0.7537 - val_loss: 0.7451 - val_accuracy: 0.7478 - lr: 0.0010\n",
            "Epoch 11/30\n",
            "1250/1250 [==============================] - 20s 16ms/step - loss: 0.6317 - accuracy: 0.7804 - val_loss: 0.6268 - val_accuracy: 0.7827 - lr: 5.0000e-04\n",
            "Epoch 12/30\n",
            "1250/1250 [==============================] - 20s 16ms/step - loss: 0.6065 - accuracy: 0.7904 - val_loss: 0.6307 - val_accuracy: 0.7825 - lr: 5.0000e-04\n",
            "Epoch 13/30\n",
            "1250/1250 [==============================] - 21s 16ms/step - loss: 0.6010 - accuracy: 0.7904 - val_loss: 0.6165 - val_accuracy: 0.7875 - lr: 5.0000e-04\n",
            "Epoch 14/30\n",
            "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5892 - accuracy: 0.7966 - val_loss: 0.5868 - val_accuracy: 0.7974 - lr: 5.0000e-04\n",
            "Epoch 15/30\n",
            "1250/1250 [==============================] - 23s 18ms/step - loss: 0.5703 - accuracy: 0.8019 - val_loss: 0.5997 - val_accuracy: 0.7943 - lr: 5.0000e-04\n",
            "Epoch 16/30\n",
            "1250/1250 [==============================] - 22s 18ms/step - loss: 0.5593 - accuracy: 0.8057 - val_loss: 0.5870 - val_accuracy: 0.7982 - lr: 5.0000e-04\n",
            "Epoch 17/30\n",
            "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5502 - accuracy: 0.8087 - val_loss: 0.5886 - val_accuracy: 0.7973 - lr: 5.0000e-04\n",
            "Epoch 18/30\n",
            "1250/1250 [==============================] - 20s 16ms/step - loss: 0.5482 - accuracy: 0.8084 - val_loss: 0.5909 - val_accuracy: 0.8009 - lr: 5.0000e-04\n",
            "Epoch 19/30\n",
            "1250/1250 [==============================] - 22s 18ms/step - loss: 0.5373 - accuracy: 0.8123 - val_loss: 0.5980 - val_accuracy: 0.7992 - lr: 5.0000e-04\n",
            "Epoch 20/30\n",
            "1250/1250 [==============================] - 19s 16ms/step - loss: 0.5281 - accuracy: 0.8166 - val_loss: 0.6144 - val_accuracy: 0.7941 - lr: 5.0000e-04\n",
            "Epoch 21/30\n",
            "1250/1250 [==============================] - 21s 16ms/step - loss: 0.4772 - accuracy: 0.8340 - val_loss: 0.5635 - val_accuracy: 0.8115 - lr: 1.2500e-04\n",
            "Epoch 22/30\n",
            "1250/1250 [==============================] - 21s 17ms/step - loss: 0.4611 - accuracy: 0.8389 - val_loss: 0.5665 - val_accuracy: 0.8134 - lr: 1.2500e-04\n",
            "Epoch 23/30\n",
            "1250/1250 [==============================] - 21s 17ms/step - loss: 0.4557 - accuracy: 0.8434 - val_loss: 0.5649 - val_accuracy: 0.8106 - lr: 1.2500e-04\n",
            "Epoch 24/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.4547 - accuracy: 0.8410 - val_loss: 0.5625 - val_accuracy: 0.8123 - lr: 1.2500e-04\n",
            "Epoch 25/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.4500 - accuracy: 0.8420 - val_loss: 0.5520 - val_accuracy: 0.8158 - lr: 1.2500e-04\n",
            "Epoch 26/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.4422 - accuracy: 0.8462 - val_loss: 0.5623 - val_accuracy: 0.8140 - lr: 1.2500e-04\n",
            "Epoch 27/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.4434 - accuracy: 0.8455 - val_loss: 0.5525 - val_accuracy: 0.8161 - lr: 1.2500e-04\n",
            "Epoch 28/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 0.4386 - accuracy: 0.8466 - val_loss: 0.5499 - val_accuracy: 0.8166 - lr: 1.2500e-04\n",
            "Epoch 29/30\n",
            "1250/1250 [==============================] - 20s 16ms/step - loss: 0.4367 - accuracy: 0.8478 - val_loss: 0.5457 - val_accuracy: 0.8164 - lr: 1.2500e-04\n",
            "Epoch 30/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.4353 - accuracy: 0.8491 - val_loss: 0.5454 - val_accuracy: 0.8192 - lr: 1.2500e-04\n",
            "Score for fold 5: loss of 0.5453624129295349; accuracy of 81.91999793052673%\n",
            "------------------------------------------------------------------------\n",
            "Score per fold\n",
            "------------------------------------------------------------------------\n",
            "> Fold 1 - Loss: 0.5495588779449463 - Accuracy: 81.7300021648407%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 2 - Loss: 0.5390692949295044 - Accuracy: 82.20999836921692%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 3 - Loss: 0.5636122822761536 - Accuracy: 81.20999932289124%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 4 - Loss: 0.542830228805542 - Accuracy: 81.72000050544739%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 5 - Loss: 0.5453624129295349 - Accuracy: 81.91999793052673%\n",
            "------------------------------------------------------------------------\n",
            "Average scores for all folds:\n",
            "> Accuracy: 81.7579996585846 (+- 0.3265205775263304)\n",
            "> Loss: 0.5480866193771362\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3 hidden layers and Middle Structure"
      ],
      "metadata": {
        "id": "jQig4pO66Xby"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "acc_per_fold = []\n",
        "loss_per_fold = []\n",
        "\n",
        "with tf.device('/device:GPU:0'):\n",
        "  fold_no = 1\n",
        "  for train, valid in kfold.split(trainX, trainy):\n",
        "\n",
        "    model = CNN_Model(batch_size=32, depth=3, hidden_neurons_1=32, hidden_neurons_2=64, hidden_neurons_3=128, kernel_size_1=3, kernel_size_2=3, kernel_size_3=3, \n",
        "               dropout_rate=0.2, batch_norm=False, pooling='max', layer_activation_1='relu', layer_activation_2='relu', layer_activation_3='relu',\n",
        "               kernel_initialiser='glorot_uniform', bias_initialiser='zeros', optimiser='Adam', weight_decay=None, learning_rate=1e-3, output_activation='softmax', SGD_momentum=0) \n",
        "  \n",
        "    train_gen = tf.keras.preprocessing.image.ImageDataGenerator(width_shift_range=0.1,\n",
        "      height_shift_range=0.1,\n",
        "      horizontal_flip=True)\n",
        "    train_gen.fit(trainX[train])\n",
        "  \n",
        "    print('------------------------------------------------------------------------')\n",
        "    print(f'Training for fold {fold_no} ...')\n",
        "\n",
        "    history = model.train(\n",
        "              train_gen,\n",
        "                trainX[train],\n",
        "                  trainy[train],\n",
        "                trainX[valid],\n",
        "                  trainy[valid],\n",
        "                  lr_rate_type='step')\n",
        "\n",
        "    scores = model.evaluate(trainX[valid], trainy[valid])\n",
        "\n",
        "    print(f'Score for fold {fold_no}: loss of {scores[0]}; accuracy of {scores[1]*100}%')\n",
        "    acc_per_fold.append(scores[1] * 100)\n",
        "    loss_per_fold.append(scores[0])\n",
        "\n",
        "    fold_no+=1\n",
        "\n",
        "# == Provide average scores ==\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Score per fold')\n",
        "for i in range(0, len(acc_per_fold)):\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Average scores for all folds:')\n",
        "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
        "print(f'> Loss: {np.mean(loss_per_fold)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8PCbCXWH6ZD2",
        "outputId": "e1e1a946-59d3-4eb8-d17a-099b67b9b9f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_60\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_165 (Conv2D)         (None, 32, 32, 32)        896       \n",
            "                                                                 \n",
            " max_pooling2d_60 (MaxPoolin  multiple                 0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_166 (Conv2D)         (None, 16, 16, 64)        18496     \n",
            "                                                                 \n",
            " dropout_60 (Dropout)        (None, 8, 8, 64)          0         \n",
            "                                                                 \n",
            " conv2d_167 (Conv2D)         (None, 8, 8, 128)         73856     \n",
            "                                                                 \n",
            " flatten_60 (Flatten)        (None, 2048)              0         \n",
            "                                                                 \n",
            " dense_60 (Dense)            (None, 10)                20490     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 113,738\n",
            "Trainable params: 113,738\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/30\n",
            "1250/1250 [==============================] - 20s 15ms/step - loss: 1.5940 - accuracy: 0.4212 - val_loss: 1.2342 - val_accuracy: 0.5611 - lr: 0.0010\n",
            "Epoch 2/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 1.2214 - accuracy: 0.5656 - val_loss: 1.0999 - val_accuracy: 0.6134 - lr: 0.0010\n",
            "Epoch 3/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.0815 - accuracy: 0.6191 - val_loss: 0.9170 - val_accuracy: 0.6824 - lr: 0.0010\n",
            "Epoch 4/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.0028 - accuracy: 0.6493 - val_loss: 0.9039 - val_accuracy: 0.6845 - lr: 0.0010\n",
            "Epoch 5/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.9412 - accuracy: 0.6733 - val_loss: 0.8873 - val_accuracy: 0.6955 - lr: 0.0010\n",
            "Epoch 6/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.8994 - accuracy: 0.6838 - val_loss: 0.7898 - val_accuracy: 0.7276 - lr: 0.0010\n",
            "Epoch 7/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.8656 - accuracy: 0.6992 - val_loss: 0.7782 - val_accuracy: 0.7312 - lr: 0.0010\n",
            "Epoch 8/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 0.8355 - accuracy: 0.7085 - val_loss: 0.7860 - val_accuracy: 0.7294 - lr: 0.0010\n",
            "Epoch 9/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 0.8171 - accuracy: 0.7142 - val_loss: 0.8064 - val_accuracy: 0.7261 - lr: 0.0010\n",
            "Epoch 10/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.7958 - accuracy: 0.7226 - val_loss: 0.7813 - val_accuracy: 0.7329 - lr: 0.0010\n",
            "Epoch 11/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.7245 - accuracy: 0.7463 - val_loss: 0.6803 - val_accuracy: 0.7666 - lr: 5.0000e-04\n",
            "Epoch 12/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 0.7069 - accuracy: 0.7535 - val_loss: 0.6799 - val_accuracy: 0.7692 - lr: 5.0000e-04\n",
            "Epoch 13/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.6913 - accuracy: 0.7592 - val_loss: 0.6830 - val_accuracy: 0.7647 - lr: 5.0000e-04\n",
            "Epoch 14/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 0.6844 - accuracy: 0.7635 - val_loss: 0.6454 - val_accuracy: 0.7817 - lr: 5.0000e-04\n",
            "Epoch 15/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.6758 - accuracy: 0.7665 - val_loss: 0.6688 - val_accuracy: 0.7736 - lr: 5.0000e-04\n",
            "Epoch 16/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 0.6710 - accuracy: 0.7668 - val_loss: 0.6438 - val_accuracy: 0.7793 - lr: 5.0000e-04\n",
            "Epoch 17/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.6565 - accuracy: 0.7724 - val_loss: 0.6415 - val_accuracy: 0.7788 - lr: 5.0000e-04\n",
            "Epoch 18/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 0.6512 - accuracy: 0.7717 - val_loss: 0.6183 - val_accuracy: 0.7916 - lr: 5.0000e-04\n",
            "Epoch 19/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.6406 - accuracy: 0.7764 - val_loss: 0.6306 - val_accuracy: 0.7846 - lr: 5.0000e-04\n",
            "Epoch 20/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.6383 - accuracy: 0.7780 - val_loss: 0.6719 - val_accuracy: 0.7720 - lr: 5.0000e-04\n",
            "Epoch 21/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 0.5885 - accuracy: 0.7951 - val_loss: 0.6061 - val_accuracy: 0.7937 - lr: 1.2500e-04\n",
            "Epoch 22/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 0.5866 - accuracy: 0.7965 - val_loss: 0.5933 - val_accuracy: 0.7978 - lr: 1.2500e-04\n",
            "Epoch 23/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.5813 - accuracy: 0.7993 - val_loss: 0.6066 - val_accuracy: 0.7938 - lr: 1.2500e-04\n",
            "Epoch 24/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.5711 - accuracy: 0.8019 - val_loss: 0.6099 - val_accuracy: 0.7927 - lr: 1.2500e-04\n",
            "Epoch 25/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.5730 - accuracy: 0.7998 - val_loss: 0.6148 - val_accuracy: 0.7888 - lr: 1.2500e-04\n",
            "Epoch 26/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.5645 - accuracy: 0.8041 - val_loss: 0.5995 - val_accuracy: 0.7939 - lr: 1.2500e-04\n",
            "Epoch 27/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 0.5698 - accuracy: 0.8041 - val_loss: 0.5902 - val_accuracy: 0.7993 - lr: 1.2500e-04\n",
            "Epoch 28/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 0.5654 - accuracy: 0.8040 - val_loss: 0.5918 - val_accuracy: 0.7972 - lr: 1.2500e-04\n",
            "Epoch 29/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.5620 - accuracy: 0.8037 - val_loss: 0.6053 - val_accuracy: 0.7942 - lr: 1.2500e-04\n",
            "Epoch 30/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.5595 - accuracy: 0.8045 - val_loss: 0.5960 - val_accuracy: 0.7970 - lr: 1.2500e-04\n",
            "Score for fold 1: loss of 0.5959774255752563; accuracy of 79.6999990940094%\n",
            "Model: \"sequential_61\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_168 (Conv2D)         (None, 32, 32, 32)        896       \n",
            "                                                                 \n",
            " max_pooling2d_61 (MaxPoolin  multiple                 0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_169 (Conv2D)         (None, 16, 16, 64)        18496     \n",
            "                                                                 \n",
            " dropout_61 (Dropout)        (None, 8, 8, 64)          0         \n",
            "                                                                 \n",
            " conv2d_170 (Conv2D)         (None, 8, 8, 128)         73856     \n",
            "                                                                 \n",
            " flatten_61 (Flatten)        (None, 2048)              0         \n",
            "                                                                 \n",
            " dense_61 (Dense)            (None, 10)                20490     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 113,738\n",
            "Trainable params: 113,738\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.5743 - accuracy: 0.4308 - val_loss: 1.2493 - val_accuracy: 0.5586 - lr: 0.0010\n",
            "Epoch 2/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 1.2225 - accuracy: 0.5683 - val_loss: 1.0700 - val_accuracy: 0.6274 - lr: 0.0010\n",
            "Epoch 3/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.0742 - accuracy: 0.6228 - val_loss: 0.9565 - val_accuracy: 0.6740 - lr: 0.0010\n",
            "Epoch 4/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.9942 - accuracy: 0.6510 - val_loss: 0.9149 - val_accuracy: 0.6812 - lr: 0.0010\n",
            "Epoch 5/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.9412 - accuracy: 0.6729 - val_loss: 0.9064 - val_accuracy: 0.6846 - lr: 0.0010\n",
            "Epoch 6/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 0.8967 - accuracy: 0.6870 - val_loss: 0.8230 - val_accuracy: 0.7167 - lr: 0.0010\n",
            "Epoch 7/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.8599 - accuracy: 0.7023 - val_loss: 0.8267 - val_accuracy: 0.7150 - lr: 0.0010\n",
            "Epoch 8/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 0.8348 - accuracy: 0.7108 - val_loss: 0.8148 - val_accuracy: 0.7226 - lr: 0.0010\n",
            "Epoch 9/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.8123 - accuracy: 0.7159 - val_loss: 0.7612 - val_accuracy: 0.7399 - lr: 0.0010\n",
            "Epoch 10/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.7950 - accuracy: 0.7220 - val_loss: 0.7234 - val_accuracy: 0.7529 - lr: 0.0010\n",
            "Epoch 11/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.7232 - accuracy: 0.7495 - val_loss: 0.7186 - val_accuracy: 0.7587 - lr: 5.0000e-04\n",
            "Epoch 12/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.7061 - accuracy: 0.7539 - val_loss: 0.6964 - val_accuracy: 0.7647 - lr: 5.0000e-04\n",
            "Epoch 13/30\n",
            "1250/1250 [==============================] - 20s 16ms/step - loss: 0.6902 - accuracy: 0.7572 - val_loss: 0.7068 - val_accuracy: 0.7591 - lr: 5.0000e-04\n",
            "Epoch 14/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 0.6817 - accuracy: 0.7600 - val_loss: 0.6718 - val_accuracy: 0.7772 - lr: 5.0000e-04\n",
            "Epoch 15/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 0.6741 - accuracy: 0.7650 - val_loss: 0.6889 - val_accuracy: 0.7708 - lr: 5.0000e-04\n",
            "Epoch 16/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 0.6576 - accuracy: 0.7702 - val_loss: 0.6772 - val_accuracy: 0.7738 - lr: 5.0000e-04\n",
            "Epoch 17/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.6549 - accuracy: 0.7714 - val_loss: 0.6779 - val_accuracy: 0.7732 - lr: 5.0000e-04\n",
            "Epoch 18/30\n",
            "1250/1250 [==============================] - 20s 16ms/step - loss: 0.6428 - accuracy: 0.7771 - val_loss: 0.6645 - val_accuracy: 0.7718 - lr: 5.0000e-04\n",
            "Epoch 19/30\n",
            "1250/1250 [==============================] - 20s 16ms/step - loss: 0.6435 - accuracy: 0.7738 - val_loss: 0.6592 - val_accuracy: 0.7788 - lr: 5.0000e-04\n",
            "Epoch 20/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 0.6302 - accuracy: 0.7808 - val_loss: 0.6778 - val_accuracy: 0.7711 - lr: 5.0000e-04\n",
            "Epoch 21/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 0.5852 - accuracy: 0.7969 - val_loss: 0.6515 - val_accuracy: 0.7818 - lr: 1.2500e-04\n",
            "Epoch 22/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.5756 - accuracy: 0.7970 - val_loss: 0.6242 - val_accuracy: 0.7938 - lr: 1.2500e-04\n",
            "Epoch 23/30\n",
            "1250/1250 [==============================] - 22s 18ms/step - loss: 0.5705 - accuracy: 0.8010 - val_loss: 0.6286 - val_accuracy: 0.7886 - lr: 1.2500e-04\n",
            "Epoch 24/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 0.5647 - accuracy: 0.8034 - val_loss: 0.6219 - val_accuracy: 0.7921 - lr: 1.2500e-04\n",
            "Epoch 25/30\n",
            "1250/1250 [==============================] - 22s 18ms/step - loss: 0.5647 - accuracy: 0.8047 - val_loss: 0.6401 - val_accuracy: 0.7888 - lr: 1.2500e-04\n",
            "Epoch 26/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 0.5645 - accuracy: 0.8036 - val_loss: 0.5957 - val_accuracy: 0.8024 - lr: 1.2500e-04\n",
            "Epoch 27/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.5595 - accuracy: 0.8059 - val_loss: 0.6205 - val_accuracy: 0.7923 - lr: 1.2500e-04\n",
            "Epoch 28/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 0.5522 - accuracy: 0.8079 - val_loss: 0.6235 - val_accuracy: 0.7949 - lr: 1.2500e-04\n",
            "Epoch 29/30\n",
            "1250/1250 [==============================] - 20s 16ms/step - loss: 0.5499 - accuracy: 0.8067 - val_loss: 0.6287 - val_accuracy: 0.7947 - lr: 1.2500e-04\n",
            "Epoch 30/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 0.5480 - accuracy: 0.8075 - val_loss: 0.6052 - val_accuracy: 0.7985 - lr: 1.2500e-04\n",
            "Score for fold 2: loss of 0.6051555871963501; accuracy of 79.85000014305115%\n",
            "Model: \"sequential_62\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_171 (Conv2D)         (None, 32, 32, 32)        896       \n",
            "                                                                 \n",
            " max_pooling2d_62 (MaxPoolin  multiple                 0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_172 (Conv2D)         (None, 16, 16, 64)        18496     \n",
            "                                                                 \n",
            " dropout_62 (Dropout)        (None, 8, 8, 64)          0         \n",
            "                                                                 \n",
            " conv2d_173 (Conv2D)         (None, 8, 8, 128)         73856     \n",
            "                                                                 \n",
            " flatten_62 (Flatten)        (None, 2048)              0         \n",
            "                                                                 \n",
            " dense_62 (Dense)            (None, 10)                20490     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 113,738\n",
            "Trainable params: 113,738\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/30\n",
            "1250/1250 [==============================] - 19s 14ms/step - loss: 1.5836 - accuracy: 0.4264 - val_loss: 1.2359 - val_accuracy: 0.5582 - lr: 0.0010\n",
            "Epoch 2/30\n",
            "1250/1250 [==============================] - 20s 16ms/step - loss: 1.2113 - accuracy: 0.5735 - val_loss: 1.1032 - val_accuracy: 0.6202 - lr: 0.0010\n",
            "Epoch 3/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.0819 - accuracy: 0.6227 - val_loss: 0.9821 - val_accuracy: 0.6572 - lr: 0.0010\n",
            "Epoch 4/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.9919 - accuracy: 0.6558 - val_loss: 0.8914 - val_accuracy: 0.6924 - lr: 0.0010\n",
            "Epoch 5/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 0.9340 - accuracy: 0.6749 - val_loss: 0.8355 - val_accuracy: 0.7081 - lr: 0.0010\n",
            "Epoch 6/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.8920 - accuracy: 0.6865 - val_loss: 0.8205 - val_accuracy: 0.7110 - lr: 0.0010\n",
            "Epoch 7/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 0.8591 - accuracy: 0.7009 - val_loss: 0.7991 - val_accuracy: 0.7253 - lr: 0.0010\n",
            "Epoch 8/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.8363 - accuracy: 0.7082 - val_loss: 0.7843 - val_accuracy: 0.7257 - lr: 0.0010\n",
            "Epoch 9/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.8219 - accuracy: 0.7144 - val_loss: 0.7464 - val_accuracy: 0.7424 - lr: 0.0010\n",
            "Epoch 10/30\n",
            "1250/1250 [==============================] - 20s 16ms/step - loss: 0.7973 - accuracy: 0.7232 - val_loss: 0.7377 - val_accuracy: 0.7452 - lr: 0.0010\n",
            "Epoch 11/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.7244 - accuracy: 0.7494 - val_loss: 0.7046 - val_accuracy: 0.7585 - lr: 5.0000e-04\n",
            "Epoch 12/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.7081 - accuracy: 0.7543 - val_loss: 0.6806 - val_accuracy: 0.7655 - lr: 5.0000e-04\n",
            "Epoch 13/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.6968 - accuracy: 0.7588 - val_loss: 0.6789 - val_accuracy: 0.7671 - lr: 5.0000e-04\n",
            "Epoch 14/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 0.6840 - accuracy: 0.7638 - val_loss: 0.6790 - val_accuracy: 0.7660 - lr: 5.0000e-04\n",
            "Epoch 15/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.6759 - accuracy: 0.7651 - val_loss: 0.6601 - val_accuracy: 0.7722 - lr: 5.0000e-04\n",
            "Epoch 16/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 0.6663 - accuracy: 0.7691 - val_loss: 0.6791 - val_accuracy: 0.7703 - lr: 5.0000e-04\n",
            "Epoch 17/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 0.6591 - accuracy: 0.7702 - val_loss: 0.6860 - val_accuracy: 0.7707 - lr: 5.0000e-04\n",
            "Epoch 18/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.6538 - accuracy: 0.7744 - val_loss: 0.6370 - val_accuracy: 0.7827 - lr: 5.0000e-04\n",
            "Epoch 19/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.6441 - accuracy: 0.7778 - val_loss: 0.6450 - val_accuracy: 0.7765 - lr: 5.0000e-04\n",
            "Epoch 20/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.6359 - accuracy: 0.7794 - val_loss: 0.6657 - val_accuracy: 0.7747 - lr: 5.0000e-04\n",
            "Epoch 21/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.5902 - accuracy: 0.7946 - val_loss: 0.6217 - val_accuracy: 0.7886 - lr: 1.2500e-04\n",
            "Epoch 22/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 0.5803 - accuracy: 0.7982 - val_loss: 0.6066 - val_accuracy: 0.7947 - lr: 1.2500e-04\n",
            "Epoch 23/30\n",
            "1250/1250 [==============================] - 20s 16ms/step - loss: 0.5779 - accuracy: 0.7992 - val_loss: 0.5991 - val_accuracy: 0.7970 - lr: 1.2500e-04\n",
            "Epoch 24/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.5764 - accuracy: 0.8002 - val_loss: 0.6010 - val_accuracy: 0.7954 - lr: 1.2500e-04\n",
            "Epoch 25/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.5741 - accuracy: 0.8004 - val_loss: 0.6213 - val_accuracy: 0.7897 - lr: 1.2500e-04\n",
            "Epoch 26/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.5695 - accuracy: 0.8010 - val_loss: 0.6024 - val_accuracy: 0.7960 - lr: 1.2500e-04\n",
            "Epoch 27/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.5629 - accuracy: 0.8043 - val_loss: 0.5890 - val_accuracy: 0.7994 - lr: 1.2500e-04\n",
            "Epoch 28/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.5719 - accuracy: 0.8022 - val_loss: 0.6033 - val_accuracy: 0.7968 - lr: 1.2500e-04\n",
            "Epoch 29/30\n",
            "1250/1250 [==============================] - 20s 16ms/step - loss: 0.5645 - accuracy: 0.8018 - val_loss: 0.5955 - val_accuracy: 0.7989 - lr: 1.2500e-04\n",
            "Epoch 30/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.5637 - accuracy: 0.8042 - val_loss: 0.5947 - val_accuracy: 0.7981 - lr: 1.2500e-04\n",
            "Score for fold 3: loss of 0.5946542024612427; accuracy of 79.80999946594238%\n",
            "Model: \"sequential_63\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_174 (Conv2D)         (None, 32, 32, 32)        896       \n",
            "                                                                 \n",
            " max_pooling2d_63 (MaxPoolin  multiple                 0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_175 (Conv2D)         (None, 16, 16, 64)        18496     \n",
            "                                                                 \n",
            " dropout_63 (Dropout)        (None, 8, 8, 64)          0         \n",
            "                                                                 \n",
            " conv2d_176 (Conv2D)         (None, 8, 8, 128)         73856     \n",
            "                                                                 \n",
            " flatten_63 (Flatten)        (None, 2048)              0         \n",
            "                                                                 \n",
            " dense_63 (Dense)            (None, 10)                20490     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 113,738\n",
            "Trainable params: 113,738\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 1.5830 - accuracy: 0.4302 - val_loss: 1.2426 - val_accuracy: 0.5496 - lr: 0.0010\n",
            "Epoch 2/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 1.2226 - accuracy: 0.5660 - val_loss: 1.1554 - val_accuracy: 0.5973 - lr: 0.0010\n",
            "Epoch 3/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.0846 - accuracy: 0.6201 - val_loss: 0.9444 - val_accuracy: 0.6741 - lr: 0.0010\n",
            "Epoch 4/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 0.9955 - accuracy: 0.6531 - val_loss: 0.9548 - val_accuracy: 0.6727 - lr: 0.0010\n",
            "Epoch 5/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.9421 - accuracy: 0.6716 - val_loss: 0.9268 - val_accuracy: 0.6849 - lr: 0.0010\n",
            "Epoch 6/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.9028 - accuracy: 0.6855 - val_loss: 0.8477 - val_accuracy: 0.7063 - lr: 0.0010\n",
            "Epoch 7/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.8641 - accuracy: 0.7015 - val_loss: 0.9076 - val_accuracy: 0.6912 - lr: 0.0010\n",
            "Epoch 8/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.8340 - accuracy: 0.7132 - val_loss: 0.8342 - val_accuracy: 0.7133 - lr: 0.0010\n",
            "Epoch 9/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.8146 - accuracy: 0.7175 - val_loss: 0.7554 - val_accuracy: 0.7389 - lr: 0.0010\n",
            "Epoch 10/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.7902 - accuracy: 0.7259 - val_loss: 0.7842 - val_accuracy: 0.7306 - lr: 0.0010\n",
            "Epoch 11/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.7221 - accuracy: 0.7493 - val_loss: 0.7469 - val_accuracy: 0.7437 - lr: 5.0000e-04\n",
            "Epoch 12/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.7021 - accuracy: 0.7552 - val_loss: 0.6866 - val_accuracy: 0.7668 - lr: 5.0000e-04\n",
            "Epoch 13/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.6910 - accuracy: 0.7602 - val_loss: 0.6749 - val_accuracy: 0.7711 - lr: 5.0000e-04\n",
            "Epoch 14/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.6859 - accuracy: 0.7615 - val_loss: 0.6680 - val_accuracy: 0.7701 - lr: 5.0000e-04\n",
            "Epoch 15/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.6732 - accuracy: 0.7674 - val_loss: 0.6738 - val_accuracy: 0.7689 - lr: 5.0000e-04\n",
            "Epoch 16/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.6659 - accuracy: 0.7695 - val_loss: 0.6915 - val_accuracy: 0.7652 - lr: 5.0000e-04\n",
            "Epoch 17/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.6582 - accuracy: 0.7714 - val_loss: 0.6696 - val_accuracy: 0.7682 - lr: 5.0000e-04\n",
            "Epoch 18/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 0.6463 - accuracy: 0.7752 - val_loss: 0.6545 - val_accuracy: 0.7766 - lr: 5.0000e-04\n",
            "Epoch 19/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 0.6414 - accuracy: 0.7756 - val_loss: 0.6524 - val_accuracy: 0.7774 - lr: 5.0000e-04\n",
            "Epoch 20/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.6318 - accuracy: 0.7803 - val_loss: 0.6569 - val_accuracy: 0.7779 - lr: 5.0000e-04\n",
            "Epoch 21/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.5897 - accuracy: 0.7961 - val_loss: 0.6145 - val_accuracy: 0.7877 - lr: 1.2500e-04\n",
            "Epoch 22/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.5794 - accuracy: 0.8001 - val_loss: 0.6180 - val_accuracy: 0.7897 - lr: 1.2500e-04\n",
            "Epoch 23/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.5765 - accuracy: 0.7998 - val_loss: 0.6226 - val_accuracy: 0.7891 - lr: 1.2500e-04\n",
            "Epoch 24/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.5726 - accuracy: 0.8022 - val_loss: 0.5988 - val_accuracy: 0.7989 - lr: 1.2500e-04\n",
            "Epoch 25/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.5705 - accuracy: 0.8011 - val_loss: 0.6040 - val_accuracy: 0.7933 - lr: 1.2500e-04\n",
            "Epoch 26/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 0.5634 - accuracy: 0.8036 - val_loss: 0.6415 - val_accuracy: 0.7831 - lr: 1.2500e-04\n",
            "Epoch 27/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.5599 - accuracy: 0.8053 - val_loss: 0.6328 - val_accuracy: 0.7818 - lr: 1.2500e-04\n",
            "Epoch 28/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.5541 - accuracy: 0.8071 - val_loss: 0.5993 - val_accuracy: 0.7960 - lr: 1.2500e-04\n",
            "Epoch 29/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.5592 - accuracy: 0.8063 - val_loss: 0.6130 - val_accuracy: 0.7907 - lr: 1.2500e-04\n",
            "Epoch 30/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.5553 - accuracy: 0.8072 - val_loss: 0.5933 - val_accuracy: 0.7967 - lr: 1.2500e-04\n",
            "Score for fold 4: loss of 0.5933048129081726; accuracy of 79.67000007629395%\n",
            "Model: \"sequential_64\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_177 (Conv2D)         (None, 32, 32, 32)        896       \n",
            "                                                                 \n",
            " max_pooling2d_64 (MaxPoolin  multiple                 0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_178 (Conv2D)         (None, 16, 16, 64)        18496     \n",
            "                                                                 \n",
            " dropout_64 (Dropout)        (None, 8, 8, 64)          0         \n",
            "                                                                 \n",
            " conv2d_179 (Conv2D)         (None, 8, 8, 128)         73856     \n",
            "                                                                 \n",
            " flatten_64 (Flatten)        (None, 2048)              0         \n",
            "                                                                 \n",
            " dense_64 (Dense)            (None, 10)                20490     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 113,738\n",
            "Trainable params: 113,738\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/30\n",
            "1250/1250 [==============================] - 19s 14ms/step - loss: 1.5672 - accuracy: 0.4316 - val_loss: 1.2286 - val_accuracy: 0.5607 - lr: 0.0010\n",
            "Epoch 2/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.2266 - accuracy: 0.5646 - val_loss: 1.0530 - val_accuracy: 0.6275 - lr: 0.0010\n",
            "Epoch 3/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 1.0918 - accuracy: 0.6168 - val_loss: 0.9823 - val_accuracy: 0.6614 - lr: 0.0010\n",
            "Epoch 4/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.0009 - accuracy: 0.6483 - val_loss: 0.9026 - val_accuracy: 0.6859 - lr: 0.0010\n",
            "Epoch 5/30\n",
            "1250/1250 [==============================] - 20s 16ms/step - loss: 0.9488 - accuracy: 0.6715 - val_loss: 0.8402 - val_accuracy: 0.7068 - lr: 0.0010\n",
            "Epoch 6/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.8995 - accuracy: 0.6886 - val_loss: 0.8012 - val_accuracy: 0.7229 - lr: 0.0010\n",
            "Epoch 7/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.8650 - accuracy: 0.6986 - val_loss: 0.7971 - val_accuracy: 0.7316 - lr: 0.0010\n",
            "Epoch 8/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.8409 - accuracy: 0.7081 - val_loss: 0.8018 - val_accuracy: 0.7309 - lr: 0.0010\n",
            "Epoch 9/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.8153 - accuracy: 0.7166 - val_loss: 0.7440 - val_accuracy: 0.7504 - lr: 0.0010\n",
            "Epoch 10/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.7939 - accuracy: 0.7226 - val_loss: 0.7002 - val_accuracy: 0.7578 - lr: 0.0010\n",
            "Epoch 11/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.7200 - accuracy: 0.7518 - val_loss: 0.6979 - val_accuracy: 0.7628 - lr: 5.0000e-04\n",
            "Epoch 12/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 0.7069 - accuracy: 0.7548 - val_loss: 0.6818 - val_accuracy: 0.7624 - lr: 5.0000e-04\n",
            "Epoch 13/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.6906 - accuracy: 0.7624 - val_loss: 0.6902 - val_accuracy: 0.7656 - lr: 5.0000e-04\n",
            "Epoch 14/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.6793 - accuracy: 0.7643 - val_loss: 0.6512 - val_accuracy: 0.7791 - lr: 5.0000e-04\n",
            "Epoch 15/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.6728 - accuracy: 0.7652 - val_loss: 0.6266 - val_accuracy: 0.7835 - lr: 5.0000e-04\n",
            "Epoch 16/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 0.6630 - accuracy: 0.7697 - val_loss: 0.6080 - val_accuracy: 0.7916 - lr: 5.0000e-04\n",
            "Epoch 17/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.6530 - accuracy: 0.7715 - val_loss: 0.6523 - val_accuracy: 0.7785 - lr: 5.0000e-04\n",
            "Epoch 18/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.6420 - accuracy: 0.7765 - val_loss: 0.6876 - val_accuracy: 0.7715 - lr: 5.0000e-04\n",
            "Epoch 19/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.6363 - accuracy: 0.7766 - val_loss: 0.6304 - val_accuracy: 0.7832 - lr: 5.0000e-04\n",
            "Epoch 20/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.6301 - accuracy: 0.7794 - val_loss: 0.6739 - val_accuracy: 0.7727 - lr: 5.0000e-04\n",
            "Epoch 21/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.5822 - accuracy: 0.7970 - val_loss: 0.5910 - val_accuracy: 0.7989 - lr: 1.2500e-04\n",
            "Epoch 22/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.5764 - accuracy: 0.7999 - val_loss: 0.5905 - val_accuracy: 0.7960 - lr: 1.2500e-04\n",
            "Epoch 23/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.5691 - accuracy: 0.8042 - val_loss: 0.5829 - val_accuracy: 0.7999 - lr: 1.2500e-04\n",
            "Epoch 24/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.5629 - accuracy: 0.8041 - val_loss: 0.6014 - val_accuracy: 0.7964 - lr: 1.2500e-04\n",
            "Epoch 25/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.5648 - accuracy: 0.8052 - val_loss: 0.5939 - val_accuracy: 0.7970 - lr: 1.2500e-04\n",
            "Epoch 26/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.5623 - accuracy: 0.8050 - val_loss: 0.5909 - val_accuracy: 0.7974 - lr: 1.2500e-04\n",
            "Epoch 27/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.5621 - accuracy: 0.8051 - val_loss: 0.5918 - val_accuracy: 0.7969 - lr: 1.2500e-04\n",
            "Epoch 28/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.5577 - accuracy: 0.8061 - val_loss: 0.5857 - val_accuracy: 0.8009 - lr: 1.2500e-04\n",
            "Epoch 29/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.5531 - accuracy: 0.8073 - val_loss: 0.5939 - val_accuracy: 0.7960 - lr: 1.2500e-04\n",
            "Epoch 30/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.5492 - accuracy: 0.8103 - val_loss: 0.5836 - val_accuracy: 0.8007 - lr: 1.2500e-04\n",
            "Score for fold 5: loss of 0.5835533142089844; accuracy of 80.07000088691711%\n",
            "------------------------------------------------------------------------\n",
            "Score per fold\n",
            "------------------------------------------------------------------------\n",
            "> Fold 1 - Loss: 0.5959774255752563 - Accuracy: 79.6999990940094%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 2 - Loss: 0.6051555871963501 - Accuracy: 79.85000014305115%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 3 - Loss: 0.5946542024612427 - Accuracy: 79.80999946594238%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 4 - Loss: 0.5933048129081726 - Accuracy: 79.67000007629395%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 5 - Loss: 0.5835533142089844 - Accuracy: 80.07000088691711%\n",
            "------------------------------------------------------------------------\n",
            "Average scores for all folds:\n",
            "> Accuracy: 79.8199999332428 (+- 0.1417043805069099)\n",
            "> Loss: 0.5945290684700012\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3 hidden layers and Narrow Structure"
      ],
      "metadata": {
        "id": "L7qGOt1A6RBW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "acc_per_fold = []\n",
        "loss_per_fold = []\n",
        "\n",
        "with tf.device('/device:GPU:0'):\n",
        "  fold_no = 1\n",
        "  for train, valid in kfold.split(trainX, trainy):\n",
        "\n",
        "    model = CNN_Model(batch_size=32, depth=3, hidden_neurons_1=4, hidden_neurons_2=32, hidden_neurons_3=64, kernel_size_1=3, kernel_size_2=3, kernel_size_3=3, \n",
        "               dropout_rate=0.2, batch_norm=False, pooling='max', layer_activation_1='relu', layer_activation_2='relu', layer_activation_3='relu',\n",
        "               kernel_initialiser='glorot_uniform', bias_initialiser='zeros', optimiser='Adam', weight_decay=None, learning_rate=1e-3, output_activation='softmax', SGD_momentum=0) \n",
        "  \n",
        "    train_gen = tf.keras.preprocessing.image.ImageDataGenerator(width_shift_range=0.1,\n",
        "      height_shift_range=0.1,\n",
        "      horizontal_flip=True)\n",
        "    train_gen.fit(trainX[train])\n",
        "  \n",
        "    print('------------------------------------------------------------------------')\n",
        "    print(f'Training for fold {fold_no} ...')\n",
        "\n",
        "    history = model.train(\n",
        "              train_gen,\n",
        "                trainX[train],\n",
        "                  trainy[train],\n",
        "                trainX[valid],\n",
        "                  trainy[valid],\n",
        "                  lr_rate_type='step')\n",
        "\n",
        "    scores = model.evaluate(trainX[valid], trainy[valid])\n",
        "\n",
        "    print(f'Score for fold {fold_no}: loss of {scores[0]}; accuracy of {scores[1]*100}%')\n",
        "    acc_per_fold.append(scores[1] * 100)\n",
        "    loss_per_fold.append(scores[0])\n",
        "\n",
        "    fold_no+=1\n",
        "\n",
        "# == Provide average scores ==\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Score per fold')\n",
        "for i in range(0, len(acc_per_fold)):\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Average scores for all folds:')\n",
        "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
        "print(f'> Loss: {np.mean(loss_per_fold)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rNwS1fq_8TmN",
        "outputId": "df5326bc-7fcf-476c-b386-7a2cb431911e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_65\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_180 (Conv2D)         (None, 32, 32, 4)         112       \n",
            "                                                                 \n",
            " max_pooling2d_65 (MaxPoolin  multiple                 0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_181 (Conv2D)         (None, 16, 16, 32)        1184      \n",
            "                                                                 \n",
            " dropout_65 (Dropout)        (None, 8, 8, 32)          0         \n",
            "                                                                 \n",
            " conv2d_182 (Conv2D)         (None, 8, 8, 64)          18496     \n",
            "                                                                 \n",
            " flatten_65 (Flatten)        (None, 1024)              0         \n",
            "                                                                 \n",
            " dense_65 (Dense)            (None, 10)                10250     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 30,042\n",
            "Trainable params: 30,042\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 1.6956 - accuracy: 0.3839 - val_loss: 1.4537 - val_accuracy: 0.4771 - lr: 0.0010\n",
            "Epoch 2/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 1.4321 - accuracy: 0.4919 - val_loss: 1.2530 - val_accuracy: 0.5631 - lr: 0.0010\n",
            "Epoch 3/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 1.3139 - accuracy: 0.5353 - val_loss: 1.1800 - val_accuracy: 0.5783 - lr: 0.0010\n",
            "Epoch 4/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 1.2339 - accuracy: 0.5624 - val_loss: 1.1108 - val_accuracy: 0.6106 - lr: 0.0010\n",
            "Epoch 5/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 1.1734 - accuracy: 0.5876 - val_loss: 1.1328 - val_accuracy: 0.5985 - lr: 0.0010\n",
            "Epoch 6/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.1374 - accuracy: 0.6013 - val_loss: 1.0347 - val_accuracy: 0.6366 - lr: 0.0010\n",
            "Epoch 7/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 1.0974 - accuracy: 0.6148 - val_loss: 1.0224 - val_accuracy: 0.6446 - lr: 0.0010\n",
            "Epoch 8/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.0780 - accuracy: 0.6227 - val_loss: 0.9845 - val_accuracy: 0.6570 - lr: 0.0010\n",
            "Epoch 9/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.0577 - accuracy: 0.6310 - val_loss: 0.9560 - val_accuracy: 0.6649 - lr: 0.0010\n",
            "Epoch 10/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 1.0407 - accuracy: 0.6374 - val_loss: 0.9604 - val_accuracy: 0.6655 - lr: 0.0010\n",
            "Epoch 11/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.9886 - accuracy: 0.6576 - val_loss: 0.9411 - val_accuracy: 0.6727 - lr: 5.0000e-04\n",
            "Epoch 12/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.9780 - accuracy: 0.6591 - val_loss: 0.9728 - val_accuracy: 0.6631 - lr: 5.0000e-04\n",
            "Epoch 13/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.9696 - accuracy: 0.6608 - val_loss: 0.9018 - val_accuracy: 0.6860 - lr: 5.0000e-04\n",
            "Epoch 14/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.9645 - accuracy: 0.6648 - val_loss: 0.9004 - val_accuracy: 0.6892 - lr: 5.0000e-04\n",
            "Epoch 15/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.9539 - accuracy: 0.6673 - val_loss: 0.8874 - val_accuracy: 0.6903 - lr: 5.0000e-04\n",
            "Epoch 16/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.9446 - accuracy: 0.6717 - val_loss: 0.8737 - val_accuracy: 0.6953 - lr: 5.0000e-04\n",
            "Epoch 17/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.9458 - accuracy: 0.6712 - val_loss: 0.8667 - val_accuracy: 0.6989 - lr: 5.0000e-04\n",
            "Epoch 18/30\n",
            "1250/1250 [==============================] - 20s 16ms/step - loss: 0.9316 - accuracy: 0.6766 - val_loss: 0.8720 - val_accuracy: 0.6989 - lr: 5.0000e-04\n",
            "Epoch 19/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.9302 - accuracy: 0.6769 - val_loss: 0.8599 - val_accuracy: 0.7005 - lr: 5.0000e-04\n",
            "Epoch 20/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 0.9313 - accuracy: 0.6778 - val_loss: 0.8874 - val_accuracy: 0.6970 - lr: 5.0000e-04\n",
            "Epoch 21/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 0.8982 - accuracy: 0.6875 - val_loss: 0.8517 - val_accuracy: 0.7075 - lr: 1.2500e-04\n",
            "Epoch 22/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.8956 - accuracy: 0.6884 - val_loss: 0.8389 - val_accuracy: 0.7098 - lr: 1.2500e-04\n",
            "Epoch 23/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.8873 - accuracy: 0.6924 - val_loss: 0.8464 - val_accuracy: 0.7085 - lr: 1.2500e-04\n",
            "Epoch 24/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.8873 - accuracy: 0.6922 - val_loss: 0.8676 - val_accuracy: 0.7029 - lr: 1.2500e-04\n",
            "Epoch 25/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.8879 - accuracy: 0.6915 - val_loss: 0.8467 - val_accuracy: 0.7084 - lr: 1.2500e-04\n",
            "Epoch 26/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 0.8861 - accuracy: 0.6920 - val_loss: 0.8579 - val_accuracy: 0.7047 - lr: 1.2500e-04\n",
            "Epoch 27/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.8805 - accuracy: 0.6925 - val_loss: 0.8608 - val_accuracy: 0.7020 - lr: 1.2500e-04\n",
            "Epoch 28/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.8821 - accuracy: 0.6942 - val_loss: 0.8433 - val_accuracy: 0.7072 - lr: 1.2500e-04\n",
            "Epoch 29/30\n",
            "1250/1250 [==============================] - 22s 18ms/step - loss: 0.8793 - accuracy: 0.6967 - val_loss: 0.8509 - val_accuracy: 0.7052 - lr: 1.2500e-04\n",
            "Epoch 30/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.8742 - accuracy: 0.6945 - val_loss: 0.8377 - val_accuracy: 0.7099 - lr: 1.2500e-04\n",
            "Score for fold 1: loss of 0.8377110362052917; accuracy of 70.99000215530396%\n",
            "Model: \"sequential_66\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_183 (Conv2D)         (None, 32, 32, 4)         112       \n",
            "                                                                 \n",
            " max_pooling2d_66 (MaxPoolin  multiple                 0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_184 (Conv2D)         (None, 16, 16, 32)        1184      \n",
            "                                                                 \n",
            " dropout_66 (Dropout)        (None, 8, 8, 32)          0         \n",
            "                                                                 \n",
            " conv2d_185 (Conv2D)         (None, 8, 8, 64)          18496     \n",
            "                                                                 \n",
            " flatten_66 (Flatten)        (None, 1024)              0         \n",
            "                                                                 \n",
            " dense_66 (Dense)            (None, 10)                10250     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 30,042\n",
            "Trainable params: 30,042\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/30\n",
            "1250/1250 [==============================] - 22s 17ms/step - loss: 1.7395 - accuracy: 0.3717 - val_loss: 1.5630 - val_accuracy: 0.4492 - lr: 0.0010\n",
            "Epoch 2/30\n",
            "1250/1250 [==============================] - 20s 16ms/step - loss: 1.4458 - accuracy: 0.4857 - val_loss: 1.3058 - val_accuracy: 0.5350 - lr: 0.0010\n",
            "Epoch 3/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.3372 - accuracy: 0.5285 - val_loss: 1.2121 - val_accuracy: 0.5726 - lr: 0.0010\n",
            "Epoch 4/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 1.2554 - accuracy: 0.5567 - val_loss: 1.1587 - val_accuracy: 0.5900 - lr: 0.0010\n",
            "Epoch 5/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.1999 - accuracy: 0.5768 - val_loss: 1.1504 - val_accuracy: 0.5900 - lr: 0.0010\n",
            "Epoch 6/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.1594 - accuracy: 0.5907 - val_loss: 1.0844 - val_accuracy: 0.6140 - lr: 0.0010\n",
            "Epoch 7/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.1228 - accuracy: 0.6075 - val_loss: 1.0660 - val_accuracy: 0.6279 - lr: 0.0010\n",
            "Epoch 8/30\n",
            "1250/1250 [==============================] - 19s 16ms/step - loss: 1.0946 - accuracy: 0.6155 - val_loss: 1.0007 - val_accuracy: 0.6500 - lr: 0.0010\n",
            "Epoch 9/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.0756 - accuracy: 0.6248 - val_loss: 1.0215 - val_accuracy: 0.6366 - lr: 0.0010\n",
            "Epoch 10/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 1.0486 - accuracy: 0.6331 - val_loss: 0.9692 - val_accuracy: 0.6529 - lr: 0.0010\n",
            "Epoch 11/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.0134 - accuracy: 0.6427 - val_loss: 0.9405 - val_accuracy: 0.6739 - lr: 5.0000e-04\n",
            "Epoch 12/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.9976 - accuracy: 0.6505 - val_loss: 0.9077 - val_accuracy: 0.6796 - lr: 5.0000e-04\n",
            "Epoch 13/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.9855 - accuracy: 0.6544 - val_loss: 0.9411 - val_accuracy: 0.6715 - lr: 5.0000e-04\n",
            "Epoch 14/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.9843 - accuracy: 0.6561 - val_loss: 0.9275 - val_accuracy: 0.6783 - lr: 5.0000e-04\n",
            "Epoch 15/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.9750 - accuracy: 0.6592 - val_loss: 0.9027 - val_accuracy: 0.6856 - lr: 5.0000e-04\n",
            "Epoch 16/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.9675 - accuracy: 0.6645 - val_loss: 0.9107 - val_accuracy: 0.6821 - lr: 5.0000e-04\n",
            "Epoch 17/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.9598 - accuracy: 0.6649 - val_loss: 0.9274 - val_accuracy: 0.6738 - lr: 5.0000e-04\n",
            "Epoch 18/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.9557 - accuracy: 0.6671 - val_loss: 0.8949 - val_accuracy: 0.6860 - lr: 5.0000e-04\n",
            "Epoch 19/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.9447 - accuracy: 0.6694 - val_loss: 0.8969 - val_accuracy: 0.6892 - lr: 5.0000e-04\n",
            "Epoch 20/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.9418 - accuracy: 0.6761 - val_loss: 0.8946 - val_accuracy: 0.6826 - lr: 5.0000e-04\n",
            "Epoch 21/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.9144 - accuracy: 0.6829 - val_loss: 0.8646 - val_accuracy: 0.7013 - lr: 1.2500e-04\n",
            "Epoch 22/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.9111 - accuracy: 0.6833 - val_loss: 0.8590 - val_accuracy: 0.7008 - lr: 1.2500e-04\n",
            "Epoch 23/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.9088 - accuracy: 0.6837 - val_loss: 0.8759 - val_accuracy: 0.6941 - lr: 1.2500e-04\n",
            "Epoch 24/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.9102 - accuracy: 0.6819 - val_loss: 0.8703 - val_accuracy: 0.6974 - lr: 1.2500e-04\n",
            "Epoch 25/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.9087 - accuracy: 0.6816 - val_loss: 0.8624 - val_accuracy: 0.7024 - lr: 1.2500e-04\n",
            "Epoch 26/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.9041 - accuracy: 0.6849 - val_loss: 0.8654 - val_accuracy: 0.6972 - lr: 1.2500e-04\n",
            "Epoch 27/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.8987 - accuracy: 0.6870 - val_loss: 0.8525 - val_accuracy: 0.7031 - lr: 1.2500e-04\n",
            "Epoch 28/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.8987 - accuracy: 0.6889 - val_loss: 0.8623 - val_accuracy: 0.6999 - lr: 1.2500e-04\n",
            "Epoch 29/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.8998 - accuracy: 0.6873 - val_loss: 0.8665 - val_accuracy: 0.6988 - lr: 1.2500e-04\n",
            "Epoch 30/30\n",
            "1250/1250 [==============================] - 20s 16ms/step - loss: 0.9004 - accuracy: 0.6852 - val_loss: 0.8597 - val_accuracy: 0.7016 - lr: 1.2500e-04\n",
            "Score for fold 2: loss of 0.8597040772438049; accuracy of 70.16000151634216%\n",
            "Model: \"sequential_67\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_186 (Conv2D)         (None, 32, 32, 4)         112       \n",
            "                                                                 \n",
            " max_pooling2d_67 (MaxPoolin  multiple                 0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_187 (Conv2D)         (None, 16, 16, 32)        1184      \n",
            "                                                                 \n",
            " dropout_67 (Dropout)        (None, 8, 8, 32)          0         \n",
            "                                                                 \n",
            " conv2d_188 (Conv2D)         (None, 8, 8, 64)          18496     \n",
            "                                                                 \n",
            " flatten_67 (Flatten)        (None, 1024)              0         \n",
            "                                                                 \n",
            " dense_67 (Dense)            (None, 10)                10250     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 30,042\n",
            "Trainable params: 30,042\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/30\n",
            "1250/1250 [==============================] - 20s 15ms/step - loss: 1.7085 - accuracy: 0.3768 - val_loss: 1.5020 - val_accuracy: 0.4628 - lr: 0.0010\n",
            "Epoch 2/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.4671 - accuracy: 0.4728 - val_loss: 1.3283 - val_accuracy: 0.5182 - lr: 0.0010\n",
            "Epoch 3/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 1.3793 - accuracy: 0.5085 - val_loss: 1.2481 - val_accuracy: 0.5552 - lr: 0.0010\n",
            "Epoch 4/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 1.3080 - accuracy: 0.5327 - val_loss: 1.1923 - val_accuracy: 0.5750 - lr: 0.0010\n",
            "Epoch 5/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 1.2409 - accuracy: 0.5598 - val_loss: 1.1180 - val_accuracy: 0.6041 - lr: 0.0010\n",
            "Epoch 6/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.1892 - accuracy: 0.5809 - val_loss: 1.0593 - val_accuracy: 0.6261 - lr: 0.0010\n",
            "Epoch 7/30\n",
            "1250/1250 [==============================] - 21s 17ms/step - loss: 1.1515 - accuracy: 0.5975 - val_loss: 1.0306 - val_accuracy: 0.6332 - lr: 0.0010\n",
            "Epoch 8/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 1.1152 - accuracy: 0.6113 - val_loss: 1.0108 - val_accuracy: 0.6439 - lr: 0.0010\n",
            "Epoch 9/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 1.0880 - accuracy: 0.6186 - val_loss: 1.0321 - val_accuracy: 0.6428 - lr: 0.0010\n",
            "Epoch 10/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 1.0595 - accuracy: 0.6305 - val_loss: 1.0008 - val_accuracy: 0.6543 - lr: 0.0010\n",
            "Epoch 11/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 1.0164 - accuracy: 0.6474 - val_loss: 0.9366 - val_accuracy: 0.6751 - lr: 5.0000e-04\n",
            "Epoch 12/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 1.0061 - accuracy: 0.6477 - val_loss: 0.9393 - val_accuracy: 0.6738 - lr: 5.0000e-04\n",
            "Epoch 13/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 0.9927 - accuracy: 0.6547 - val_loss: 0.9532 - val_accuracy: 0.6752 - lr: 5.0000e-04\n",
            "Epoch 14/30\n",
            "1250/1250 [==============================] - 20s 16ms/step - loss: 0.9772 - accuracy: 0.6588 - val_loss: 0.9355 - val_accuracy: 0.6797 - lr: 5.0000e-04\n",
            "Epoch 15/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.9700 - accuracy: 0.6620 - val_loss: 0.9346 - val_accuracy: 0.6810 - lr: 5.0000e-04\n",
            "Epoch 16/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.9632 - accuracy: 0.6627 - val_loss: 0.9531 - val_accuracy: 0.6784 - lr: 5.0000e-04\n",
            "Epoch 17/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.9544 - accuracy: 0.6678 - val_loss: 0.9132 - val_accuracy: 0.6873 - lr: 5.0000e-04\n",
            "Epoch 18/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.9478 - accuracy: 0.6704 - val_loss: 0.9320 - val_accuracy: 0.6822 - lr: 5.0000e-04\n",
            "Epoch 19/30\n",
            "1250/1250 [==============================] - 19s 16ms/step - loss: 0.9403 - accuracy: 0.6721 - val_loss: 0.9062 - val_accuracy: 0.6922 - lr: 5.0000e-04\n",
            "Epoch 20/30\n",
            "1250/1250 [==============================] - 19s 16ms/step - loss: 0.9426 - accuracy: 0.6713 - val_loss: 0.9222 - val_accuracy: 0.6846 - lr: 5.0000e-04\n",
            "Epoch 21/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.9085 - accuracy: 0.6837 - val_loss: 0.8764 - val_accuracy: 0.6982 - lr: 1.2500e-04\n",
            "Epoch 22/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 0.9021 - accuracy: 0.6861 - val_loss: 0.8759 - val_accuracy: 0.6973 - lr: 1.2500e-04\n",
            "Epoch 23/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.9059 - accuracy: 0.6874 - val_loss: 0.8643 - val_accuracy: 0.7044 - lr: 1.2500e-04\n",
            "Epoch 24/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.9026 - accuracy: 0.6865 - val_loss: 0.8818 - val_accuracy: 0.6972 - lr: 1.2500e-04\n",
            "Epoch 25/30\n",
            "1250/1250 [==============================] - 20s 16ms/step - loss: 0.8984 - accuracy: 0.6857 - val_loss: 0.8804 - val_accuracy: 0.6991 - lr: 1.2500e-04\n",
            "Epoch 26/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 0.9001 - accuracy: 0.6875 - val_loss: 0.8769 - val_accuracy: 0.6994 - lr: 1.2500e-04\n",
            "Epoch 27/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 0.8947 - accuracy: 0.6902 - val_loss: 0.8786 - val_accuracy: 0.6967 - lr: 1.2500e-04\n",
            "Epoch 28/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.8913 - accuracy: 0.6893 - val_loss: 0.8712 - val_accuracy: 0.7006 - lr: 1.2500e-04\n",
            "Epoch 29/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.8905 - accuracy: 0.6933 - val_loss: 0.8707 - val_accuracy: 0.7002 - lr: 1.2500e-04\n",
            "Epoch 30/30\n",
            "1250/1250 [==============================] - 20s 16ms/step - loss: 0.8883 - accuracy: 0.6902 - val_loss: 0.8847 - val_accuracy: 0.6936 - lr: 1.2500e-04\n",
            "Score for fold 3: loss of 0.8846662044525146; accuracy of 69.35999989509583%\n",
            "Model: \"sequential_68\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_189 (Conv2D)         (None, 32, 32, 4)         112       \n",
            "                                                                 \n",
            " max_pooling2d_68 (MaxPoolin  multiple                 0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_190 (Conv2D)         (None, 16, 16, 32)        1184      \n",
            "                                                                 \n",
            " dropout_68 (Dropout)        (None, 8, 8, 32)          0         \n",
            "                                                                 \n",
            " conv2d_191 (Conv2D)         (None, 8, 8, 64)          18496     \n",
            "                                                                 \n",
            " flatten_68 (Flatten)        (None, 1024)              0         \n",
            "                                                                 \n",
            " dense_68 (Dense)            (None, 10)                10250     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 30,042\n",
            "Trainable params: 30,042\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/30\n",
            "1250/1250 [==============================] - 20s 15ms/step - loss: 1.7047 - accuracy: 0.3828 - val_loss: 1.4612 - val_accuracy: 0.4725 - lr: 0.0010\n",
            "Epoch 2/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 1.4216 - accuracy: 0.4933 - val_loss: 1.2622 - val_accuracy: 0.5559 - lr: 0.0010\n",
            "Epoch 3/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 1.3106 - accuracy: 0.5359 - val_loss: 1.2321 - val_accuracy: 0.5671 - lr: 0.0010\n",
            "Epoch 4/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 1.2367 - accuracy: 0.5628 - val_loss: 1.1187 - val_accuracy: 0.6113 - lr: 0.0010\n",
            "Epoch 5/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 1.1891 - accuracy: 0.5792 - val_loss: 1.1058 - val_accuracy: 0.6016 - lr: 0.0010\n",
            "Epoch 6/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 1.1441 - accuracy: 0.5958 - val_loss: 1.0729 - val_accuracy: 0.6176 - lr: 0.0010\n",
            "Epoch 7/30\n",
            "1250/1250 [==============================] - 20s 16ms/step - loss: 1.1176 - accuracy: 0.6065 - val_loss: 1.1245 - val_accuracy: 0.6083 - lr: 0.0010\n",
            "Epoch 8/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 1.0893 - accuracy: 0.6165 - val_loss: 1.0567 - val_accuracy: 0.6307 - lr: 0.0010\n",
            "Epoch 9/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.0687 - accuracy: 0.6241 - val_loss: 0.9628 - val_accuracy: 0.6632 - lr: 0.0010\n",
            "Epoch 10/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 1.0487 - accuracy: 0.6298 - val_loss: 0.9567 - val_accuracy: 0.6687 - lr: 0.0010\n",
            "Epoch 11/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.0016 - accuracy: 0.6477 - val_loss: 0.9395 - val_accuracy: 0.6692 - lr: 5.0000e-04\n",
            "Epoch 12/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.9905 - accuracy: 0.6536 - val_loss: 0.9841 - val_accuracy: 0.6533 - lr: 5.0000e-04\n",
            "Epoch 13/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.9829 - accuracy: 0.6553 - val_loss: 0.9512 - val_accuracy: 0.6676 - lr: 5.0000e-04\n",
            "Epoch 14/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 0.9747 - accuracy: 0.6581 - val_loss: 0.9582 - val_accuracy: 0.6653 - lr: 5.0000e-04\n",
            "Epoch 15/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.9651 - accuracy: 0.6618 - val_loss: 0.9289 - val_accuracy: 0.6756 - lr: 5.0000e-04\n",
            "Epoch 16/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.9571 - accuracy: 0.6655 - val_loss: 0.9141 - val_accuracy: 0.6826 - lr: 5.0000e-04\n",
            "Epoch 17/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 0.9518 - accuracy: 0.6694 - val_loss: 0.9303 - val_accuracy: 0.6741 - lr: 5.0000e-04\n",
            "Epoch 18/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 0.9508 - accuracy: 0.6661 - val_loss: 0.8702 - val_accuracy: 0.6996 - lr: 5.0000e-04\n",
            "Epoch 19/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.9454 - accuracy: 0.6698 - val_loss: 0.8958 - val_accuracy: 0.6862 - lr: 5.0000e-04\n",
            "Epoch 20/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 0.9364 - accuracy: 0.6705 - val_loss: 0.8972 - val_accuracy: 0.6885 - lr: 5.0000e-04\n",
            "Epoch 21/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.9087 - accuracy: 0.6819 - val_loss: 0.8843 - val_accuracy: 0.6927 - lr: 1.2500e-04\n",
            "Epoch 22/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.9049 - accuracy: 0.6819 - val_loss: 0.8770 - val_accuracy: 0.6954 - lr: 1.2500e-04\n",
            "Epoch 23/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.8983 - accuracy: 0.6844 - val_loss: 0.9079 - val_accuracy: 0.6847 - lr: 1.2500e-04\n",
            "Epoch 24/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.8993 - accuracy: 0.6872 - val_loss: 0.8902 - val_accuracy: 0.6888 - lr: 1.2500e-04\n",
            "Epoch 25/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.8994 - accuracy: 0.6863 - val_loss: 0.8582 - val_accuracy: 0.7000 - lr: 1.2500e-04\n",
            "Epoch 26/30\n",
            "1250/1250 [==============================] - 20s 16ms/step - loss: 0.8953 - accuracy: 0.6860 - val_loss: 0.8809 - val_accuracy: 0.6939 - lr: 1.2500e-04\n",
            "Epoch 27/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.8937 - accuracy: 0.6885 - val_loss: 0.8862 - val_accuracy: 0.6902 - lr: 1.2500e-04\n",
            "Epoch 28/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.8926 - accuracy: 0.6886 - val_loss: 0.8685 - val_accuracy: 0.6981 - lr: 1.2500e-04\n",
            "Epoch 29/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.8933 - accuracy: 0.6872 - val_loss: 0.8674 - val_accuracy: 0.6976 - lr: 1.2500e-04\n",
            "Epoch 30/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.8921 - accuracy: 0.6880 - val_loss: 0.8660 - val_accuracy: 0.6991 - lr: 1.2500e-04\n",
            "Score for fold 4: loss of 0.8660297989845276; accuracy of 69.91000175476074%\n",
            "Model: \"sequential_69\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_192 (Conv2D)         (None, 32, 32, 4)         112       \n",
            "                                                                 \n",
            " max_pooling2d_69 (MaxPoolin  multiple                 0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_193 (Conv2D)         (None, 16, 16, 32)        1184      \n",
            "                                                                 \n",
            " dropout_69 (Dropout)        (None, 8, 8, 32)          0         \n",
            "                                                                 \n",
            " conv2d_194 (Conv2D)         (None, 8, 8, 64)          18496     \n",
            "                                                                 \n",
            " flatten_69 (Flatten)        (None, 1024)              0         \n",
            "                                                                 \n",
            " dense_69 (Dense)            (None, 10)                10250     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 30,042\n",
            "Trainable params: 30,042\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 1.7134 - accuracy: 0.3786 - val_loss: 1.4599 - val_accuracy: 0.4811 - lr: 0.0010\n",
            "Epoch 2/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.4416 - accuracy: 0.4858 - val_loss: 1.2953 - val_accuracy: 0.5452 - lr: 0.0010\n",
            "Epoch 3/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.3351 - accuracy: 0.5264 - val_loss: 1.2513 - val_accuracy: 0.5633 - lr: 0.0010\n",
            "Epoch 4/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.2636 - accuracy: 0.5508 - val_loss: 1.1846 - val_accuracy: 0.5953 - lr: 0.0010\n",
            "Epoch 5/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 1.2104 - accuracy: 0.5716 - val_loss: 1.1005 - val_accuracy: 0.6207 - lr: 0.0010\n",
            "Epoch 6/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.1684 - accuracy: 0.5883 - val_loss: 1.1165 - val_accuracy: 0.6182 - lr: 0.0010\n",
            "Epoch 7/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 1.1345 - accuracy: 0.6020 - val_loss: 1.0317 - val_accuracy: 0.6464 - lr: 0.0010\n",
            "Epoch 8/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.1116 - accuracy: 0.6084 - val_loss: 1.0308 - val_accuracy: 0.6485 - lr: 0.0010\n",
            "Epoch 9/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.0945 - accuracy: 0.6155 - val_loss: 1.0219 - val_accuracy: 0.6499 - lr: 0.0010\n",
            "Epoch 10/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.0702 - accuracy: 0.6247 - val_loss: 0.9669 - val_accuracy: 0.6652 - lr: 0.0010\n",
            "Epoch 11/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.0266 - accuracy: 0.6396 - val_loss: 0.9961 - val_accuracy: 0.6622 - lr: 5.0000e-04\n",
            "Epoch 12/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 1.0106 - accuracy: 0.6459 - val_loss: 0.9714 - val_accuracy: 0.6660 - lr: 5.0000e-04\n",
            "Epoch 13/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.9992 - accuracy: 0.6494 - val_loss: 0.9814 - val_accuracy: 0.6623 - lr: 5.0000e-04\n",
            "Epoch 14/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.9934 - accuracy: 0.6527 - val_loss: 0.9501 - val_accuracy: 0.6742 - lr: 5.0000e-04\n",
            "Epoch 15/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.9848 - accuracy: 0.6548 - val_loss: 0.9414 - val_accuracy: 0.6768 - lr: 5.0000e-04\n",
            "Epoch 16/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 0.9784 - accuracy: 0.6567 - val_loss: 0.9352 - val_accuracy: 0.6755 - lr: 5.0000e-04\n",
            "Epoch 17/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 0.9753 - accuracy: 0.6589 - val_loss: 0.9239 - val_accuracy: 0.6838 - lr: 5.0000e-04\n",
            "Epoch 18/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 0.9658 - accuracy: 0.6627 - val_loss: 0.9093 - val_accuracy: 0.6851 - lr: 5.0000e-04\n",
            "Epoch 19/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 0.9629 - accuracy: 0.6643 - val_loss: 0.9521 - val_accuracy: 0.6752 - lr: 5.0000e-04\n",
            "Epoch 20/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.9539 - accuracy: 0.6660 - val_loss: 0.9005 - val_accuracy: 0.6896 - lr: 5.0000e-04\n",
            "Epoch 21/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.9231 - accuracy: 0.6786 - val_loss: 0.9287 - val_accuracy: 0.6796 - lr: 1.2500e-04\n",
            "Epoch 22/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.9223 - accuracy: 0.6780 - val_loss: 0.8951 - val_accuracy: 0.6911 - lr: 1.2500e-04\n",
            "Epoch 23/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.9151 - accuracy: 0.6810 - val_loss: 0.9183 - val_accuracy: 0.6853 - lr: 1.2500e-04\n",
            "Epoch 24/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.9177 - accuracy: 0.6794 - val_loss: 0.8885 - val_accuracy: 0.6961 - lr: 1.2500e-04\n",
            "Epoch 25/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.9132 - accuracy: 0.6802 - val_loss: 0.8971 - val_accuracy: 0.6929 - lr: 1.2500e-04\n",
            "Epoch 26/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.9105 - accuracy: 0.6822 - val_loss: 0.9108 - val_accuracy: 0.6884 - lr: 1.2500e-04\n",
            "Epoch 27/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.9042 - accuracy: 0.6886 - val_loss: 0.8918 - val_accuracy: 0.6918 - lr: 1.2500e-04\n",
            "Epoch 28/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.9056 - accuracy: 0.6860 - val_loss: 0.9057 - val_accuracy: 0.6867 - lr: 1.2500e-04\n",
            "Epoch 29/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.9072 - accuracy: 0.6844 - val_loss: 0.8958 - val_accuracy: 0.6915 - lr: 1.2500e-04\n",
            "Epoch 30/30\n",
            "1250/1250 [==============================] - 20s 16ms/step - loss: 0.8994 - accuracy: 0.6874 - val_loss: 0.8764 - val_accuracy: 0.7013 - lr: 1.2500e-04\n",
            "Score for fold 5: loss of 0.8763742446899414; accuracy of 70.13000249862671%\n",
            "------------------------------------------------------------------------\n",
            "Score per fold\n",
            "------------------------------------------------------------------------\n",
            "> Fold 1 - Loss: 0.8377110362052917 - Accuracy: 70.99000215530396%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 2 - Loss: 0.8597040772438049 - Accuracy: 70.16000151634216%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 3 - Loss: 0.8846662044525146 - Accuracy: 69.35999989509583%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 4 - Loss: 0.8660297989845276 - Accuracy: 69.91000175476074%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 5 - Loss: 0.8763742446899414 - Accuracy: 70.13000249862671%\n",
            "------------------------------------------------------------------------\n",
            "Average scores for all folds:\n",
            "> Accuracy: 70.11000156402588 (+- 0.525319617065698)\n",
            "> Loss: 0.8648970723152161\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 4 hidden layers and Wide Structure"
      ],
      "metadata": {
        "id": "-EVb-2bj7NBs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "acc_per_fold = []\n",
        "loss_per_fold = []\n",
        "\n",
        "with tf.device('/device:GPU:0'):\n",
        "  fold_no = 1\n",
        "  for train, valid in kfold.split(trainX, trainy):\n",
        "\n",
        "    model = CNN_Model(batch_size=32, depth=4, hidden_neurons_1=64, hidden_neurons_2=128, hidden_neurons_3=256, kernel_size_1=3, kernel_size_2=3, kernel_size_3=3, \n",
        "               dropout_rate=0.2, batch_norm=False, pooling='max', layer_activation_1='relu', layer_activation_2='relu', layer_activation_3='relu',\n",
        "               kernel_initialiser='glorot_uniform', bias_initialiser='zeros', optimiser='Adam', weight_decay=None, learning_rate=1e-3, output_activation='softmax', SGD_momentum=0) \n",
        "  \n",
        "    train_gen = tf.keras.preprocessing.image.ImageDataGenerator(width_shift_range=0.1,\n",
        "      height_shift_range=0.1,\n",
        "      horizontal_flip=True)\n",
        "    train_gen.fit(trainX[train])\n",
        "  \n",
        "    print('------------------------------------------------------------------------')\n",
        "    print(f'Training for fold {fold_no} ...')\n",
        "\n",
        "    history = model.train(\n",
        "              train_gen,\n",
        "                trainX[train],\n",
        "                  trainy[train],\n",
        "                trainX[valid],\n",
        "                  trainy[valid],\n",
        "                  lr_rate_type='step')\n",
        "\n",
        "    scores = model.evaluate(trainX[valid], trainy[valid])\n",
        "\n",
        "    print(f'Score for fold {fold_no}: loss of {scores[0]}; accuracy of {scores[1]*100}%')\n",
        "    acc_per_fold.append(scores[1] * 100)\n",
        "    loss_per_fold.append(scores[0])\n",
        "\n",
        "    fold_no+=1\n",
        "\n",
        "# == Provide average scores ==\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Score per fold')\n",
        "for i in range(0, len(acc_per_fold)):\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Average scores for all folds:')\n",
        "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
        "print(f'> Loss: {np.mean(loss_per_fold)}')"
      ],
      "metadata": {
        "id": "iesSwsDC7bsn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "396612b7-660c-4e71-cc21-ee31264a6ca9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 32, 32, 64)        1792      \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  multiple                 0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 16, 16, 128)       73856     \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 8, 8, 128)         0         \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 8, 8, 256)         295168    \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 2, 2, 256)         590080    \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 256)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 10)                2570      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 963,466\n",
            "Trainable params: 963,466\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/30\n",
            "1250/1250 [==============================] - 23s 14ms/step - loss: 1.6002 - accuracy: 0.4141 - val_loss: 1.2159 - val_accuracy: 0.5635 - lr: 0.0010\n",
            "Epoch 2/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 1.2037 - accuracy: 0.5724 - val_loss: 1.0228 - val_accuracy: 0.6384 - lr: 0.0010\n",
            "Epoch 3/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.0473 - accuracy: 0.6288 - val_loss: 0.9043 - val_accuracy: 0.6845 - lr: 0.0010\n",
            "Epoch 4/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.9464 - accuracy: 0.6686 - val_loss: 0.8120 - val_accuracy: 0.7123 - lr: 0.0010\n",
            "Epoch 5/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.8735 - accuracy: 0.6938 - val_loss: 0.9478 - val_accuracy: 0.6866 - lr: 0.0010\n",
            "Epoch 6/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.8329 - accuracy: 0.7084 - val_loss: 0.7801 - val_accuracy: 0.7291 - lr: 0.0010\n",
            "Epoch 7/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.7913 - accuracy: 0.7229 - val_loss: 0.8177 - val_accuracy: 0.7087 - lr: 0.0010\n",
            "Epoch 8/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.7560 - accuracy: 0.7370 - val_loss: 0.7254 - val_accuracy: 0.7458 - lr: 0.0010\n",
            "Epoch 9/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.7399 - accuracy: 0.7406 - val_loss: 0.6684 - val_accuracy: 0.7650 - lr: 0.0010\n",
            "Epoch 10/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.7052 - accuracy: 0.7530 - val_loss: 0.6601 - val_accuracy: 0.7712 - lr: 0.0010\n",
            "Epoch 11/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.6241 - accuracy: 0.7814 - val_loss: 0.6192 - val_accuracy: 0.7870 - lr: 5.0000e-04\n",
            "Epoch 12/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.6104 - accuracy: 0.7872 - val_loss: 0.6105 - val_accuracy: 0.7890 - lr: 5.0000e-04\n",
            "Epoch 13/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.5853 - accuracy: 0.7956 - val_loss: 0.6158 - val_accuracy: 0.7889 - lr: 5.0000e-04\n",
            "Epoch 14/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.5767 - accuracy: 0.7973 - val_loss: 0.5946 - val_accuracy: 0.7963 - lr: 5.0000e-04\n",
            "Epoch 15/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.5628 - accuracy: 0.8022 - val_loss: 0.6113 - val_accuracy: 0.7901 - lr: 5.0000e-04\n",
            "Epoch 16/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.5524 - accuracy: 0.8051 - val_loss: 0.5648 - val_accuracy: 0.8060 - lr: 5.0000e-04\n",
            "Epoch 17/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.5445 - accuracy: 0.8102 - val_loss: 0.6028 - val_accuracy: 0.7927 - lr: 5.0000e-04\n",
            "Epoch 18/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.5293 - accuracy: 0.8145 - val_loss: 0.6045 - val_accuracy: 0.7964 - lr: 5.0000e-04\n",
            "Epoch 19/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.5216 - accuracy: 0.8165 - val_loss: 0.5710 - val_accuracy: 0.8053 - lr: 5.0000e-04\n",
            "Epoch 20/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.5172 - accuracy: 0.8180 - val_loss: 0.5980 - val_accuracy: 0.7998 - lr: 5.0000e-04\n",
            "Epoch 21/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.4552 - accuracy: 0.8415 - val_loss: 0.5533 - val_accuracy: 0.8125 - lr: 1.2500e-04\n",
            "Epoch 22/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.4441 - accuracy: 0.8433 - val_loss: 0.5510 - val_accuracy: 0.8137 - lr: 1.2500e-04\n",
            "Epoch 23/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.4387 - accuracy: 0.8466 - val_loss: 0.5352 - val_accuracy: 0.8142 - lr: 1.2500e-04\n",
            "Epoch 24/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.4320 - accuracy: 0.8495 - val_loss: 0.5427 - val_accuracy: 0.8138 - lr: 1.2500e-04\n",
            "Epoch 25/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.4281 - accuracy: 0.8490 - val_loss: 0.5395 - val_accuracy: 0.8195 - lr: 1.2500e-04\n",
            "Epoch 26/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.4254 - accuracy: 0.8511 - val_loss: 0.5406 - val_accuracy: 0.8160 - lr: 1.2500e-04\n",
            "Epoch 27/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.4202 - accuracy: 0.8528 - val_loss: 0.5419 - val_accuracy: 0.8162 - lr: 1.2500e-04\n",
            "Epoch 28/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.4114 - accuracy: 0.8557 - val_loss: 0.5237 - val_accuracy: 0.8205 - lr: 1.2500e-04\n",
            "Epoch 29/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.4150 - accuracy: 0.8538 - val_loss: 0.5260 - val_accuracy: 0.8241 - lr: 1.2500e-04\n",
            "Epoch 30/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.4104 - accuracy: 0.8557 - val_loss: 0.5459 - val_accuracy: 0.8169 - lr: 1.2500e-04\n",
            "Score for fold 1: loss of 0.5458500981330872; accuracy of 81.69000148773193%\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_4 (Conv2D)           (None, 32, 32, 64)        1792      \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  multiple                 0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_5 (Conv2D)           (None, 16, 16, 128)       73856     \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 8, 8, 128)         0         \n",
            "                                                                 \n",
            " conv2d_6 (Conv2D)           (None, 8, 8, 256)         295168    \n",
            "                                                                 \n",
            " conv2d_7 (Conv2D)           (None, 2, 2, 256)         590080    \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 256)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 10)                2570      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 963,466\n",
            "Trainable params: 963,466\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.6029 - accuracy: 0.4116 - val_loss: 1.2860 - val_accuracy: 0.5448 - lr: 0.0010\n",
            "Epoch 2/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.2006 - accuracy: 0.5752 - val_loss: 1.0126 - val_accuracy: 0.6439 - lr: 0.0010\n",
            "Epoch 3/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.0238 - accuracy: 0.6386 - val_loss: 0.9132 - val_accuracy: 0.6841 - lr: 0.0010\n",
            "Epoch 4/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.9264 - accuracy: 0.6745 - val_loss: 0.8550 - val_accuracy: 0.6981 - lr: 0.0010\n",
            "Epoch 5/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.8526 - accuracy: 0.7020 - val_loss: 0.7474 - val_accuracy: 0.7377 - lr: 0.0010\n",
            "Epoch 6/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.7989 - accuracy: 0.7197 - val_loss: 0.7426 - val_accuracy: 0.7438 - lr: 0.0010\n",
            "Epoch 7/30\n",
            "1250/1250 [==============================] - 20s 16ms/step - loss: 0.7527 - accuracy: 0.7365 - val_loss: 0.6975 - val_accuracy: 0.7572 - lr: 0.0010\n",
            "Epoch 8/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.7236 - accuracy: 0.7480 - val_loss: 0.6864 - val_accuracy: 0.7650 - lr: 0.0010\n",
            "Epoch 9/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.6936 - accuracy: 0.7576 - val_loss: 0.7124 - val_accuracy: 0.7595 - lr: 0.0010\n",
            "Epoch 10/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.6732 - accuracy: 0.7648 - val_loss: 0.6699 - val_accuracy: 0.7749 - lr: 0.0010\n",
            "Epoch 11/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.5874 - accuracy: 0.7946 - val_loss: 0.5900 - val_accuracy: 0.8018 - lr: 5.0000e-04\n",
            "Epoch 12/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.5612 - accuracy: 0.8043 - val_loss: 0.5826 - val_accuracy: 0.7973 - lr: 5.0000e-04\n",
            "Epoch 13/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.5435 - accuracy: 0.8090 - val_loss: 0.5856 - val_accuracy: 0.8034 - lr: 5.0000e-04\n",
            "Epoch 14/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.5319 - accuracy: 0.8148 - val_loss: 0.6139 - val_accuracy: 0.7984 - lr: 5.0000e-04\n",
            "Epoch 15/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.5216 - accuracy: 0.8190 - val_loss: 0.5675 - val_accuracy: 0.8039 - lr: 5.0000e-04\n",
            "Epoch 16/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.5127 - accuracy: 0.8201 - val_loss: 0.5618 - val_accuracy: 0.8120 - lr: 5.0000e-04\n",
            "Epoch 17/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.5000 - accuracy: 0.8254 - val_loss: 0.5796 - val_accuracy: 0.8081 - lr: 5.0000e-04\n",
            "Epoch 18/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.4895 - accuracy: 0.8284 - val_loss: 0.5602 - val_accuracy: 0.8131 - lr: 5.0000e-04\n",
            "Epoch 19/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.4819 - accuracy: 0.8301 - val_loss: 0.5791 - val_accuracy: 0.8120 - lr: 5.0000e-04\n",
            "Epoch 20/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.4742 - accuracy: 0.8352 - val_loss: 0.5654 - val_accuracy: 0.8119 - lr: 5.0000e-04\n",
            "Epoch 21/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.4092 - accuracy: 0.8572 - val_loss: 0.5332 - val_accuracy: 0.8231 - lr: 1.2500e-04\n",
            "Epoch 22/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.4026 - accuracy: 0.8597 - val_loss: 0.5380 - val_accuracy: 0.8229 - lr: 1.2500e-04\n",
            "Epoch 23/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.3949 - accuracy: 0.8601 - val_loss: 0.5447 - val_accuracy: 0.8218 - lr: 1.2500e-04\n",
            "Epoch 24/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.3895 - accuracy: 0.8620 - val_loss: 0.5238 - val_accuracy: 0.8283 - lr: 1.2500e-04\n",
            "Epoch 25/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.3842 - accuracy: 0.8654 - val_loss: 0.5365 - val_accuracy: 0.8257 - lr: 1.2500e-04\n",
            "Epoch 26/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.3838 - accuracy: 0.8648 - val_loss: 0.5567 - val_accuracy: 0.8238 - lr: 1.2500e-04\n",
            "Epoch 27/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.3777 - accuracy: 0.8674 - val_loss: 0.5198 - val_accuracy: 0.8302 - lr: 1.2500e-04\n",
            "Epoch 28/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 0.3707 - accuracy: 0.8702 - val_loss: 0.5329 - val_accuracy: 0.8282 - lr: 1.2500e-04\n",
            "Epoch 29/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.3715 - accuracy: 0.8687 - val_loss: 0.5331 - val_accuracy: 0.8290 - lr: 1.2500e-04\n",
            "Epoch 30/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.3657 - accuracy: 0.8723 - val_loss: 0.5213 - val_accuracy: 0.8341 - lr: 1.2500e-04\n",
            "Score for fold 2: loss of 0.5212924480438232; accuracy of 83.41000080108643%\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_8 (Conv2D)           (None, 32, 32, 64)        1792      \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPooling  multiple                 0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_9 (Conv2D)           (None, 16, 16, 128)       73856     \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 8, 8, 128)         0         \n",
            "                                                                 \n",
            " conv2d_10 (Conv2D)          (None, 8, 8, 256)         295168    \n",
            "                                                                 \n",
            " conv2d_11 (Conv2D)          (None, 2, 2, 256)         590080    \n",
            "                                                                 \n",
            " flatten_2 (Flatten)         (None, 256)               0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 10)                2570      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 963,466\n",
            "Trainable params: 963,466\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/30\n",
            "1250/1250 [==============================] - 20s 16ms/step - loss: 1.5644 - accuracy: 0.4281 - val_loss: 1.2728 - val_accuracy: 0.5509 - lr: 0.0010\n",
            "Epoch 2/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.1737 - accuracy: 0.5805 - val_loss: 1.0193 - val_accuracy: 0.6438 - lr: 0.0010\n",
            "Epoch 3/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 1.0049 - accuracy: 0.6451 - val_loss: 0.9861 - val_accuracy: 0.6654 - lr: 0.0010\n",
            "Epoch 4/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.9032 - accuracy: 0.6820 - val_loss: 0.8224 - val_accuracy: 0.7087 - lr: 0.0010\n",
            "Epoch 5/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 0.8288 - accuracy: 0.7096 - val_loss: 0.7932 - val_accuracy: 0.7274 - lr: 0.0010\n",
            "Epoch 6/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.7841 - accuracy: 0.7254 - val_loss: 0.7837 - val_accuracy: 0.7316 - lr: 0.0010\n",
            "Epoch 7/30\n",
            "1250/1250 [==============================] - 20s 16ms/step - loss: 0.7442 - accuracy: 0.7384 - val_loss: 0.7344 - val_accuracy: 0.7531 - lr: 0.0010\n",
            "Epoch 8/30\n",
            "1250/1250 [==============================] - 20s 16ms/step - loss: 0.7100 - accuracy: 0.7541 - val_loss: 0.7052 - val_accuracy: 0.7606 - lr: 0.0010\n",
            "Epoch 9/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.6812 - accuracy: 0.7631 - val_loss: 0.6978 - val_accuracy: 0.7664 - lr: 0.0010\n",
            "Epoch 10/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.6571 - accuracy: 0.7709 - val_loss: 0.6853 - val_accuracy: 0.7665 - lr: 0.0010\n",
            "Epoch 11/30\n",
            "1250/1250 [==============================] - 20s 16ms/step - loss: 0.5768 - accuracy: 0.7976 - val_loss: 0.6207 - val_accuracy: 0.7893 - lr: 5.0000e-04\n",
            "Epoch 12/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.5459 - accuracy: 0.8099 - val_loss: 0.6279 - val_accuracy: 0.7930 - lr: 5.0000e-04\n",
            "Epoch 13/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.5321 - accuracy: 0.8113 - val_loss: 0.6088 - val_accuracy: 0.8027 - lr: 5.0000e-04\n",
            "Epoch 14/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.5178 - accuracy: 0.8172 - val_loss: 0.6223 - val_accuracy: 0.7953 - lr: 5.0000e-04\n",
            "Epoch 15/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.5060 - accuracy: 0.8217 - val_loss: 0.6191 - val_accuracy: 0.8002 - lr: 5.0000e-04\n",
            "Epoch 16/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.4948 - accuracy: 0.8263 - val_loss: 0.6244 - val_accuracy: 0.7974 - lr: 5.0000e-04\n",
            "Epoch 17/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.4901 - accuracy: 0.8289 - val_loss: 0.5932 - val_accuracy: 0.8078 - lr: 5.0000e-04\n",
            "Epoch 18/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.4768 - accuracy: 0.8323 - val_loss: 0.6225 - val_accuracy: 0.7983 - lr: 5.0000e-04\n",
            "Epoch 19/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.4663 - accuracy: 0.8360 - val_loss: 0.5916 - val_accuracy: 0.8099 - lr: 5.0000e-04\n",
            "Epoch 20/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.4568 - accuracy: 0.8385 - val_loss: 0.5961 - val_accuracy: 0.8081 - lr: 5.0000e-04\n",
            "Epoch 21/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.3961 - accuracy: 0.8611 - val_loss: 0.5660 - val_accuracy: 0.8196 - lr: 1.2500e-04\n",
            "Epoch 22/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.3845 - accuracy: 0.8647 - val_loss: 0.5625 - val_accuracy: 0.8225 - lr: 1.2500e-04\n",
            "Epoch 23/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.3756 - accuracy: 0.8680 - val_loss: 0.5780 - val_accuracy: 0.8166 - lr: 1.2500e-04\n",
            "Epoch 24/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.3725 - accuracy: 0.8688 - val_loss: 0.5771 - val_accuracy: 0.8177 - lr: 1.2500e-04\n",
            "Epoch 25/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.3671 - accuracy: 0.8724 - val_loss: 0.5757 - val_accuracy: 0.8195 - lr: 1.2500e-04\n",
            "Epoch 26/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.3640 - accuracy: 0.8726 - val_loss: 0.5794 - val_accuracy: 0.8179 - lr: 1.2500e-04\n",
            "Epoch 27/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.3573 - accuracy: 0.8725 - val_loss: 0.5850 - val_accuracy: 0.8184 - lr: 1.2500e-04\n",
            "Epoch 28/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.3564 - accuracy: 0.8746 - val_loss: 0.5619 - val_accuracy: 0.8247 - lr: 1.2500e-04\n",
            "Epoch 29/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.3522 - accuracy: 0.8749 - val_loss: 0.5729 - val_accuracy: 0.8212 - lr: 1.2500e-04\n",
            "Epoch 30/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.3443 - accuracy: 0.8771 - val_loss: 0.5735 - val_accuracy: 0.8237 - lr: 1.2500e-04\n",
            "Score for fold 3: loss of 0.5734756588935852; accuracy of 82.37000107765198%\n",
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_12 (Conv2D)          (None, 32, 32, 64)        1792      \n",
            "                                                                 \n",
            " max_pooling2d_3 (MaxPooling  multiple                 0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_13 (Conv2D)          (None, 16, 16, 128)       73856     \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 8, 8, 128)         0         \n",
            "                                                                 \n",
            " conv2d_14 (Conv2D)          (None, 8, 8, 256)         295168    \n",
            "                                                                 \n",
            " conv2d_15 (Conv2D)          (None, 2, 2, 256)         590080    \n",
            "                                                                 \n",
            " flatten_3 (Flatten)         (None, 256)               0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 10)                2570      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 963,466\n",
            "Trainable params: 963,466\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.5585 - accuracy: 0.4319 - val_loss: 1.2840 - val_accuracy: 0.5542 - lr: 0.0010\n",
            "Epoch 2/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.1361 - accuracy: 0.5983 - val_loss: 0.9597 - val_accuracy: 0.6623 - lr: 0.0010\n",
            "Epoch 3/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.9692 - accuracy: 0.6585 - val_loss: 0.8669 - val_accuracy: 0.7025 - lr: 0.0010\n",
            "Epoch 4/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.8756 - accuracy: 0.6932 - val_loss: 0.8626 - val_accuracy: 0.7023 - lr: 0.0010\n",
            "Epoch 5/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.8089 - accuracy: 0.7190 - val_loss: 0.7868 - val_accuracy: 0.7296 - lr: 0.0010\n",
            "Epoch 6/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.7677 - accuracy: 0.7325 - val_loss: 0.7470 - val_accuracy: 0.7424 - lr: 0.0010\n",
            "Epoch 7/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.7272 - accuracy: 0.7450 - val_loss: 0.6992 - val_accuracy: 0.7562 - lr: 0.0010\n",
            "Epoch 8/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.6983 - accuracy: 0.7559 - val_loss: 0.6983 - val_accuracy: 0.7644 - lr: 0.0010\n",
            "Epoch 9/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.6713 - accuracy: 0.7657 - val_loss: 0.6835 - val_accuracy: 0.7658 - lr: 0.0010\n",
            "Epoch 10/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.6431 - accuracy: 0.7743 - val_loss: 0.7116 - val_accuracy: 0.7553 - lr: 0.0010\n",
            "Epoch 11/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.5499 - accuracy: 0.8072 - val_loss: 0.6036 - val_accuracy: 0.7946 - lr: 5.0000e-04\n",
            "Epoch 12/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.5342 - accuracy: 0.8114 - val_loss: 0.6133 - val_accuracy: 0.7930 - lr: 5.0000e-04\n",
            "Epoch 13/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.5143 - accuracy: 0.8186 - val_loss: 0.6006 - val_accuracy: 0.8010 - lr: 5.0000e-04\n",
            "Epoch 14/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.4991 - accuracy: 0.8258 - val_loss: 0.6164 - val_accuracy: 0.7938 - lr: 5.0000e-04\n",
            "Epoch 15/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.4888 - accuracy: 0.8289 - val_loss: 0.6020 - val_accuracy: 0.7964 - lr: 5.0000e-04\n",
            "Epoch 16/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.4746 - accuracy: 0.8327 - val_loss: 0.5732 - val_accuracy: 0.8106 - lr: 5.0000e-04\n",
            "Epoch 17/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.4663 - accuracy: 0.8355 - val_loss: 0.6129 - val_accuracy: 0.7957 - lr: 5.0000e-04\n",
            "Epoch 18/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.4542 - accuracy: 0.8394 - val_loss: 0.5760 - val_accuracy: 0.8093 - lr: 5.0000e-04\n",
            "Epoch 19/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.4447 - accuracy: 0.8408 - val_loss: 0.6111 - val_accuracy: 0.8014 - lr: 5.0000e-04\n",
            "Epoch 20/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.4355 - accuracy: 0.8456 - val_loss: 0.5513 - val_accuracy: 0.8164 - lr: 5.0000e-04\n",
            "Epoch 21/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.3784 - accuracy: 0.8663 - val_loss: 0.5354 - val_accuracy: 0.8272 - lr: 1.2500e-04\n",
            "Epoch 22/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.3577 - accuracy: 0.8746 - val_loss: 0.5305 - val_accuracy: 0.8262 - lr: 1.2500e-04\n",
            "Epoch 23/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.3517 - accuracy: 0.8766 - val_loss: 0.5362 - val_accuracy: 0.8255 - lr: 1.2500e-04\n",
            "Epoch 24/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.3488 - accuracy: 0.8777 - val_loss: 0.5546 - val_accuracy: 0.8201 - lr: 1.2500e-04\n",
            "Epoch 25/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.3440 - accuracy: 0.8805 - val_loss: 0.5454 - val_accuracy: 0.8240 - lr: 1.2500e-04\n",
            "Epoch 26/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.3393 - accuracy: 0.8805 - val_loss: 0.5589 - val_accuracy: 0.8214 - lr: 1.2500e-04\n",
            "Epoch 27/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.3368 - accuracy: 0.8824 - val_loss: 0.5354 - val_accuracy: 0.8305 - lr: 1.2500e-04\n",
            "Epoch 28/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.3304 - accuracy: 0.8839 - val_loss: 0.5452 - val_accuracy: 0.8276 - lr: 1.2500e-04\n",
            "Epoch 29/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.3266 - accuracy: 0.8850 - val_loss: 0.5733 - val_accuracy: 0.8192 - lr: 1.2500e-04\n",
            "Epoch 30/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.3240 - accuracy: 0.8840 - val_loss: 0.5370 - val_accuracy: 0.8287 - lr: 1.2500e-04\n",
            "Score for fold 4: loss of 0.5369959473609924; accuracy of 82.87000060081482%\n",
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_16 (Conv2D)          (None, 32, 32, 64)        1792      \n",
            "                                                                 \n",
            " max_pooling2d_4 (MaxPooling  multiple                 0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_17 (Conv2D)          (None, 16, 16, 128)       73856     \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 8, 8, 128)         0         \n",
            "                                                                 \n",
            " conv2d_18 (Conv2D)          (None, 8, 8, 256)         295168    \n",
            "                                                                 \n",
            " conv2d_19 (Conv2D)          (None, 2, 2, 256)         590080    \n",
            "                                                                 \n",
            " flatten_4 (Flatten)         (None, 256)               0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 10)                2570      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 963,466\n",
            "Trainable params: 963,466\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.5770 - accuracy: 0.4225 - val_loss: 1.2426 - val_accuracy: 0.5509 - lr: 0.0010\n",
            "Epoch 2/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 1.1650 - accuracy: 0.5836 - val_loss: 0.9919 - val_accuracy: 0.6444 - lr: 0.0010\n",
            "Epoch 3/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 1.0082 - accuracy: 0.6459 - val_loss: 0.9112 - val_accuracy: 0.6770 - lr: 0.0010\n",
            "Epoch 4/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.9041 - accuracy: 0.6826 - val_loss: 0.8345 - val_accuracy: 0.7055 - lr: 0.0010\n",
            "Epoch 5/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.8333 - accuracy: 0.7060 - val_loss: 0.7706 - val_accuracy: 0.7314 - lr: 0.0010\n",
            "Epoch 6/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.7806 - accuracy: 0.7253 - val_loss: 0.6972 - val_accuracy: 0.7625 - lr: 0.0010\n",
            "Epoch 7/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.7417 - accuracy: 0.7410 - val_loss: 0.6818 - val_accuracy: 0.7626 - lr: 0.0010\n",
            "Epoch 8/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.7088 - accuracy: 0.7534 - val_loss: 0.6724 - val_accuracy: 0.7688 - lr: 0.0010\n",
            "Epoch 9/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.6763 - accuracy: 0.7648 - val_loss: 0.7136 - val_accuracy: 0.7564 - lr: 0.0010\n",
            "Epoch 10/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.6589 - accuracy: 0.7700 - val_loss: 0.6234 - val_accuracy: 0.7894 - lr: 0.0010\n",
            "Epoch 11/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.5611 - accuracy: 0.8024 - val_loss: 0.6106 - val_accuracy: 0.7946 - lr: 5.0000e-04\n",
            "Epoch 12/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.5478 - accuracy: 0.8073 - val_loss: 0.5787 - val_accuracy: 0.8025 - lr: 5.0000e-04\n",
            "Epoch 13/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.5313 - accuracy: 0.8161 - val_loss: 0.5500 - val_accuracy: 0.8106 - lr: 5.0000e-04\n",
            "Epoch 14/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.5164 - accuracy: 0.8196 - val_loss: 0.5778 - val_accuracy: 0.8085 - lr: 5.0000e-04\n",
            "Epoch 15/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.5045 - accuracy: 0.8226 - val_loss: 0.5609 - val_accuracy: 0.8130 - lr: 5.0000e-04\n",
            "Epoch 16/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.4880 - accuracy: 0.8293 - val_loss: 0.5916 - val_accuracy: 0.8032 - lr: 5.0000e-04\n",
            "Epoch 17/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.4746 - accuracy: 0.8339 - val_loss: 0.5878 - val_accuracy: 0.8071 - lr: 5.0000e-04\n",
            "Epoch 18/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.4714 - accuracy: 0.8339 - val_loss: 0.6235 - val_accuracy: 0.7996 - lr: 5.0000e-04\n",
            "Epoch 19/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.4623 - accuracy: 0.8398 - val_loss: 0.5797 - val_accuracy: 0.8159 - lr: 5.0000e-04\n",
            "Epoch 20/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.4489 - accuracy: 0.8413 - val_loss: 0.5721 - val_accuracy: 0.8137 - lr: 5.0000e-04\n",
            "Epoch 21/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.3894 - accuracy: 0.8650 - val_loss: 0.5496 - val_accuracy: 0.8233 - lr: 1.2500e-04\n",
            "Epoch 22/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.3812 - accuracy: 0.8678 - val_loss: 0.5237 - val_accuracy: 0.8285 - lr: 1.2500e-04\n",
            "Epoch 23/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.3782 - accuracy: 0.8676 - val_loss: 0.5455 - val_accuracy: 0.8231 - lr: 1.2500e-04\n",
            "Epoch 24/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.3706 - accuracy: 0.8697 - val_loss: 0.5530 - val_accuracy: 0.8212 - lr: 1.2500e-04\n",
            "Epoch 25/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.3647 - accuracy: 0.8705 - val_loss: 0.5376 - val_accuracy: 0.8317 - lr: 1.2500e-04\n",
            "Epoch 26/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.3593 - accuracy: 0.8730 - val_loss: 0.5298 - val_accuracy: 0.8316 - lr: 1.2500e-04\n",
            "Epoch 27/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.3546 - accuracy: 0.8746 - val_loss: 0.5421 - val_accuracy: 0.8304 - lr: 1.2500e-04\n",
            "Epoch 28/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.3474 - accuracy: 0.8777 - val_loss: 0.5425 - val_accuracy: 0.8284 - lr: 1.2500e-04\n",
            "Epoch 29/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.3498 - accuracy: 0.8750 - val_loss: 0.5424 - val_accuracy: 0.8281 - lr: 1.2500e-04\n",
            "Epoch 30/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.3437 - accuracy: 0.8791 - val_loss: 0.5369 - val_accuracy: 0.8293 - lr: 1.2500e-04\n",
            "Score for fold 5: loss of 0.5368508100509644; accuracy of 82.92999863624573%\n",
            "------------------------------------------------------------------------\n",
            "Score per fold\n",
            "------------------------------------------------------------------------\n",
            "> Fold 1 - Loss: 0.5458500981330872 - Accuracy: 81.69000148773193%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 2 - Loss: 0.5212924480438232 - Accuracy: 83.41000080108643%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 3 - Loss: 0.5734756588935852 - Accuracy: 82.37000107765198%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 4 - Loss: 0.5369959473609924 - Accuracy: 82.87000060081482%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 5 - Loss: 0.5368508100509644 - Accuracy: 82.92999863624573%\n",
            "------------------------------------------------------------------------\n",
            "Average scores for all folds:\n",
            "> Accuracy: 82.65400052070618 (+- 0.5838351201415876)\n",
            "> Loss: 0.5428929924964905\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 4 hidden layers and Middle Structure"
      ],
      "metadata": {
        "id": "eddrp84z7ojv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "acc_per_fold = []\n",
        "loss_per_fold = []\n",
        "\n",
        "with tf.device('/device:GPU:0'):\n",
        "  fold_no = 1\n",
        "  for train, valid in kfold.split(trainX, trainy):\n",
        "\n",
        "    model = CNN_Model(batch_size=32, depth=4, hidden_neurons_1=32, hidden_neurons_2=64, hidden_neurons_3=128, kernel_size_1=3, kernel_size_2=3, kernel_size_3=3, \n",
        "               dropout_rate=0.2, batch_norm=False, pooling='max', layer_activation_1='relu', layer_activation_2='relu', layer_activation_3='relu',\n",
        "               kernel_initialiser='glorot_uniform', bias_initialiser='zeros', optimiser='Adam', weight_decay=None, learning_rate=1e-3, output_activation='softmax', SGD_momentum=0) \n",
        "  \n",
        "    train_gen = tf.keras.preprocessing.image.ImageDataGenerator(width_shift_range=0.1,\n",
        "      height_shift_range=0.1,\n",
        "      horizontal_flip=True)\n",
        "    train_gen.fit(trainX[train])\n",
        "  \n",
        "    print('------------------------------------------------------------------------')\n",
        "    print(f'Training for fold {fold_no} ...')\n",
        "\n",
        "    history = model.train(\n",
        "              train_gen,\n",
        "                trainX[train],\n",
        "                  trainy[train],\n",
        "                trainX[valid],\n",
        "                  trainy[valid],\n",
        "                  lr_rate_type='step')\n",
        "\n",
        "    scores = model.evaluate(trainX[valid], trainy[valid])\n",
        "\n",
        "    print(f'Score for fold {fold_no}: loss of {scores[0]}; accuracy of {scores[1]*100}%')\n",
        "    acc_per_fold.append(scores[1] * 100)\n",
        "    loss_per_fold.append(scores[0])\n",
        "\n",
        "    fold_no+=1\n",
        "\n",
        "# == Provide average scores ==\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Score per fold')\n",
        "for i in range(0, len(acc_per_fold)):\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Average scores for all folds:')\n",
        "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
        "print(f'> Loss: {np.mean(loss_per_fold)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ELYn-eds7sY-",
        "outputId": "12277bd8-4387-4360-8282-6f610b3c3e04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_20 (Conv2D)          (None, 32, 32, 32)        896       \n",
            "                                                                 \n",
            " max_pooling2d_5 (MaxPooling  multiple                 0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_21 (Conv2D)          (None, 16, 16, 64)        18496     \n",
            "                                                                 \n",
            " dropout_5 (Dropout)         (None, 8, 8, 64)          0         \n",
            "                                                                 \n",
            " conv2d_22 (Conv2D)          (None, 8, 8, 128)         73856     \n",
            "                                                                 \n",
            " conv2d_23 (Conv2D)          (None, 2, 2, 256)         295168    \n",
            "                                                                 \n",
            " flatten_5 (Flatten)         (None, 256)               0         \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 10)                2570      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 390,986\n",
            "Trainable params: 390,986\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.6202 - accuracy: 0.4034 - val_loss: 1.2712 - val_accuracy: 0.5299 - lr: 0.0010\n",
            "Epoch 2/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.2332 - accuracy: 0.5587 - val_loss: 1.1016 - val_accuracy: 0.6101 - lr: 0.0010\n",
            "Epoch 3/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.0706 - accuracy: 0.6174 - val_loss: 0.9821 - val_accuracy: 0.6585 - lr: 0.0010\n",
            "Epoch 4/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.9727 - accuracy: 0.6567 - val_loss: 0.8544 - val_accuracy: 0.7021 - lr: 0.0010\n",
            "Epoch 5/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.9025 - accuracy: 0.6824 - val_loss: 0.8189 - val_accuracy: 0.7155 - lr: 0.0010\n",
            "Epoch 6/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.8480 - accuracy: 0.7007 - val_loss: 0.8231 - val_accuracy: 0.7140 - lr: 0.0010\n",
            "Epoch 7/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.8118 - accuracy: 0.7182 - val_loss: 0.7673 - val_accuracy: 0.7325 - lr: 0.0010\n",
            "Epoch 8/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.7785 - accuracy: 0.7258 - val_loss: 0.6774 - val_accuracy: 0.7631 - lr: 0.0010\n",
            "Epoch 9/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.7568 - accuracy: 0.7373 - val_loss: 0.7565 - val_accuracy: 0.7448 - lr: 0.0010\n",
            "Epoch 10/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.7273 - accuracy: 0.7465 - val_loss: 0.7390 - val_accuracy: 0.7468 - lr: 0.0010\n",
            "Epoch 11/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.6518 - accuracy: 0.7739 - val_loss: 0.6523 - val_accuracy: 0.7789 - lr: 5.0000e-04\n",
            "Epoch 12/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.6306 - accuracy: 0.7820 - val_loss: 0.6192 - val_accuracy: 0.7861 - lr: 5.0000e-04\n",
            "Epoch 13/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.6190 - accuracy: 0.7843 - val_loss: 0.6354 - val_accuracy: 0.7854 - lr: 5.0000e-04\n",
            "Epoch 14/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.6072 - accuracy: 0.7885 - val_loss: 0.5935 - val_accuracy: 0.7933 - lr: 5.0000e-04\n",
            "Epoch 15/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.5924 - accuracy: 0.7911 - val_loss: 0.5894 - val_accuracy: 0.7985 - lr: 5.0000e-04\n",
            "Epoch 16/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.5807 - accuracy: 0.7952 - val_loss: 0.6178 - val_accuracy: 0.7909 - lr: 5.0000e-04\n",
            "Epoch 17/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.5709 - accuracy: 0.8001 - val_loss: 0.6010 - val_accuracy: 0.7980 - lr: 5.0000e-04\n",
            "Epoch 18/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.5597 - accuracy: 0.8043 - val_loss: 0.5926 - val_accuracy: 0.7972 - lr: 5.0000e-04\n",
            "Epoch 19/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.5515 - accuracy: 0.8080 - val_loss: 0.6229 - val_accuracy: 0.7927 - lr: 5.0000e-04\n",
            "Epoch 20/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.5368 - accuracy: 0.8104 - val_loss: 0.6296 - val_accuracy: 0.7875 - lr: 5.0000e-04\n",
            "Epoch 21/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.4873 - accuracy: 0.8294 - val_loss: 0.5621 - val_accuracy: 0.8091 - lr: 1.2500e-04\n",
            "Epoch 22/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.4798 - accuracy: 0.8307 - val_loss: 0.5563 - val_accuracy: 0.8106 - lr: 1.2500e-04\n",
            "Epoch 23/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.4731 - accuracy: 0.8337 - val_loss: 0.5491 - val_accuracy: 0.8166 - lr: 1.2500e-04\n",
            "Epoch 24/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.4695 - accuracy: 0.8340 - val_loss: 0.5395 - val_accuracy: 0.8201 - lr: 1.2500e-04\n",
            "Epoch 25/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.4606 - accuracy: 0.8383 - val_loss: 0.5426 - val_accuracy: 0.8175 - lr: 1.2500e-04\n",
            "Epoch 26/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.4565 - accuracy: 0.8387 - val_loss: 0.5532 - val_accuracy: 0.8131 - lr: 1.2500e-04\n",
            "Epoch 27/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.4584 - accuracy: 0.8399 - val_loss: 0.5497 - val_accuracy: 0.8143 - lr: 1.2500e-04\n",
            "Epoch 28/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.4542 - accuracy: 0.8406 - val_loss: 0.5431 - val_accuracy: 0.8168 - lr: 1.2500e-04\n",
            "Epoch 29/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.4552 - accuracy: 0.8404 - val_loss: 0.5395 - val_accuracy: 0.8176 - lr: 1.2500e-04\n",
            "Epoch 30/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.4500 - accuracy: 0.8395 - val_loss: 0.5445 - val_accuracy: 0.8164 - lr: 1.2500e-04\n",
            "Score for fold 1: loss of 0.5444541573524475; accuracy of 81.63999915122986%\n",
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_24 (Conv2D)          (None, 32, 32, 32)        896       \n",
            "                                                                 \n",
            " max_pooling2d_6 (MaxPooling  multiple                 0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_25 (Conv2D)          (None, 16, 16, 64)        18496     \n",
            "                                                                 \n",
            " dropout_6 (Dropout)         (None, 8, 8, 64)          0         \n",
            "                                                                 \n",
            " conv2d_26 (Conv2D)          (None, 8, 8, 128)         73856     \n",
            "                                                                 \n",
            " conv2d_27 (Conv2D)          (None, 2, 2, 256)         295168    \n",
            "                                                                 \n",
            " flatten_6 (Flatten)         (None, 256)               0         \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 10)                2570      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 390,986\n",
            "Trainable params: 390,986\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.6148 - accuracy: 0.4057 - val_loss: 1.2846 - val_accuracy: 0.5393 - lr: 0.0010\n",
            "Epoch 2/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.2165 - accuracy: 0.5647 - val_loss: 1.0619 - val_accuracy: 0.6304 - lr: 0.0010\n",
            "Epoch 3/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.0654 - accuracy: 0.6224 - val_loss: 0.9506 - val_accuracy: 0.6663 - lr: 0.0010\n",
            "Epoch 4/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.9601 - accuracy: 0.6625 - val_loss: 0.8531 - val_accuracy: 0.7045 - lr: 0.0010\n",
            "Epoch 5/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.8969 - accuracy: 0.6816 - val_loss: 0.9070 - val_accuracy: 0.6886 - lr: 0.0010\n",
            "Epoch 6/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.8530 - accuracy: 0.7013 - val_loss: 0.8106 - val_accuracy: 0.7184 - lr: 0.0010\n",
            "Epoch 7/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.8022 - accuracy: 0.7177 - val_loss: 0.7405 - val_accuracy: 0.7430 - lr: 0.0010\n",
            "Epoch 8/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.7790 - accuracy: 0.7261 - val_loss: 0.7089 - val_accuracy: 0.7545 - lr: 0.0010\n",
            "Epoch 9/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.7534 - accuracy: 0.7351 - val_loss: 0.7487 - val_accuracy: 0.7427 - lr: 0.0010\n",
            "Epoch 10/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.7362 - accuracy: 0.7414 - val_loss: 0.7259 - val_accuracy: 0.7528 - lr: 0.0010\n",
            "Epoch 11/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.6577 - accuracy: 0.7696 - val_loss: 0.6573 - val_accuracy: 0.7696 - lr: 5.0000e-04\n",
            "Epoch 12/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.6333 - accuracy: 0.7771 - val_loss: 0.6910 - val_accuracy: 0.7648 - lr: 5.0000e-04\n",
            "Epoch 13/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.6211 - accuracy: 0.7851 - val_loss: 0.6911 - val_accuracy: 0.7726 - lr: 5.0000e-04\n",
            "Epoch 14/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.6009 - accuracy: 0.7894 - val_loss: 0.6302 - val_accuracy: 0.7842 - lr: 5.0000e-04\n",
            "Epoch 15/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.5966 - accuracy: 0.7888 - val_loss: 0.6322 - val_accuracy: 0.7828 - lr: 5.0000e-04\n",
            "Epoch 16/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.5819 - accuracy: 0.7946 - val_loss: 0.6185 - val_accuracy: 0.7925 - lr: 5.0000e-04\n",
            "Epoch 17/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.5736 - accuracy: 0.7983 - val_loss: 0.6042 - val_accuracy: 0.7933 - lr: 5.0000e-04\n",
            "Epoch 18/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.5668 - accuracy: 0.8000 - val_loss: 0.6100 - val_accuracy: 0.7926 - lr: 5.0000e-04\n",
            "Epoch 19/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.5551 - accuracy: 0.8048 - val_loss: 0.5926 - val_accuracy: 0.8006 - lr: 5.0000e-04\n",
            "Epoch 20/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.5444 - accuracy: 0.8080 - val_loss: 0.6236 - val_accuracy: 0.7907 - lr: 5.0000e-04\n",
            "Epoch 21/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.4926 - accuracy: 0.8279 - val_loss: 0.6008 - val_accuracy: 0.8011 - lr: 1.2500e-04\n",
            "Epoch 22/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.4860 - accuracy: 0.8304 - val_loss: 0.5865 - val_accuracy: 0.8032 - lr: 1.2500e-04\n",
            "Epoch 23/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.4726 - accuracy: 0.8335 - val_loss: 0.5900 - val_accuracy: 0.8045 - lr: 1.2500e-04\n",
            "Epoch 24/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.4738 - accuracy: 0.8334 - val_loss: 0.5802 - val_accuracy: 0.8098 - lr: 1.2500e-04\n",
            "Epoch 25/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.4646 - accuracy: 0.8377 - val_loss: 0.5883 - val_accuracy: 0.8073 - lr: 1.2500e-04\n",
            "Epoch 26/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.4677 - accuracy: 0.8356 - val_loss: 0.5771 - val_accuracy: 0.8079 - lr: 1.2500e-04\n",
            "Epoch 27/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.4608 - accuracy: 0.8375 - val_loss: 0.5755 - val_accuracy: 0.8093 - lr: 1.2500e-04\n",
            "Epoch 28/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.4558 - accuracy: 0.8396 - val_loss: 0.5824 - val_accuracy: 0.8076 - lr: 1.2500e-04\n",
            "Epoch 29/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.4568 - accuracy: 0.8382 - val_loss: 0.5961 - val_accuracy: 0.8083 - lr: 1.2500e-04\n",
            "Epoch 30/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.4522 - accuracy: 0.8428 - val_loss: 0.5738 - val_accuracy: 0.8082 - lr: 1.2500e-04\n",
            "Score for fold 2: loss of 0.5737953186035156; accuracy of 80.82000017166138%\n",
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_28 (Conv2D)          (None, 32, 32, 32)        896       \n",
            "                                                                 \n",
            " max_pooling2d_7 (MaxPooling  multiple                 0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_29 (Conv2D)          (None, 16, 16, 64)        18496     \n",
            "                                                                 \n",
            " dropout_7 (Dropout)         (None, 8, 8, 64)          0         \n",
            "                                                                 \n",
            " conv2d_30 (Conv2D)          (None, 8, 8, 128)         73856     \n",
            "                                                                 \n",
            " conv2d_31 (Conv2D)          (None, 2, 2, 256)         295168    \n",
            "                                                                 \n",
            " flatten_7 (Flatten)         (None, 256)               0         \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 10)                2570      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 390,986\n",
            "Trainable params: 390,986\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.6032 - accuracy: 0.4086 - val_loss: 1.2915 - val_accuracy: 0.5337 - lr: 0.0010\n",
            "Epoch 2/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.2091 - accuracy: 0.5670 - val_loss: 1.0267 - val_accuracy: 0.6447 - lr: 0.0010\n",
            "Epoch 3/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 1.0552 - accuracy: 0.6262 - val_loss: 0.8694 - val_accuracy: 0.6908 - lr: 0.0010\n",
            "Epoch 4/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.9573 - accuracy: 0.6654 - val_loss: 0.9377 - val_accuracy: 0.6772 - lr: 0.0010\n",
            "Epoch 5/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.8875 - accuracy: 0.6895 - val_loss: 0.7645 - val_accuracy: 0.7301 - lr: 0.0010\n",
            "Epoch 6/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.8389 - accuracy: 0.7075 - val_loss: 0.7687 - val_accuracy: 0.7346 - lr: 0.0010\n",
            "Epoch 7/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.7976 - accuracy: 0.7231 - val_loss: 0.7445 - val_accuracy: 0.7427 - lr: 0.0010\n",
            "Epoch 8/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.7781 - accuracy: 0.7297 - val_loss: 0.6809 - val_accuracy: 0.7644 - lr: 0.0010\n",
            "Epoch 9/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.7400 - accuracy: 0.7405 - val_loss: 0.6821 - val_accuracy: 0.7586 - lr: 0.0010\n",
            "Epoch 10/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.7211 - accuracy: 0.7476 - val_loss: 0.6638 - val_accuracy: 0.7730 - lr: 0.0010\n",
            "Epoch 11/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.6373 - accuracy: 0.7769 - val_loss: 0.6066 - val_accuracy: 0.7950 - lr: 5.0000e-04\n",
            "Epoch 12/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.6191 - accuracy: 0.7834 - val_loss: 0.6346 - val_accuracy: 0.7836 - lr: 5.0000e-04\n",
            "Epoch 13/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.6099 - accuracy: 0.7862 - val_loss: 0.6157 - val_accuracy: 0.7901 - lr: 5.0000e-04\n",
            "Epoch 14/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.5922 - accuracy: 0.7924 - val_loss: 0.5912 - val_accuracy: 0.7984 - lr: 5.0000e-04\n",
            "Epoch 15/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.5823 - accuracy: 0.7955 - val_loss: 0.6020 - val_accuracy: 0.7972 - lr: 5.0000e-04\n",
            "Epoch 16/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.5737 - accuracy: 0.7959 - val_loss: 0.5845 - val_accuracy: 0.8050 - lr: 5.0000e-04\n",
            "Epoch 17/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.5612 - accuracy: 0.8026 - val_loss: 0.6431 - val_accuracy: 0.7875 - lr: 5.0000e-04\n",
            "Epoch 18/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.5544 - accuracy: 0.8051 - val_loss: 0.6027 - val_accuracy: 0.7982 - lr: 5.0000e-04\n",
            "Epoch 19/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.5435 - accuracy: 0.8091 - val_loss: 0.5748 - val_accuracy: 0.8048 - lr: 5.0000e-04\n",
            "Epoch 20/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.5319 - accuracy: 0.8130 - val_loss: 0.5919 - val_accuracy: 0.8034 - lr: 5.0000e-04\n",
            "Epoch 21/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.4804 - accuracy: 0.8322 - val_loss: 0.5684 - val_accuracy: 0.8113 - lr: 1.2500e-04\n",
            "Epoch 22/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.4688 - accuracy: 0.8342 - val_loss: 0.5670 - val_accuracy: 0.8137 - lr: 1.2500e-04\n",
            "Epoch 23/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.4666 - accuracy: 0.8342 - val_loss: 0.5457 - val_accuracy: 0.8165 - lr: 1.2500e-04\n",
            "Epoch 24/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.4612 - accuracy: 0.8387 - val_loss: 0.5571 - val_accuracy: 0.8140 - lr: 1.2500e-04\n",
            "Epoch 25/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.4593 - accuracy: 0.8400 - val_loss: 0.5629 - val_accuracy: 0.8145 - lr: 1.2500e-04\n",
            "Epoch 26/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.4511 - accuracy: 0.8418 - val_loss: 0.5611 - val_accuracy: 0.8140 - lr: 1.2500e-04\n",
            "Epoch 27/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.4477 - accuracy: 0.8407 - val_loss: 0.5541 - val_accuracy: 0.8167 - lr: 1.2500e-04\n",
            "Epoch 28/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.4457 - accuracy: 0.8445 - val_loss: 0.5445 - val_accuracy: 0.8215 - lr: 1.2500e-04\n",
            "Epoch 29/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.4453 - accuracy: 0.8424 - val_loss: 0.5437 - val_accuracy: 0.8220 - lr: 1.2500e-04\n",
            "Epoch 30/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.4386 - accuracy: 0.8464 - val_loss: 0.5560 - val_accuracy: 0.8192 - lr: 1.2500e-04\n",
            "Score for fold 3: loss of 0.555976927280426; accuracy of 81.91999793052673%\n",
            "Model: \"sequential_8\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_32 (Conv2D)          (None, 32, 32, 32)        896       \n",
            "                                                                 \n",
            " max_pooling2d_8 (MaxPooling  multiple                 0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_33 (Conv2D)          (None, 16, 16, 64)        18496     \n",
            "                                                                 \n",
            " dropout_8 (Dropout)         (None, 8, 8, 64)          0         \n",
            "                                                                 \n",
            " conv2d_34 (Conv2D)          (None, 8, 8, 128)         73856     \n",
            "                                                                 \n",
            " conv2d_35 (Conv2D)          (None, 2, 2, 256)         295168    \n",
            "                                                                 \n",
            " flatten_8 (Flatten)         (None, 256)               0         \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 10)                2570      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 390,986\n",
            "Trainable params: 390,986\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.6028 - accuracy: 0.4091 - val_loss: 1.2660 - val_accuracy: 0.5378 - lr: 0.0010\n",
            "Epoch 2/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.2294 - accuracy: 0.5586 - val_loss: 1.0526 - val_accuracy: 0.6285 - lr: 0.0010\n",
            "Epoch 3/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 1.0486 - accuracy: 0.6323 - val_loss: 0.9018 - val_accuracy: 0.6808 - lr: 0.0010\n",
            "Epoch 4/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.9440 - accuracy: 0.6694 - val_loss: 0.8726 - val_accuracy: 0.6905 - lr: 0.0010\n",
            "Epoch 5/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.8700 - accuracy: 0.6929 - val_loss: 0.8313 - val_accuracy: 0.7088 - lr: 0.0010\n",
            "Epoch 6/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.8228 - accuracy: 0.7122 - val_loss: 0.7571 - val_accuracy: 0.7343 - lr: 0.0010\n",
            "Epoch 7/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.7859 - accuracy: 0.7251 - val_loss: 0.7583 - val_accuracy: 0.7403 - lr: 0.0010\n",
            "Epoch 8/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.7490 - accuracy: 0.7386 - val_loss: 0.7572 - val_accuracy: 0.7428 - lr: 0.0010\n",
            "Epoch 9/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.7333 - accuracy: 0.7453 - val_loss: 0.7077 - val_accuracy: 0.7530 - lr: 0.0010\n",
            "Epoch 10/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.7034 - accuracy: 0.7545 - val_loss: 0.7275 - val_accuracy: 0.7493 - lr: 0.0010\n",
            "Epoch 11/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.6258 - accuracy: 0.7798 - val_loss: 0.6725 - val_accuracy: 0.7728 - lr: 5.0000e-04\n",
            "Epoch 12/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.6007 - accuracy: 0.7876 - val_loss: 0.6533 - val_accuracy: 0.7788 - lr: 5.0000e-04\n",
            "Epoch 13/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.5922 - accuracy: 0.7933 - val_loss: 0.6410 - val_accuracy: 0.7832 - lr: 5.0000e-04\n",
            "Epoch 14/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.5816 - accuracy: 0.7968 - val_loss: 0.6460 - val_accuracy: 0.7786 - lr: 5.0000e-04\n",
            "Epoch 15/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.5644 - accuracy: 0.8027 - val_loss: 0.6489 - val_accuracy: 0.7815 - lr: 5.0000e-04\n",
            "Epoch 16/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.5550 - accuracy: 0.8046 - val_loss: 0.6196 - val_accuracy: 0.7941 - lr: 5.0000e-04\n",
            "Epoch 17/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.5495 - accuracy: 0.8060 - val_loss: 0.6103 - val_accuracy: 0.7966 - lr: 5.0000e-04\n",
            "Epoch 18/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.5378 - accuracy: 0.8099 - val_loss: 0.6243 - val_accuracy: 0.7910 - lr: 5.0000e-04\n",
            "Epoch 19/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.5253 - accuracy: 0.8149 - val_loss: 0.5983 - val_accuracy: 0.7997 - lr: 5.0000e-04\n",
            "Epoch 20/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.5180 - accuracy: 0.8170 - val_loss: 0.6086 - val_accuracy: 0.7990 - lr: 5.0000e-04\n",
            "Epoch 21/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.4677 - accuracy: 0.8354 - val_loss: 0.5791 - val_accuracy: 0.8108 - lr: 1.2500e-04\n",
            "Epoch 22/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.4518 - accuracy: 0.8407 - val_loss: 0.5942 - val_accuracy: 0.8051 - lr: 1.2500e-04\n",
            "Epoch 23/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.4498 - accuracy: 0.8415 - val_loss: 0.5940 - val_accuracy: 0.8089 - lr: 1.2500e-04\n",
            "Epoch 24/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.4465 - accuracy: 0.8426 - val_loss: 0.5733 - val_accuracy: 0.8125 - lr: 1.2500e-04\n",
            "Epoch 25/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.4420 - accuracy: 0.8439 - val_loss: 0.5770 - val_accuracy: 0.8125 - lr: 1.2500e-04\n",
            "Epoch 26/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.4376 - accuracy: 0.8468 - val_loss: 0.5514 - val_accuracy: 0.8177 - lr: 1.2500e-04\n",
            "Epoch 27/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.4351 - accuracy: 0.8454 - val_loss: 0.5908 - val_accuracy: 0.8095 - lr: 1.2500e-04\n",
            "Epoch 28/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.4373 - accuracy: 0.8465 - val_loss: 0.5750 - val_accuracy: 0.8116 - lr: 1.2500e-04\n",
            "Epoch 29/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.4295 - accuracy: 0.8497 - val_loss: 0.5519 - val_accuracy: 0.8195 - lr: 1.2500e-04\n",
            "Epoch 30/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.4265 - accuracy: 0.8503 - val_loss: 0.5962 - val_accuracy: 0.8054 - lr: 1.2500e-04\n",
            "Score for fold 4: loss of 0.5962417721748352; accuracy of 80.5400013923645%\n",
            "Model: \"sequential_9\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_36 (Conv2D)          (None, 32, 32, 32)        896       \n",
            "                                                                 \n",
            " max_pooling2d_9 (MaxPooling  multiple                 0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_37 (Conv2D)          (None, 16, 16, 64)        18496     \n",
            "                                                                 \n",
            " dropout_9 (Dropout)         (None, 8, 8, 64)          0         \n",
            "                                                                 \n",
            " conv2d_38 (Conv2D)          (None, 8, 8, 128)         73856     \n",
            "                                                                 \n",
            " conv2d_39 (Conv2D)          (None, 2, 2, 256)         295168    \n",
            "                                                                 \n",
            " flatten_9 (Flatten)         (None, 256)               0         \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 10)                2570      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 390,986\n",
            "Trainable params: 390,986\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.5824 - accuracy: 0.4165 - val_loss: 1.2865 - val_accuracy: 0.5449 - lr: 0.0010\n",
            "Epoch 2/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 1.1887 - accuracy: 0.5802 - val_loss: 1.0015 - val_accuracy: 0.6478 - lr: 0.0010\n",
            "Epoch 3/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 1.0283 - accuracy: 0.6381 - val_loss: 0.8858 - val_accuracy: 0.6879 - lr: 0.0010\n",
            "Epoch 4/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.9349 - accuracy: 0.6715 - val_loss: 0.8781 - val_accuracy: 0.6964 - lr: 0.0010\n",
            "Epoch 5/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.8737 - accuracy: 0.6938 - val_loss: 0.8317 - val_accuracy: 0.7131 - lr: 0.0010\n",
            "Epoch 6/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.8231 - accuracy: 0.7129 - val_loss: 0.7359 - val_accuracy: 0.7461 - lr: 0.0010\n",
            "Epoch 7/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.7807 - accuracy: 0.7271 - val_loss: 0.7392 - val_accuracy: 0.7501 - lr: 0.0010\n",
            "Epoch 8/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.7552 - accuracy: 0.7344 - val_loss: 0.7862 - val_accuracy: 0.7339 - lr: 0.0010\n",
            "Epoch 9/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.7209 - accuracy: 0.7469 - val_loss: 0.6916 - val_accuracy: 0.7666 - lr: 0.0010\n",
            "Epoch 10/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.7013 - accuracy: 0.7527 - val_loss: 0.7030 - val_accuracy: 0.7623 - lr: 0.0010\n",
            "Epoch 11/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.6183 - accuracy: 0.7848 - val_loss: 0.6524 - val_accuracy: 0.7850 - lr: 5.0000e-04\n",
            "Epoch 12/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.6001 - accuracy: 0.7893 - val_loss: 0.6706 - val_accuracy: 0.7765 - lr: 5.0000e-04\n",
            "Epoch 13/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.5805 - accuracy: 0.7975 - val_loss: 0.6626 - val_accuracy: 0.7827 - lr: 5.0000e-04\n",
            "Epoch 14/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.5749 - accuracy: 0.7997 - val_loss: 0.6627 - val_accuracy: 0.7760 - lr: 5.0000e-04\n",
            "Epoch 15/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.5619 - accuracy: 0.8044 - val_loss: 0.6814 - val_accuracy: 0.7751 - lr: 5.0000e-04\n",
            "Epoch 16/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.5461 - accuracy: 0.8097 - val_loss: 0.6658 - val_accuracy: 0.7835 - lr: 5.0000e-04\n",
            "Epoch 17/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.5332 - accuracy: 0.8149 - val_loss: 0.6687 - val_accuracy: 0.7826 - lr: 5.0000e-04\n",
            "Epoch 18/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.5329 - accuracy: 0.8138 - val_loss: 0.6258 - val_accuracy: 0.7939 - lr: 5.0000e-04\n",
            "Epoch 19/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.5224 - accuracy: 0.8146 - val_loss: 0.6575 - val_accuracy: 0.7884 - lr: 5.0000e-04\n",
            "Epoch 20/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.5113 - accuracy: 0.8204 - val_loss: 0.7028 - val_accuracy: 0.7727 - lr: 5.0000e-04\n",
            "Epoch 21/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.4519 - accuracy: 0.8400 - val_loss: 0.6027 - val_accuracy: 0.8052 - lr: 1.2500e-04\n",
            "Epoch 22/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.4482 - accuracy: 0.8407 - val_loss: 0.6124 - val_accuracy: 0.8044 - lr: 1.2500e-04\n",
            "Epoch 23/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.4461 - accuracy: 0.8428 - val_loss: 0.6053 - val_accuracy: 0.8065 - lr: 1.2500e-04\n",
            "Epoch 24/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.4368 - accuracy: 0.8456 - val_loss: 0.6216 - val_accuracy: 0.7992 - lr: 1.2500e-04\n",
            "Epoch 25/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.4295 - accuracy: 0.8468 - val_loss: 0.6112 - val_accuracy: 0.8027 - lr: 1.2500e-04\n",
            "Epoch 26/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.4306 - accuracy: 0.8476 - val_loss: 0.6025 - val_accuracy: 0.8102 - lr: 1.2500e-04\n",
            "Epoch 27/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.4291 - accuracy: 0.8482 - val_loss: 0.5984 - val_accuracy: 0.8074 - lr: 1.2500e-04\n",
            "Epoch 28/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.4218 - accuracy: 0.8503 - val_loss: 0.5926 - val_accuracy: 0.8095 - lr: 1.2500e-04\n",
            "Epoch 29/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.4223 - accuracy: 0.8492 - val_loss: 0.5877 - val_accuracy: 0.8102 - lr: 1.2500e-04\n",
            "Epoch 30/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.4165 - accuracy: 0.8519 - val_loss: 0.6196 - val_accuracy: 0.8051 - lr: 1.2500e-04\n",
            "Score for fold 5: loss of 0.6196204423904419; accuracy of 80.51000237464905%\n",
            "------------------------------------------------------------------------\n",
            "Score per fold\n",
            "------------------------------------------------------------------------\n",
            "> Fold 1 - Loss: 0.5444541573524475 - Accuracy: 81.63999915122986%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 2 - Loss: 0.5737953186035156 - Accuracy: 80.82000017166138%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 3 - Loss: 0.555976927280426 - Accuracy: 81.91999793052673%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 4 - Loss: 0.5962417721748352 - Accuracy: 80.5400013923645%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 5 - Loss: 0.6196204423904419 - Accuracy: 80.51000237464905%\n",
            "------------------------------------------------------------------------\n",
            "Average scores for all folds:\n",
            "> Accuracy: 81.0860002040863 (+- 0.5836285223174215)\n",
            "> Loss: 0.5780177235603332\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 4 hidden layers and Narrow Structure"
      ],
      "metadata": {
        "id": "k-sZD7xB7s5O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "acc_per_fold = []\n",
        "loss_per_fold = []\n",
        "\n",
        "with tf.device('/device:GPU:0'):\n",
        "  fold_no = 1\n",
        "  for train, valid in kfold.split(trainX, trainy):\n",
        "\n",
        "    model = CNN_Model(batch_size=32, depth=4, hidden_neurons_1=4, hidden_neurons_2=32, hidden_neurons_3=64, kernel_size_1=3, kernel_size_2=3, kernel_size_3=3, \n",
        "               dropout_rate=0.4, batch_norm=False, pooling='max', layer_activation_1='relu', layer_activation_2='relu', layer_activation_3='relu',\n",
        "               kernel_initialiser='glorot_uniform', bias_initialiser='zeros', optimiser='Adam', weight_decay=None, learning_rate=1e-3, output_activation='softmax', SGD_momentum=0) \n",
        "  \n",
        "    train_gen = tf.keras.preprocessing.image.ImageDataGenerator(width_shift_range=0.1,\n",
        "      height_shift_range=0.1,\n",
        "      horizontal_flip=True)\n",
        "    train_gen.fit(trainX[train])\n",
        "  \n",
        "    print('------------------------------------------------------------------------')\n",
        "    print(f'Training for fold {fold_no} ...')\n",
        "\n",
        "    history = model.train(\n",
        "              train_gen,\n",
        "                trainX[train],\n",
        "                  trainy[train],\n",
        "                trainX[valid],\n",
        "                  trainy[valid],\n",
        "                  lr_rate_type='step')\n",
        "\n",
        "    scores = model.evaluate(trainX[valid], trainy[valid])\n",
        "\n",
        "    print(f'Score for fold {fold_no}: loss of {scores[0]}; accuracy of {scores[1]*100}%')\n",
        "    acc_per_fold.append(scores[1] * 100)\n",
        "    loss_per_fold.append(scores[0])\n",
        "\n",
        "    fold_no+=1\n",
        "\n",
        "# == Provide average scores ==\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Score per fold')\n",
        "for i in range(0, len(acc_per_fold)):\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Average scores for all folds:')\n",
        "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
        "print(f'> Loss: {np.mean(loss_per_fold)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UyIqBqHr7uZ-",
        "outputId": "b497cb0b-ba63-45f8-9def-55611a72073a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_10\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_40 (Conv2D)          (None, 32, 32, 4)         112       \n",
            "                                                                 \n",
            " max_pooling2d_10 (MaxPoolin  multiple                 0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_41 (Conv2D)          (None, 16, 16, 32)        1184      \n",
            "                                                                 \n",
            " dropout_10 (Dropout)        (None, 8, 8, 32)          0         \n",
            "                                                                 \n",
            " conv2d_42 (Conv2D)          (None, 8, 8, 64)          18496     \n",
            "                                                                 \n",
            " conv2d_43 (Conv2D)          (None, 2, 2, 256)         147712    \n",
            "                                                                 \n",
            " flatten_10 (Flatten)        (None, 256)               0         \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 10)                2570      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 170,074\n",
            "Trainable params: 170,074\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.7154 - accuracy: 0.3710 - val_loss: 1.4108 - val_accuracy: 0.4956 - lr: 0.0010\n",
            "Epoch 2/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 1.4343 - accuracy: 0.4824 - val_loss: 1.3224 - val_accuracy: 0.5336 - lr: 0.0010\n",
            "Epoch 3/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 1.3216 - accuracy: 0.5253 - val_loss: 1.1927 - val_accuracy: 0.5749 - lr: 0.0010\n",
            "Epoch 4/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.2463 - accuracy: 0.5570 - val_loss: 1.1180 - val_accuracy: 0.6005 - lr: 0.0010\n",
            "Epoch 5/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 1.1953 - accuracy: 0.5702 - val_loss: 1.0969 - val_accuracy: 0.6113 - lr: 0.0010\n",
            "Epoch 6/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 1.1513 - accuracy: 0.5894 - val_loss: 1.0446 - val_accuracy: 0.6268 - lr: 0.0010\n",
            "Epoch 7/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 1.1124 - accuracy: 0.6024 - val_loss: 1.0369 - val_accuracy: 0.6357 - lr: 0.0010\n",
            "Epoch 8/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 1.0855 - accuracy: 0.6144 - val_loss: 1.0039 - val_accuracy: 0.6467 - lr: 0.0010\n",
            "Epoch 9/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.0554 - accuracy: 0.6259 - val_loss: 0.9847 - val_accuracy: 0.6523 - lr: 0.0010\n",
            "Epoch 10/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.0348 - accuracy: 0.6370 - val_loss: 0.9613 - val_accuracy: 0.6632 - lr: 0.0010\n",
            "Epoch 11/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.9751 - accuracy: 0.6557 - val_loss: 0.9605 - val_accuracy: 0.6597 - lr: 5.0000e-04\n",
            "Epoch 12/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.9590 - accuracy: 0.6607 - val_loss: 0.9349 - val_accuracy: 0.6758 - lr: 5.0000e-04\n",
            "Epoch 13/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.9410 - accuracy: 0.6678 - val_loss: 0.8731 - val_accuracy: 0.6960 - lr: 5.0000e-04\n",
            "Epoch 14/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.9362 - accuracy: 0.6667 - val_loss: 0.8780 - val_accuracy: 0.6929 - lr: 5.0000e-04\n",
            "Epoch 15/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.9264 - accuracy: 0.6744 - val_loss: 0.9413 - val_accuracy: 0.6730 - lr: 5.0000e-04\n",
            "Epoch 16/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.9096 - accuracy: 0.6787 - val_loss: 0.8767 - val_accuracy: 0.6957 - lr: 5.0000e-04\n",
            "Epoch 17/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.9039 - accuracy: 0.6809 - val_loss: 0.8714 - val_accuracy: 0.6970 - lr: 5.0000e-04\n",
            "Epoch 18/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.8929 - accuracy: 0.6836 - val_loss: 0.8517 - val_accuracy: 0.7053 - lr: 5.0000e-04\n",
            "Epoch 19/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.8890 - accuracy: 0.6848 - val_loss: 0.8836 - val_accuracy: 0.6952 - lr: 5.0000e-04\n",
            "Epoch 20/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.8807 - accuracy: 0.6890 - val_loss: 0.8619 - val_accuracy: 0.6996 - lr: 5.0000e-04\n",
            "Epoch 21/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.8409 - accuracy: 0.7032 - val_loss: 0.8347 - val_accuracy: 0.7114 - lr: 1.2500e-04\n",
            "Epoch 22/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.8370 - accuracy: 0.7052 - val_loss: 0.8359 - val_accuracy: 0.7095 - lr: 1.2500e-04\n",
            "Epoch 23/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.8313 - accuracy: 0.7063 - val_loss: 0.8342 - val_accuracy: 0.7105 - lr: 1.2500e-04\n",
            "Epoch 24/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.8306 - accuracy: 0.7070 - val_loss: 0.8194 - val_accuracy: 0.7166 - lr: 1.2500e-04\n",
            "Epoch 25/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.8227 - accuracy: 0.7081 - val_loss: 0.8078 - val_accuracy: 0.7183 - lr: 1.2500e-04\n",
            "Epoch 26/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.8266 - accuracy: 0.7102 - val_loss: 0.8464 - val_accuracy: 0.7069 - lr: 1.2500e-04\n",
            "Epoch 27/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.8170 - accuracy: 0.7117 - val_loss: 0.8207 - val_accuracy: 0.7163 - lr: 1.2500e-04\n",
            "Epoch 28/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.8172 - accuracy: 0.7131 - val_loss: 0.8203 - val_accuracy: 0.7179 - lr: 1.2500e-04\n",
            "Epoch 29/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.8177 - accuracy: 0.7084 - val_loss: 0.8159 - val_accuracy: 0.7194 - lr: 1.2500e-04\n",
            "Epoch 30/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.8105 - accuracy: 0.7119 - val_loss: 0.8283 - val_accuracy: 0.7150 - lr: 1.2500e-04\n",
            "Score for fold 1: loss of 0.8283233642578125; accuracy of 71.49999737739563%\n",
            "Model: \"sequential_11\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_44 (Conv2D)          (None, 32, 32, 4)         112       \n",
            "                                                                 \n",
            " max_pooling2d_11 (MaxPoolin  multiple                 0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_45 (Conv2D)          (None, 16, 16, 32)        1184      \n",
            "                                                                 \n",
            " dropout_11 (Dropout)        (None, 8, 8, 32)          0         \n",
            "                                                                 \n",
            " conv2d_46 (Conv2D)          (None, 8, 8, 64)          18496     \n",
            "                                                                 \n",
            " conv2d_47 (Conv2D)          (None, 2, 2, 256)         147712    \n",
            "                                                                 \n",
            " flatten_11 (Flatten)        (None, 256)               0         \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 10)                2570      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 170,074\n",
            "Trainable params: 170,074\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.7137 - accuracy: 0.3688 - val_loss: 1.4442 - val_accuracy: 0.4791 - lr: 0.0010\n",
            "Epoch 2/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 1.4323 - accuracy: 0.4843 - val_loss: 1.2657 - val_accuracy: 0.5498 - lr: 0.0010\n",
            "Epoch 3/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 1.3043 - accuracy: 0.5343 - val_loss: 1.1897 - val_accuracy: 0.5752 - lr: 0.0010\n",
            "Epoch 4/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.2228 - accuracy: 0.5628 - val_loss: 1.0983 - val_accuracy: 0.6131 - lr: 0.0010\n",
            "Epoch 5/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 1.1669 - accuracy: 0.5856 - val_loss: 1.1215 - val_accuracy: 0.6015 - lr: 0.0010\n",
            "Epoch 6/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 1.1154 - accuracy: 0.6023 - val_loss: 1.0170 - val_accuracy: 0.6395 - lr: 0.0010\n",
            "Epoch 7/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 1.0786 - accuracy: 0.6158 - val_loss: 1.0092 - val_accuracy: 0.6480 - lr: 0.0010\n",
            "Epoch 8/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 1.0536 - accuracy: 0.6278 - val_loss: 0.9779 - val_accuracy: 0.6545 - lr: 0.0010\n",
            "Epoch 9/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 1.0299 - accuracy: 0.6375 - val_loss: 0.9378 - val_accuracy: 0.6705 - lr: 0.0010\n",
            "Epoch 10/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 1.0083 - accuracy: 0.6424 - val_loss: 0.9930 - val_accuracy: 0.6551 - lr: 0.0010\n",
            "Epoch 11/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.9465 - accuracy: 0.6636 - val_loss: 0.8707 - val_accuracy: 0.6954 - lr: 5.0000e-04\n",
            "Epoch 12/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.9387 - accuracy: 0.6695 - val_loss: 0.8539 - val_accuracy: 0.7014 - lr: 5.0000e-04\n",
            "Epoch 13/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.9163 - accuracy: 0.6787 - val_loss: 0.8623 - val_accuracy: 0.7037 - lr: 5.0000e-04\n",
            "Epoch 14/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.9118 - accuracy: 0.6774 - val_loss: 0.8839 - val_accuracy: 0.6915 - lr: 5.0000e-04\n",
            "Epoch 15/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.9046 - accuracy: 0.6808 - val_loss: 0.8543 - val_accuracy: 0.7013 - lr: 5.0000e-04\n",
            "Epoch 16/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.8904 - accuracy: 0.6864 - val_loss: 0.8811 - val_accuracy: 0.6949 - lr: 5.0000e-04\n",
            "Epoch 17/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.8808 - accuracy: 0.6893 - val_loss: 0.8430 - val_accuracy: 0.7091 - lr: 5.0000e-04\n",
            "Epoch 18/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.8737 - accuracy: 0.6925 - val_loss: 0.8279 - val_accuracy: 0.7124 - lr: 5.0000e-04\n",
            "Epoch 19/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.8710 - accuracy: 0.6930 - val_loss: 0.8398 - val_accuracy: 0.7066 - lr: 5.0000e-04\n",
            "Epoch 20/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.8671 - accuracy: 0.6976 - val_loss: 0.8271 - val_accuracy: 0.7126 - lr: 5.0000e-04\n",
            "Epoch 21/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.8222 - accuracy: 0.7117 - val_loss: 0.8196 - val_accuracy: 0.7152 - lr: 1.2500e-04\n",
            "Epoch 22/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.8224 - accuracy: 0.7099 - val_loss: 0.8261 - val_accuracy: 0.7142 - lr: 1.2500e-04\n",
            "Epoch 23/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.8097 - accuracy: 0.7145 - val_loss: 0.7930 - val_accuracy: 0.7259 - lr: 1.2500e-04\n",
            "Epoch 24/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.8190 - accuracy: 0.7138 - val_loss: 0.7976 - val_accuracy: 0.7235 - lr: 1.2500e-04\n",
            "Epoch 25/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.8031 - accuracy: 0.7152 - val_loss: 0.8069 - val_accuracy: 0.7220 - lr: 1.2500e-04\n",
            "Epoch 26/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.8048 - accuracy: 0.7194 - val_loss: 0.8100 - val_accuracy: 0.7214 - lr: 1.2500e-04\n",
            "Epoch 27/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.8044 - accuracy: 0.7178 - val_loss: 0.8055 - val_accuracy: 0.7223 - lr: 1.2500e-04\n",
            "Epoch 28/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.8012 - accuracy: 0.7175 - val_loss: 0.7835 - val_accuracy: 0.7316 - lr: 1.2500e-04\n",
            "Epoch 29/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.7946 - accuracy: 0.7198 - val_loss: 0.8227 - val_accuracy: 0.7158 - lr: 1.2500e-04\n",
            "Epoch 30/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.8055 - accuracy: 0.7170 - val_loss: 0.8013 - val_accuracy: 0.7214 - lr: 1.2500e-04\n",
            "Score for fold 2: loss of 0.801276445388794; accuracy of 72.14000225067139%\n",
            "Model: \"sequential_12\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_48 (Conv2D)          (None, 32, 32, 4)         112       \n",
            "                                                                 \n",
            " max_pooling2d_12 (MaxPoolin  multiple                 0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_49 (Conv2D)          (None, 16, 16, 32)        1184      \n",
            "                                                                 \n",
            " dropout_12 (Dropout)        (None, 8, 8, 32)          0         \n",
            "                                                                 \n",
            " conv2d_50 (Conv2D)          (None, 8, 8, 64)          18496     \n",
            "                                                                 \n",
            " conv2d_51 (Conv2D)          (None, 2, 2, 256)         147712    \n",
            "                                                                 \n",
            " flatten_12 (Flatten)        (None, 256)               0         \n",
            "                                                                 \n",
            " dense_12 (Dense)            (None, 10)                2570      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 170,074\n",
            "Trainable params: 170,074\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.6810 - accuracy: 0.3794 - val_loss: 1.4629 - val_accuracy: 0.4617 - lr: 0.0010\n",
            "Epoch 2/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.4230 - accuracy: 0.4867 - val_loss: 1.4079 - val_accuracy: 0.4991 - lr: 0.0010\n",
            "Epoch 3/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.3055 - accuracy: 0.5299 - val_loss: 1.1399 - val_accuracy: 0.5984 - lr: 0.0010\n",
            "Epoch 4/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.2346 - accuracy: 0.5577 - val_loss: 1.0884 - val_accuracy: 0.6158 - lr: 0.0010\n",
            "Epoch 5/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 1.1727 - accuracy: 0.5847 - val_loss: 1.0896 - val_accuracy: 0.6132 - lr: 0.0010\n",
            "Epoch 6/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 1.1327 - accuracy: 0.5999 - val_loss: 1.0338 - val_accuracy: 0.6298 - lr: 0.0010\n",
            "Epoch 7/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 1.0990 - accuracy: 0.6107 - val_loss: 1.0862 - val_accuracy: 0.6202 - lr: 0.0010\n",
            "Epoch 8/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.0675 - accuracy: 0.6224 - val_loss: 0.9820 - val_accuracy: 0.6560 - lr: 0.0010\n",
            "Epoch 9/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.0475 - accuracy: 0.6301 - val_loss: 0.9649 - val_accuracy: 0.6609 - lr: 0.0010\n",
            "Epoch 10/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 1.0276 - accuracy: 0.6344 - val_loss: 0.9505 - val_accuracy: 0.6685 - lr: 0.0010\n",
            "Epoch 11/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.9693 - accuracy: 0.6580 - val_loss: 0.8821 - val_accuracy: 0.6901 - lr: 5.0000e-04\n",
            "Epoch 12/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.9564 - accuracy: 0.6626 - val_loss: 0.8620 - val_accuracy: 0.6988 - lr: 5.0000e-04\n",
            "Epoch 13/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.9452 - accuracy: 0.6669 - val_loss: 0.8575 - val_accuracy: 0.6973 - lr: 5.0000e-04\n",
            "Epoch 14/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.9284 - accuracy: 0.6741 - val_loss: 0.8600 - val_accuracy: 0.7029 - lr: 5.0000e-04\n",
            "Epoch 15/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.9263 - accuracy: 0.6730 - val_loss: 0.8351 - val_accuracy: 0.7070 - lr: 5.0000e-04\n",
            "Epoch 16/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.9143 - accuracy: 0.6774 - val_loss: 0.8477 - val_accuracy: 0.7039 - lr: 5.0000e-04\n",
            "Epoch 17/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.9039 - accuracy: 0.6821 - val_loss: 0.8217 - val_accuracy: 0.7128 - lr: 5.0000e-04\n",
            "Epoch 18/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.8985 - accuracy: 0.6813 - val_loss: 0.8361 - val_accuracy: 0.7073 - lr: 5.0000e-04\n",
            "Epoch 19/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.8875 - accuracy: 0.6870 - val_loss: 0.8683 - val_accuracy: 0.6943 - lr: 5.0000e-04\n",
            "Epoch 20/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.8852 - accuracy: 0.6859 - val_loss: 0.8075 - val_accuracy: 0.7142 - lr: 5.0000e-04\n",
            "Epoch 21/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.8478 - accuracy: 0.7026 - val_loss: 0.7979 - val_accuracy: 0.7177 - lr: 1.2500e-04\n",
            "Epoch 22/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.8442 - accuracy: 0.7028 - val_loss: 0.7943 - val_accuracy: 0.7214 - lr: 1.2500e-04\n",
            "Epoch 23/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.8336 - accuracy: 0.7085 - val_loss: 0.8127 - val_accuracy: 0.7161 - lr: 1.2500e-04\n",
            "Epoch 24/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.8306 - accuracy: 0.7058 - val_loss: 0.7959 - val_accuracy: 0.7224 - lr: 1.2500e-04\n",
            "Epoch 25/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.8329 - accuracy: 0.7066 - val_loss: 0.8167 - val_accuracy: 0.7137 - lr: 1.2500e-04\n",
            "Epoch 26/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.8308 - accuracy: 0.7082 - val_loss: 0.7985 - val_accuracy: 0.7203 - lr: 1.2500e-04\n",
            "Epoch 27/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.8208 - accuracy: 0.7134 - val_loss: 0.8089 - val_accuracy: 0.7161 - lr: 1.2500e-04\n",
            "Epoch 28/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.8232 - accuracy: 0.7127 - val_loss: 0.8000 - val_accuracy: 0.7155 - lr: 1.2500e-04\n",
            "Epoch 29/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.8172 - accuracy: 0.7136 - val_loss: 0.7916 - val_accuracy: 0.7223 - lr: 1.2500e-04\n",
            "Epoch 30/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.8165 - accuracy: 0.7123 - val_loss: 0.7950 - val_accuracy: 0.7174 - lr: 1.2500e-04\n",
            "Score for fold 3: loss of 0.7950364947319031; accuracy of 71.74000144004822%\n",
            "Model: \"sequential_13\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_52 (Conv2D)          (None, 32, 32, 4)         112       \n",
            "                                                                 \n",
            " max_pooling2d_13 (MaxPoolin  multiple                 0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_53 (Conv2D)          (None, 16, 16, 32)        1184      \n",
            "                                                                 \n",
            " dropout_13 (Dropout)        (None, 8, 8, 32)          0         \n",
            "                                                                 \n",
            " conv2d_54 (Conv2D)          (None, 8, 8, 64)          18496     \n",
            "                                                                 \n",
            " conv2d_55 (Conv2D)          (None, 2, 2, 256)         147712    \n",
            "                                                                 \n",
            " flatten_13 (Flatten)        (None, 256)               0         \n",
            "                                                                 \n",
            " dense_13 (Dense)            (None, 10)                2570      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 170,074\n",
            "Trainable params: 170,074\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.7191 - accuracy: 0.3625 - val_loss: 1.4690 - val_accuracy: 0.4654 - lr: 0.0010\n",
            "Epoch 2/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.4385 - accuracy: 0.4800 - val_loss: 1.3057 - val_accuracy: 0.5286 - lr: 0.0010\n",
            "Epoch 3/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.3013 - accuracy: 0.5335 - val_loss: 1.1994 - val_accuracy: 0.5767 - lr: 0.0010\n",
            "Epoch 4/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.2259 - accuracy: 0.5634 - val_loss: 1.0882 - val_accuracy: 0.6169 - lr: 0.0010\n",
            "Epoch 5/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 1.1621 - accuracy: 0.5847 - val_loss: 1.0380 - val_accuracy: 0.6295 - lr: 0.0010\n",
            "Epoch 6/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 1.1106 - accuracy: 0.6069 - val_loss: 1.0441 - val_accuracy: 0.6288 - lr: 0.0010\n",
            "Epoch 7/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 1.0734 - accuracy: 0.6186 - val_loss: 1.0117 - val_accuracy: 0.6473 - lr: 0.0010\n",
            "Epoch 8/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.0462 - accuracy: 0.6316 - val_loss: 0.9467 - val_accuracy: 0.6624 - lr: 0.0010\n",
            "Epoch 9/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.0202 - accuracy: 0.6422 - val_loss: 0.9379 - val_accuracy: 0.6717 - lr: 0.0010\n",
            "Epoch 10/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.9959 - accuracy: 0.6496 - val_loss: 0.9521 - val_accuracy: 0.6714 - lr: 0.0010\n",
            "Epoch 11/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.9409 - accuracy: 0.6674 - val_loss: 0.9052 - val_accuracy: 0.6884 - lr: 5.0000e-04\n",
            "Epoch 12/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.9253 - accuracy: 0.6743 - val_loss: 0.8663 - val_accuracy: 0.6997 - lr: 5.0000e-04\n",
            "Epoch 13/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.9073 - accuracy: 0.6799 - val_loss: 0.8617 - val_accuracy: 0.7004 - lr: 5.0000e-04\n",
            "Epoch 14/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.8949 - accuracy: 0.6847 - val_loss: 0.8739 - val_accuracy: 0.6985 - lr: 5.0000e-04\n",
            "Epoch 15/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.8924 - accuracy: 0.6841 - val_loss: 0.8404 - val_accuracy: 0.7098 - lr: 5.0000e-04\n",
            "Epoch 16/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.8854 - accuracy: 0.6867 - val_loss: 0.8464 - val_accuracy: 0.7046 - lr: 5.0000e-04\n",
            "Epoch 17/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.8798 - accuracy: 0.6911 - val_loss: 0.8354 - val_accuracy: 0.7114 - lr: 5.0000e-04\n",
            "Epoch 18/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.8723 - accuracy: 0.6931 - val_loss: 0.8303 - val_accuracy: 0.7103 - lr: 5.0000e-04\n",
            "Epoch 19/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.8626 - accuracy: 0.6963 - val_loss: 0.8174 - val_accuracy: 0.7143 - lr: 5.0000e-04\n",
            "Epoch 20/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.8534 - accuracy: 0.6989 - val_loss: 0.8720 - val_accuracy: 0.7005 - lr: 5.0000e-04\n",
            "Epoch 21/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.8161 - accuracy: 0.7140 - val_loss: 0.7950 - val_accuracy: 0.7223 - lr: 1.2500e-04\n",
            "Epoch 22/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.8053 - accuracy: 0.7169 - val_loss: 0.8096 - val_accuracy: 0.7192 - lr: 1.2500e-04\n",
            "Epoch 23/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.8020 - accuracy: 0.7191 - val_loss: 0.7782 - val_accuracy: 0.7316 - lr: 1.2500e-04\n",
            "Epoch 24/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.8014 - accuracy: 0.7188 - val_loss: 0.8199 - val_accuracy: 0.7154 - lr: 1.2500e-04\n",
            "Epoch 25/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.7996 - accuracy: 0.7186 - val_loss: 0.7887 - val_accuracy: 0.7292 - lr: 1.2500e-04\n",
            "Epoch 26/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.7904 - accuracy: 0.7230 - val_loss: 0.8075 - val_accuracy: 0.7197 - lr: 1.2500e-04\n",
            "Epoch 27/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.7934 - accuracy: 0.7215 - val_loss: 0.8153 - val_accuracy: 0.7168 - lr: 1.2500e-04\n",
            "Epoch 28/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.7946 - accuracy: 0.7208 - val_loss: 0.7872 - val_accuracy: 0.7280 - lr: 1.2500e-04\n",
            "Epoch 29/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.7956 - accuracy: 0.7188 - val_loss: 0.7879 - val_accuracy: 0.7294 - lr: 1.2500e-04\n",
            "Epoch 30/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.7862 - accuracy: 0.7241 - val_loss: 0.7831 - val_accuracy: 0.7284 - lr: 1.2500e-04\n",
            "Score for fold 4: loss of 0.7830852270126343; accuracy of 72.83999919891357%\n",
            "Model: \"sequential_14\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_56 (Conv2D)          (None, 32, 32, 4)         112       \n",
            "                                                                 \n",
            " max_pooling2d_14 (MaxPoolin  multiple                 0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_57 (Conv2D)          (None, 16, 16, 32)        1184      \n",
            "                                                                 \n",
            " dropout_14 (Dropout)        (None, 8, 8, 32)          0         \n",
            "                                                                 \n",
            " conv2d_58 (Conv2D)          (None, 8, 8, 64)          18496     \n",
            "                                                                 \n",
            " conv2d_59 (Conv2D)          (None, 2, 2, 256)         147712    \n",
            "                                                                 \n",
            " flatten_14 (Flatten)        (None, 256)               0         \n",
            "                                                                 \n",
            " dense_14 (Dense)            (None, 10)                2570      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 170,074\n",
            "Trainable params: 170,074\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.7269 - accuracy: 0.3623 - val_loss: 1.4766 - val_accuracy: 0.4571 - lr: 0.0010\n",
            "Epoch 2/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.4732 - accuracy: 0.4646 - val_loss: 1.3066 - val_accuracy: 0.5257 - lr: 0.0010\n",
            "Epoch 3/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 1.3413 - accuracy: 0.5174 - val_loss: 1.1988 - val_accuracy: 0.5696 - lr: 0.0010\n",
            "Epoch 4/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 1.2513 - accuracy: 0.5537 - val_loss: 1.1221 - val_accuracy: 0.6018 - lr: 0.0010\n",
            "Epoch 5/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 1.1899 - accuracy: 0.5765 - val_loss: 1.0445 - val_accuracy: 0.6288 - lr: 0.0010\n",
            "Epoch 6/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.1366 - accuracy: 0.5979 - val_loss: 1.0272 - val_accuracy: 0.6331 - lr: 0.0010\n",
            "Epoch 7/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.1039 - accuracy: 0.6088 - val_loss: 1.0448 - val_accuracy: 0.6303 - lr: 0.0010\n",
            "Epoch 8/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.0727 - accuracy: 0.6193 - val_loss: 0.9991 - val_accuracy: 0.6459 - lr: 0.0010\n",
            "Epoch 9/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 1.0473 - accuracy: 0.6291 - val_loss: 0.9033 - val_accuracy: 0.6851 - lr: 0.0010\n",
            "Epoch 10/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 1.0212 - accuracy: 0.6387 - val_loss: 0.9226 - val_accuracy: 0.6772 - lr: 0.0010\n",
            "Epoch 11/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.9632 - accuracy: 0.6589 - val_loss: 0.9407 - val_accuracy: 0.6654 - lr: 5.0000e-04\n",
            "Epoch 12/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.9501 - accuracy: 0.6648 - val_loss: 0.8579 - val_accuracy: 0.6962 - lr: 5.0000e-04\n",
            "Epoch 13/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.9430 - accuracy: 0.6693 - val_loss: 0.9183 - val_accuracy: 0.6748 - lr: 5.0000e-04\n",
            "Epoch 14/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.9296 - accuracy: 0.6715 - val_loss: 0.8569 - val_accuracy: 0.6934 - lr: 5.0000e-04\n",
            "Epoch 15/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.9201 - accuracy: 0.6751 - val_loss: 0.8470 - val_accuracy: 0.6986 - lr: 5.0000e-04\n",
            "Epoch 16/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.9078 - accuracy: 0.6782 - val_loss: 0.8831 - val_accuracy: 0.6920 - lr: 5.0000e-04\n",
            "Epoch 17/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.9043 - accuracy: 0.6809 - val_loss: 0.8729 - val_accuracy: 0.6899 - lr: 5.0000e-04\n",
            "Epoch 18/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.8956 - accuracy: 0.6838 - val_loss: 0.8770 - val_accuracy: 0.6933 - lr: 5.0000e-04\n",
            "Epoch 19/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.8906 - accuracy: 0.6847 - val_loss: 0.8305 - val_accuracy: 0.7068 - lr: 5.0000e-04\n",
            "Epoch 20/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.8867 - accuracy: 0.6902 - val_loss: 0.8688 - val_accuracy: 0.6976 - lr: 5.0000e-04\n",
            "Epoch 21/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.8441 - accuracy: 0.7020 - val_loss: 0.8131 - val_accuracy: 0.7113 - lr: 1.2500e-04\n",
            "Epoch 22/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.8395 - accuracy: 0.7042 - val_loss: 0.8204 - val_accuracy: 0.7094 - lr: 1.2500e-04\n",
            "Epoch 23/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.8371 - accuracy: 0.7038 - val_loss: 0.8129 - val_accuracy: 0.7145 - lr: 1.2500e-04\n",
            "Epoch 24/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.8381 - accuracy: 0.7061 - val_loss: 0.7972 - val_accuracy: 0.7180 - lr: 1.2500e-04\n",
            "Epoch 25/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.8345 - accuracy: 0.7068 - val_loss: 0.7991 - val_accuracy: 0.7194 - lr: 1.2500e-04\n",
            "Epoch 26/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.8246 - accuracy: 0.7106 - val_loss: 0.8415 - val_accuracy: 0.7070 - lr: 1.2500e-04\n",
            "Epoch 27/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.8254 - accuracy: 0.7098 - val_loss: 0.7913 - val_accuracy: 0.7212 - lr: 1.2500e-04\n",
            "Epoch 28/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.8260 - accuracy: 0.7068 - val_loss: 0.7731 - val_accuracy: 0.7270 - lr: 1.2500e-04\n",
            "Epoch 29/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.8196 - accuracy: 0.7112 - val_loss: 0.7984 - val_accuracy: 0.7196 - lr: 1.2500e-04\n",
            "Epoch 30/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.8222 - accuracy: 0.7108 - val_loss: 0.7939 - val_accuracy: 0.7196 - lr: 1.2500e-04\n",
            "Score for fold 5: loss of 0.7939099669456482; accuracy of 71.96000218391418%\n",
            "------------------------------------------------------------------------\n",
            "Score per fold\n",
            "------------------------------------------------------------------------\n",
            "> Fold 1 - Loss: 0.8283233642578125 - Accuracy: 71.49999737739563%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 2 - Loss: 0.801276445388794 - Accuracy: 72.14000225067139%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 3 - Loss: 0.7950364947319031 - Accuracy: 71.74000144004822%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 4 - Loss: 0.7830852270126343 - Accuracy: 72.83999919891357%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 5 - Loss: 0.7939099669456482 - Accuracy: 71.96000218391418%\n",
            "------------------------------------------------------------------------\n",
            "Average scores for all folds:\n",
            "> Accuracy: 72.0360004901886 (+- 0.45561404872529704)\n",
            "> Loss: 0.8003262996673584\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3 hidden layers, Wide and Kernel size 3x3"
      ],
      "metadata": {
        "id": "rmX3n2ukYzW1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "acc_per_fold = []\n",
        "loss_per_fold = []\n",
        "\n",
        "with tf.device('/device:GPU:0'):\n",
        "  fold_no = 1\n",
        "  for train, valid in kfold.split(trainX, trainy):\n",
        "\n",
        "    model = CNN_Model(batch_size=32, depth=3, hidden_neurons_1=64, hidden_neurons_2=128, hidden_neurons_3=256, kernel_size_1=3, kernel_size_2=3, kernel_size_3=3, \n",
        "               dropout_rate=0.2, batch_norm=False, pooling='max', layer_activation_1='relu', layer_activation_2='relu', layer_activation_3='relu',\n",
        "               kernel_initialiser='glorot_uniform', bias_initialiser='zeros', optimiser='Adam', weight_decay=None, learning_rate=1e-3, output_activation='softmax', SGD_momentum=0) \n",
        "  \n",
        "    train_gen = tf.keras.preprocessing.image.ImageDataGenerator(width_shift_range=0.1,\n",
        "      height_shift_range=0.1,\n",
        "      horizontal_flip=True)\n",
        "    train_gen.fit(trainX[train])\n",
        "  \n",
        "    print('------------------------------------------------------------------------')\n",
        "    print(f'Training for fold {fold_no} ...')\n",
        "\n",
        "    history = model.train(\n",
        "              train_gen,\n",
        "                trainX[train],\n",
        "                  trainy[train],\n",
        "                trainX[valid],\n",
        "                  trainy[valid],\n",
        "                  lr_rate_type='step')\n",
        "\n",
        "    scores = model.evaluate(trainX[valid], trainy[valid])\n",
        "\n",
        "    print(f'Score for fold {fold_no}: loss of {scores[0]}; accuracy of {scores[1]*100}%')\n",
        "    acc_per_fold.append(scores[1] * 100)\n",
        "    loss_per_fold.append(scores[0])\n",
        "\n",
        "    fold_no+=1\n",
        "\n",
        "# == Provide average scores ==\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Score per fold')\n",
        "for i in range(0, len(acc_per_fold)):\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Average scores for all folds:')\n",
        "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
        "print(f'> Loss: {np.mean(loss_per_fold)}')"
      ],
      "metadata": {
        "id": "q_M6NSSzY1OF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ec7ccb2-17a7-4408-8bb4-8b4246ae54f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_15\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_60 (Conv2D)          (None, 32, 32, 64)        1792      \n",
            "                                                                 \n",
            " max_pooling2d_15 (MaxPoolin  multiple                 0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_61 (Conv2D)          (None, 16, 16, 128)       73856     \n",
            "                                                                 \n",
            " dropout_15 (Dropout)        (None, 8, 8, 128)         0         \n",
            "                                                                 \n",
            " conv2d_62 (Conv2D)          (None, 8, 8, 256)         295168    \n",
            "                                                                 \n",
            " flatten_15 (Flatten)        (None, 4096)              0         \n",
            "                                                                 \n",
            " dense_15 (Dense)            (None, 10)                40970     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 411,786\n",
            "Trainable params: 411,786\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.5256 - accuracy: 0.4492 - val_loss: 1.1942 - val_accuracy: 0.5684 - lr: 0.0010\n",
            "Epoch 2/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 1.1651 - accuracy: 0.5929 - val_loss: 0.9766 - val_accuracy: 0.6534 - lr: 0.0010\n",
            "Epoch 3/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 1.0111 - accuracy: 0.6475 - val_loss: 0.9219 - val_accuracy: 0.6793 - lr: 0.0010\n",
            "Epoch 4/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.9289 - accuracy: 0.6750 - val_loss: 0.8793 - val_accuracy: 0.6933 - lr: 0.0010\n",
            "Epoch 5/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.8727 - accuracy: 0.6939 - val_loss: 0.8938 - val_accuracy: 0.6930 - lr: 0.0010\n",
            "Epoch 6/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.8215 - accuracy: 0.7141 - val_loss: 0.7350 - val_accuracy: 0.7500 - lr: 0.0010\n",
            "Epoch 7/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.7872 - accuracy: 0.7266 - val_loss: 0.7064 - val_accuracy: 0.7570 - lr: 0.0010\n",
            "Epoch 8/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.7599 - accuracy: 0.7345 - val_loss: 0.7007 - val_accuracy: 0.7612 - lr: 0.0010\n",
            "Epoch 9/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.7374 - accuracy: 0.7437 - val_loss: 0.8086 - val_accuracy: 0.7225 - lr: 0.0010\n",
            "Epoch 10/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.7133 - accuracy: 0.7502 - val_loss: 0.6713 - val_accuracy: 0.7704 - lr: 0.0010\n",
            "Epoch 11/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.6335 - accuracy: 0.7803 - val_loss: 0.6145 - val_accuracy: 0.7912 - lr: 5.0000e-04\n",
            "Epoch 12/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.6098 - accuracy: 0.7867 - val_loss: 0.6206 - val_accuracy: 0.7883 - lr: 5.0000e-04\n",
            "Epoch 13/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.6008 - accuracy: 0.7922 - val_loss: 0.6089 - val_accuracy: 0.7966 - lr: 5.0000e-04\n",
            "Epoch 14/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.5841 - accuracy: 0.7962 - val_loss: 0.6114 - val_accuracy: 0.7942 - lr: 5.0000e-04\n",
            "Epoch 15/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.5735 - accuracy: 0.7990 - val_loss: 0.6060 - val_accuracy: 0.7932 - lr: 5.0000e-04\n",
            "Epoch 16/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.5668 - accuracy: 0.8019 - val_loss: 0.6043 - val_accuracy: 0.8000 - lr: 5.0000e-04\n",
            "Epoch 17/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.5632 - accuracy: 0.8033 - val_loss: 0.5936 - val_accuracy: 0.8006 - lr: 5.0000e-04\n",
            "Epoch 18/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.5479 - accuracy: 0.8083 - val_loss: 0.5774 - val_accuracy: 0.8075 - lr: 5.0000e-04\n",
            "Epoch 19/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.5441 - accuracy: 0.8113 - val_loss: 0.5754 - val_accuracy: 0.8066 - lr: 5.0000e-04\n",
            "Epoch 20/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.5263 - accuracy: 0.8152 - val_loss: 0.5892 - val_accuracy: 0.8001 - lr: 5.0000e-04\n",
            "Epoch 21/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.4721 - accuracy: 0.8357 - val_loss: 0.5440 - val_accuracy: 0.8165 - lr: 1.2500e-04\n",
            "Epoch 22/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.4597 - accuracy: 0.8403 - val_loss: 0.5493 - val_accuracy: 0.8142 - lr: 1.2500e-04\n",
            "Epoch 23/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.4541 - accuracy: 0.8416 - val_loss: 0.5373 - val_accuracy: 0.8181 - lr: 1.2500e-04\n",
            "Epoch 24/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.4579 - accuracy: 0.8407 - val_loss: 0.5450 - val_accuracy: 0.8186 - lr: 1.2500e-04\n",
            "Epoch 25/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.4517 - accuracy: 0.8408 - val_loss: 0.5505 - val_accuracy: 0.8178 - lr: 1.2500e-04\n",
            "Epoch 26/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.4478 - accuracy: 0.8438 - val_loss: 0.5414 - val_accuracy: 0.8188 - lr: 1.2500e-04\n",
            "Epoch 27/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.4413 - accuracy: 0.8473 - val_loss: 0.5368 - val_accuracy: 0.8210 - lr: 1.2500e-04\n",
            "Epoch 28/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.4416 - accuracy: 0.8468 - val_loss: 0.5481 - val_accuracy: 0.8167 - lr: 1.2500e-04\n",
            "Epoch 29/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.4400 - accuracy: 0.8477 - val_loss: 0.5373 - val_accuracy: 0.8213 - lr: 1.2500e-04\n",
            "Epoch 30/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.4353 - accuracy: 0.8476 - val_loss: 0.5333 - val_accuracy: 0.8223 - lr: 1.2500e-04\n",
            "Score for fold 1: loss of 0.5333242416381836; accuracy of 82.23000168800354%\n",
            "Model: \"sequential_16\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_63 (Conv2D)          (None, 32, 32, 64)        1792      \n",
            "                                                                 \n",
            " max_pooling2d_16 (MaxPoolin  multiple                 0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_64 (Conv2D)          (None, 16, 16, 128)       73856     \n",
            "                                                                 \n",
            " dropout_16 (Dropout)        (None, 8, 8, 128)         0         \n",
            "                                                                 \n",
            " conv2d_65 (Conv2D)          (None, 8, 8, 256)         295168    \n",
            "                                                                 \n",
            " flatten_16 (Flatten)        (None, 4096)              0         \n",
            "                                                                 \n",
            " dense_16 (Dense)            (None, 10)                40970     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 411,786\n",
            "Trainable params: 411,786\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.5235 - accuracy: 0.4498 - val_loss: 1.1585 - val_accuracy: 0.5900 - lr: 0.0010\n",
            "Epoch 2/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 1.1633 - accuracy: 0.5916 - val_loss: 1.0875 - val_accuracy: 0.6317 - lr: 0.0010\n",
            "Epoch 3/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 1.0132 - accuracy: 0.6460 - val_loss: 0.9008 - val_accuracy: 0.6826 - lr: 0.0010\n",
            "Epoch 4/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.9334 - accuracy: 0.6748 - val_loss: 0.8669 - val_accuracy: 0.7020 - lr: 0.0010\n",
            "Epoch 5/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.8738 - accuracy: 0.6974 - val_loss: 0.7891 - val_accuracy: 0.7301 - lr: 0.0010\n",
            "Epoch 6/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.8337 - accuracy: 0.7115 - val_loss: 0.9063 - val_accuracy: 0.6903 - lr: 0.0010\n",
            "Epoch 7/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.7967 - accuracy: 0.7244 - val_loss: 0.8131 - val_accuracy: 0.7174 - lr: 0.0010\n",
            "Epoch 8/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.7662 - accuracy: 0.7327 - val_loss: 0.7493 - val_accuracy: 0.7401 - lr: 0.0010\n",
            "Epoch 9/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.7402 - accuracy: 0.7428 - val_loss: 0.7098 - val_accuracy: 0.7568 - lr: 0.0010\n",
            "Epoch 10/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.7198 - accuracy: 0.7527 - val_loss: 0.7082 - val_accuracy: 0.7582 - lr: 0.0010\n",
            "Epoch 11/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.6372 - accuracy: 0.7783 - val_loss: 0.6505 - val_accuracy: 0.7776 - lr: 5.0000e-04\n",
            "Epoch 12/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.6118 - accuracy: 0.7891 - val_loss: 0.6539 - val_accuracy: 0.7793 - lr: 5.0000e-04\n",
            "Epoch 13/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.6011 - accuracy: 0.7920 - val_loss: 0.6634 - val_accuracy: 0.7779 - lr: 5.0000e-04\n",
            "Epoch 14/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.5830 - accuracy: 0.7968 - val_loss: 0.6142 - val_accuracy: 0.7896 - lr: 5.0000e-04\n",
            "Epoch 15/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.5705 - accuracy: 0.8056 - val_loss: 0.5917 - val_accuracy: 0.7987 - lr: 5.0000e-04\n",
            "Epoch 16/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.5632 - accuracy: 0.8032 - val_loss: 0.6257 - val_accuracy: 0.7902 - lr: 5.0000e-04\n",
            "Epoch 17/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.5511 - accuracy: 0.8071 - val_loss: 0.6177 - val_accuracy: 0.7908 - lr: 5.0000e-04\n",
            "Epoch 18/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.5464 - accuracy: 0.8094 - val_loss: 0.5956 - val_accuracy: 0.7996 - lr: 5.0000e-04\n",
            "Epoch 19/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.5363 - accuracy: 0.8153 - val_loss: 0.6124 - val_accuracy: 0.7920 - lr: 5.0000e-04\n",
            "Epoch 20/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.5290 - accuracy: 0.8153 - val_loss: 0.6167 - val_accuracy: 0.7945 - lr: 5.0000e-04\n",
            "Epoch 21/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.4698 - accuracy: 0.8377 - val_loss: 0.5468 - val_accuracy: 0.8167 - lr: 1.2500e-04\n",
            "Epoch 22/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.4587 - accuracy: 0.8412 - val_loss: 0.5452 - val_accuracy: 0.8165 - lr: 1.2500e-04\n",
            "Epoch 23/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.4560 - accuracy: 0.8424 - val_loss: 0.5384 - val_accuracy: 0.8187 - lr: 1.2500e-04\n",
            "Epoch 24/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.4477 - accuracy: 0.8430 - val_loss: 0.5458 - val_accuracy: 0.8140 - lr: 1.2500e-04\n",
            "Epoch 25/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.4479 - accuracy: 0.8442 - val_loss: 0.5537 - val_accuracy: 0.8141 - lr: 1.2500e-04\n",
            "Epoch 26/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.4483 - accuracy: 0.8438 - val_loss: 0.5658 - val_accuracy: 0.8104 - lr: 1.2500e-04\n",
            "Epoch 27/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.4400 - accuracy: 0.8460 - val_loss: 0.5383 - val_accuracy: 0.8202 - lr: 1.2500e-04\n",
            "Epoch 28/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.4346 - accuracy: 0.8493 - val_loss: 0.5374 - val_accuracy: 0.8204 - lr: 1.2500e-04\n",
            "Epoch 29/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 0.4336 - accuracy: 0.8479 - val_loss: 0.5379 - val_accuracy: 0.8216 - lr: 1.2500e-04\n",
            "Epoch 30/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.4326 - accuracy: 0.8494 - val_loss: 0.5458 - val_accuracy: 0.8182 - lr: 1.2500e-04\n",
            "Score for fold 2: loss of 0.5457882285118103; accuracy of 81.81999921798706%\n",
            "Model: \"sequential_17\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_66 (Conv2D)          (None, 32, 32, 64)        1792      \n",
            "                                                                 \n",
            " max_pooling2d_17 (MaxPoolin  multiple                 0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_67 (Conv2D)          (None, 16, 16, 128)       73856     \n",
            "                                                                 \n",
            " dropout_17 (Dropout)        (None, 8, 8, 128)         0         \n",
            "                                                                 \n",
            " conv2d_68 (Conv2D)          (None, 8, 8, 256)         295168    \n",
            "                                                                 \n",
            " flatten_17 (Flatten)        (None, 4096)              0         \n",
            "                                                                 \n",
            " dense_17 (Dense)            (None, 10)                40970     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 411,786\n",
            "Trainable params: 411,786\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.5461 - accuracy: 0.4387 - val_loss: 1.1853 - val_accuracy: 0.5842 - lr: 0.0010\n",
            "Epoch 2/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.1457 - accuracy: 0.5946 - val_loss: 0.9667 - val_accuracy: 0.6637 - lr: 0.0010\n",
            "Epoch 3/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.0045 - accuracy: 0.6462 - val_loss: 0.8763 - val_accuracy: 0.6983 - lr: 0.0010\n",
            "Epoch 4/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.9170 - accuracy: 0.6800 - val_loss: 0.8055 - val_accuracy: 0.7269 - lr: 0.0010\n",
            "Epoch 5/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.8571 - accuracy: 0.7024 - val_loss: 0.8502 - val_accuracy: 0.7146 - lr: 0.0010\n",
            "Epoch 6/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.8162 - accuracy: 0.7149 - val_loss: 0.7551 - val_accuracy: 0.7401 - lr: 0.0010\n",
            "Epoch 7/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.7782 - accuracy: 0.7283 - val_loss: 0.7238 - val_accuracy: 0.7547 - lr: 0.0010\n",
            "Epoch 8/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.7502 - accuracy: 0.7408 - val_loss: 0.7124 - val_accuracy: 0.7569 - lr: 0.0010\n",
            "Epoch 9/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.7245 - accuracy: 0.7503 - val_loss: 0.7124 - val_accuracy: 0.7576 - lr: 0.0010\n",
            "Epoch 10/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.7062 - accuracy: 0.7554 - val_loss: 0.6993 - val_accuracy: 0.7655 - lr: 0.0010\n",
            "Epoch 11/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.6227 - accuracy: 0.7840 - val_loss: 0.6339 - val_accuracy: 0.7882 - lr: 5.0000e-04\n",
            "Epoch 12/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.5981 - accuracy: 0.7898 - val_loss: 0.6070 - val_accuracy: 0.7938 - lr: 5.0000e-04\n",
            "Epoch 13/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.5855 - accuracy: 0.7966 - val_loss: 0.6343 - val_accuracy: 0.7853 - lr: 5.0000e-04\n",
            "Epoch 14/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.5694 - accuracy: 0.7996 - val_loss: 0.6229 - val_accuracy: 0.7915 - lr: 5.0000e-04\n",
            "Epoch 15/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.5565 - accuracy: 0.8055 - val_loss: 0.6157 - val_accuracy: 0.7949 - lr: 5.0000e-04\n",
            "Epoch 16/30\n",
            "1250/1250 [==============================] - 20s 16ms/step - loss: 0.5471 - accuracy: 0.8080 - val_loss: 0.5977 - val_accuracy: 0.7995 - lr: 5.0000e-04\n",
            "Epoch 17/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.5359 - accuracy: 0.8106 - val_loss: 0.5933 - val_accuracy: 0.7994 - lr: 5.0000e-04\n",
            "Epoch 18/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.5323 - accuracy: 0.8126 - val_loss: 0.5963 - val_accuracy: 0.8013 - lr: 5.0000e-04\n",
            "Epoch 19/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.5230 - accuracy: 0.8175 - val_loss: 0.6106 - val_accuracy: 0.7952 - lr: 5.0000e-04\n",
            "Epoch 20/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.5148 - accuracy: 0.8199 - val_loss: 0.5771 - val_accuracy: 0.8066 - lr: 5.0000e-04\n",
            "Epoch 21/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.4527 - accuracy: 0.8444 - val_loss: 0.5802 - val_accuracy: 0.8075 - lr: 1.2500e-04\n",
            "Epoch 22/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.4464 - accuracy: 0.8433 - val_loss: 0.5636 - val_accuracy: 0.8162 - lr: 1.2500e-04\n",
            "Epoch 23/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.4359 - accuracy: 0.8476 - val_loss: 0.5465 - val_accuracy: 0.8163 - lr: 1.2500e-04\n",
            "Epoch 24/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.4355 - accuracy: 0.8468 - val_loss: 0.5417 - val_accuracy: 0.8168 - lr: 1.2500e-04\n",
            "Epoch 25/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.4308 - accuracy: 0.8479 - val_loss: 0.5594 - val_accuracy: 0.8152 - lr: 1.2500e-04\n",
            "Epoch 26/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.4255 - accuracy: 0.8517 - val_loss: 0.5456 - val_accuracy: 0.8178 - lr: 1.2500e-04\n",
            "Epoch 27/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.4246 - accuracy: 0.8503 - val_loss: 0.5603 - val_accuracy: 0.8160 - lr: 1.2500e-04\n",
            "Epoch 28/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.4248 - accuracy: 0.8503 - val_loss: 0.5454 - val_accuracy: 0.8174 - lr: 1.2500e-04\n",
            "Epoch 29/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.4160 - accuracy: 0.8547 - val_loss: 0.5535 - val_accuracy: 0.8167 - lr: 1.2500e-04\n",
            "Epoch 30/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.4168 - accuracy: 0.8545 - val_loss: 0.5396 - val_accuracy: 0.8236 - lr: 1.2500e-04\n",
            "Score for fold 3: loss of 0.5395516753196716; accuracy of 82.35999941825867%\n",
            "Model: \"sequential_18\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_69 (Conv2D)          (None, 32, 32, 64)        1792      \n",
            "                                                                 \n",
            " max_pooling2d_18 (MaxPoolin  multiple                 0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_70 (Conv2D)          (None, 16, 16, 128)       73856     \n",
            "                                                                 \n",
            " dropout_18 (Dropout)        (None, 8, 8, 128)         0         \n",
            "                                                                 \n",
            " conv2d_71 (Conv2D)          (None, 8, 8, 256)         295168    \n",
            "                                                                 \n",
            " flatten_18 (Flatten)        (None, 4096)              0         \n",
            "                                                                 \n",
            " dense_18 (Dense)            (None, 10)                40970     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 411,786\n",
            "Trainable params: 411,786\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/30\n",
            "1250/1250 [==============================] - 20s 15ms/step - loss: 1.5187 - accuracy: 0.4504 - val_loss: 1.2541 - val_accuracy: 0.5574 - lr: 0.0010\n",
            "Epoch 2/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.1240 - accuracy: 0.6041 - val_loss: 0.9585 - val_accuracy: 0.6626 - lr: 0.0010\n",
            "Epoch 3/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.9768 - accuracy: 0.6590 - val_loss: 0.8803 - val_accuracy: 0.7017 - lr: 0.0010\n",
            "Epoch 4/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.8946 - accuracy: 0.6897 - val_loss: 0.8091 - val_accuracy: 0.7223 - lr: 0.0010\n",
            "Epoch 5/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.8442 - accuracy: 0.7071 - val_loss: 0.7346 - val_accuracy: 0.7517 - lr: 0.0010\n",
            "Epoch 6/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.7932 - accuracy: 0.7251 - val_loss: 0.7400 - val_accuracy: 0.7426 - lr: 0.0010\n",
            "Epoch 7/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.7611 - accuracy: 0.7350 - val_loss: 0.7914 - val_accuracy: 0.7288 - lr: 0.0010\n",
            "Epoch 8/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.7340 - accuracy: 0.7448 - val_loss: 0.7064 - val_accuracy: 0.7581 - lr: 0.0010\n",
            "Epoch 9/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.7099 - accuracy: 0.7517 - val_loss: 0.6883 - val_accuracy: 0.7619 - lr: 0.0010\n",
            "Epoch 10/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.6863 - accuracy: 0.7612 - val_loss: 0.6864 - val_accuracy: 0.7627 - lr: 0.0010\n",
            "Epoch 11/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.6043 - accuracy: 0.7902 - val_loss: 0.6351 - val_accuracy: 0.7838 - lr: 5.0000e-04\n",
            "Epoch 12/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.5775 - accuracy: 0.7991 - val_loss: 0.6887 - val_accuracy: 0.7694 - lr: 5.0000e-04\n",
            "Epoch 13/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.5596 - accuracy: 0.8050 - val_loss: 0.6089 - val_accuracy: 0.7922 - lr: 5.0000e-04\n",
            "Epoch 14/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.5459 - accuracy: 0.8097 - val_loss: 0.6066 - val_accuracy: 0.7962 - lr: 5.0000e-04\n",
            "Epoch 15/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.5402 - accuracy: 0.8140 - val_loss: 0.5731 - val_accuracy: 0.8024 - lr: 5.0000e-04\n",
            "Epoch 16/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.5300 - accuracy: 0.8151 - val_loss: 0.5805 - val_accuracy: 0.7987 - lr: 5.0000e-04\n",
            "Epoch 17/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.5212 - accuracy: 0.8187 - val_loss: 0.5755 - val_accuracy: 0.8030 - lr: 5.0000e-04\n",
            "Epoch 18/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.5085 - accuracy: 0.8228 - val_loss: 0.5716 - val_accuracy: 0.8052 - lr: 5.0000e-04\n",
            "Epoch 19/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.5061 - accuracy: 0.8238 - val_loss: 0.6011 - val_accuracy: 0.7992 - lr: 5.0000e-04\n",
            "Epoch 20/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.4918 - accuracy: 0.8274 - val_loss: 0.5686 - val_accuracy: 0.8081 - lr: 5.0000e-04\n",
            "Epoch 21/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.4378 - accuracy: 0.8473 - val_loss: 0.5711 - val_accuracy: 0.8096 - lr: 1.2500e-04\n",
            "Epoch 22/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.4266 - accuracy: 0.8513 - val_loss: 0.5411 - val_accuracy: 0.8188 - lr: 1.2500e-04\n",
            "Epoch 23/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.4200 - accuracy: 0.8543 - val_loss: 0.5199 - val_accuracy: 0.8249 - lr: 1.2500e-04\n",
            "Epoch 24/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.4140 - accuracy: 0.8547 - val_loss: 0.5499 - val_accuracy: 0.8187 - lr: 1.2500e-04\n",
            "Epoch 25/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.4095 - accuracy: 0.8570 - val_loss: 0.5310 - val_accuracy: 0.8238 - lr: 1.2500e-04\n",
            "Epoch 26/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.4044 - accuracy: 0.8584 - val_loss: 0.5299 - val_accuracy: 0.8227 - lr: 1.2500e-04\n",
            "Epoch 27/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.4040 - accuracy: 0.8570 - val_loss: 0.5237 - val_accuracy: 0.8251 - lr: 1.2500e-04\n",
            "Epoch 28/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.4033 - accuracy: 0.8583 - val_loss: 0.5375 - val_accuracy: 0.8212 - lr: 1.2500e-04\n",
            "Epoch 29/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.3958 - accuracy: 0.8612 - val_loss: 0.5231 - val_accuracy: 0.8247 - lr: 1.2500e-04\n",
            "Epoch 30/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.3961 - accuracy: 0.8615 - val_loss: 0.5310 - val_accuracy: 0.8212 - lr: 1.2500e-04\n",
            "Score for fold 4: loss of 0.531012773513794; accuracy of 82.12000131607056%\n",
            "Model: \"sequential_19\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_72 (Conv2D)          (None, 32, 32, 64)        1792      \n",
            "                                                                 \n",
            " max_pooling2d_19 (MaxPoolin  multiple                 0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_73 (Conv2D)          (None, 16, 16, 128)       73856     \n",
            "                                                                 \n",
            " dropout_19 (Dropout)        (None, 8, 8, 128)         0         \n",
            "                                                                 \n",
            " conv2d_74 (Conv2D)          (None, 8, 8, 256)         295168    \n",
            "                                                                 \n",
            " flatten_19 (Flatten)        (None, 4096)              0         \n",
            "                                                                 \n",
            " dense_19 (Dense)            (None, 10)                40970     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 411,786\n",
            "Trainable params: 411,786\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.5440 - accuracy: 0.4412 - val_loss: 1.1937 - val_accuracy: 0.5800 - lr: 0.0010\n",
            "Epoch 2/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.1728 - accuracy: 0.5871 - val_loss: 1.0072 - val_accuracy: 0.6453 - lr: 0.0010\n",
            "Epoch 3/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.0242 - accuracy: 0.6379 - val_loss: 0.9640 - val_accuracy: 0.6647 - lr: 0.0010\n",
            "Epoch 4/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.9379 - accuracy: 0.6726 - val_loss: 0.9005 - val_accuracy: 0.6907 - lr: 0.0010\n",
            "Epoch 5/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.8845 - accuracy: 0.6888 - val_loss: 0.7991 - val_accuracy: 0.7316 - lr: 0.0010\n",
            "Epoch 6/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.8404 - accuracy: 0.7079 - val_loss: 0.8391 - val_accuracy: 0.7087 - lr: 0.0010\n",
            "Epoch 7/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.8065 - accuracy: 0.7216 - val_loss: 0.7655 - val_accuracy: 0.7408 - lr: 0.0010\n",
            "Epoch 8/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.7702 - accuracy: 0.7322 - val_loss: 0.7551 - val_accuracy: 0.7411 - lr: 0.0010\n",
            "Epoch 9/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.7447 - accuracy: 0.7424 - val_loss: 0.6963 - val_accuracy: 0.7587 - lr: 0.0010\n",
            "Epoch 10/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.7306 - accuracy: 0.7444 - val_loss: 0.7007 - val_accuracy: 0.7608 - lr: 0.0010\n",
            "Epoch 11/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.6520 - accuracy: 0.7751 - val_loss: 0.6516 - val_accuracy: 0.7766 - lr: 5.0000e-04\n",
            "Epoch 12/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.6281 - accuracy: 0.7819 - val_loss: 0.6538 - val_accuracy: 0.7748 - lr: 5.0000e-04\n",
            "Epoch 13/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.6099 - accuracy: 0.7875 - val_loss: 0.6767 - val_accuracy: 0.7717 - lr: 5.0000e-04\n",
            "Epoch 14/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.5954 - accuracy: 0.7918 - val_loss: 0.6262 - val_accuracy: 0.7915 - lr: 5.0000e-04\n",
            "Epoch 15/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.5921 - accuracy: 0.7943 - val_loss: 0.6444 - val_accuracy: 0.7817 - lr: 5.0000e-04\n",
            "Epoch 16/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.5731 - accuracy: 0.8002 - val_loss: 0.6483 - val_accuracy: 0.7826 - lr: 5.0000e-04\n",
            "Epoch 17/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.5708 - accuracy: 0.8005 - val_loss: 0.6155 - val_accuracy: 0.7930 - lr: 5.0000e-04\n",
            "Epoch 18/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.5614 - accuracy: 0.8052 - val_loss: 0.6355 - val_accuracy: 0.7868 - lr: 5.0000e-04\n",
            "Epoch 19/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.5511 - accuracy: 0.8080 - val_loss: 0.6535 - val_accuracy: 0.7869 - lr: 5.0000e-04\n",
            "Epoch 20/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.5342 - accuracy: 0.8138 - val_loss: 0.6080 - val_accuracy: 0.7977 - lr: 5.0000e-04\n",
            "Epoch 21/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.4865 - accuracy: 0.8324 - val_loss: 0.5806 - val_accuracy: 0.8091 - lr: 1.2500e-04\n",
            "Epoch 22/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.4800 - accuracy: 0.8339 - val_loss: 0.5805 - val_accuracy: 0.8095 - lr: 1.2500e-04\n",
            "Epoch 23/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.4667 - accuracy: 0.8383 - val_loss: 0.5749 - val_accuracy: 0.8132 - lr: 1.2500e-04\n",
            "Epoch 24/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.4643 - accuracy: 0.8377 - val_loss: 0.5834 - val_accuracy: 0.8106 - lr: 1.2500e-04\n",
            "Epoch 25/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.4642 - accuracy: 0.8389 - val_loss: 0.5858 - val_accuracy: 0.8125 - lr: 1.2500e-04\n",
            "Epoch 26/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.4575 - accuracy: 0.8421 - val_loss: 0.5858 - val_accuracy: 0.8081 - lr: 1.2500e-04\n",
            "Epoch 27/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.4509 - accuracy: 0.8431 - val_loss: 0.5777 - val_accuracy: 0.8081 - lr: 1.2500e-04\n",
            "Epoch 28/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.4504 - accuracy: 0.8435 - val_loss: 0.5757 - val_accuracy: 0.8136 - lr: 1.2500e-04\n",
            "Epoch 29/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.4462 - accuracy: 0.8451 - val_loss: 0.5708 - val_accuracy: 0.8128 - lr: 1.2500e-04\n",
            "Epoch 30/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.4498 - accuracy: 0.8420 - val_loss: 0.5849 - val_accuracy: 0.8078 - lr: 1.2500e-04\n",
            "Score for fold 5: loss of 0.5849189162254333; accuracy of 80.77999949455261%\n",
            "------------------------------------------------------------------------\n",
            "Score per fold\n",
            "------------------------------------------------------------------------\n",
            "> Fold 1 - Loss: 0.5333242416381836 - Accuracy: 82.23000168800354%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 2 - Loss: 0.5457882285118103 - Accuracy: 81.81999921798706%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 3 - Loss: 0.5395516753196716 - Accuracy: 82.35999941825867%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 4 - Loss: 0.531012773513794 - Accuracy: 82.12000131607056%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 5 - Loss: 0.5849189162254333 - Accuracy: 80.77999949455261%\n",
            "------------------------------------------------------------------------\n",
            "Average scores for all folds:\n",
            "> Accuracy: 81.86200022697449 (+- 0.5696459429384902)\n",
            "> Loss: 0.5469191670417786\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3 hidden layers, Middle and Kernel size 3x3"
      ],
      "metadata": {
        "id": "hF9OyP7PYVu1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "acc_per_fold = []\n",
        "loss_per_fold = []\n",
        "\n",
        "with tf.device('/device:GPU:0'):\n",
        "  fold_no = 1\n",
        "  for train, valid in kfold.split(trainX, trainy):\n",
        "\n",
        "    model = CNN_Model(batch_size=32, depth=3, hidden_neurons_1=32, hidden_neurons_2=64, hidden_neurons_3=128, kernel_size_1=3, kernel_size_2=3, kernel_size_3=3, \n",
        "               dropout_rate=0.2, batch_norm=False, pooling='max', layer_activation_1='relu', layer_activation_2='relu', layer_activation_3='relu',\n",
        "               kernel_initialiser='glorot_uniform', bias_initialiser='zeros', optimiser='Adam', weight_decay=None, learning_rate=1e-3, output_activation='softmax', SGD_momentum=0) \n",
        "  \n",
        "    train_gen = tf.keras.preprocessing.image.ImageDataGenerator(width_shift_range=0.1,\n",
        "      height_shift_range=0.1,\n",
        "      horizontal_flip=True)\n",
        "    train_gen.fit(trainX[train])\n",
        "  \n",
        "    print('------------------------------------------------------------------------')\n",
        "    print(f'Training for fold {fold_no} ...')\n",
        "\n",
        "    history = model.train(\n",
        "              train_gen,\n",
        "                trainX[train],\n",
        "                  trainy[train],\n",
        "                trainX[valid],\n",
        "                  trainy[valid],\n",
        "                  lr_rate_type='step')\n",
        "\n",
        "    scores = model.evaluate(trainX[valid], trainy[valid])\n",
        "\n",
        "    print(f'Score for fold {fold_no}: loss of {scores[0]}; accuracy of {scores[1]*100}%')\n",
        "    acc_per_fold.append(scores[1] * 100)\n",
        "    loss_per_fold.append(scores[0])\n",
        "\n",
        "    fold_no+=1\n",
        "\n",
        "# == Provide average scores ==\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Score per fold')\n",
        "for i in range(0, len(acc_per_fold)):\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Average scores for all folds:')\n",
        "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
        "print(f'> Loss: {np.mean(loss_per_fold)}')"
      ],
      "metadata": {
        "id": "NQ9__RwrYcAH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79c41878-d127-47a4-ae61-7a495464b2a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_20\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_75 (Conv2D)          (None, 32, 32, 32)        896       \n",
            "                                                                 \n",
            " max_pooling2d_20 (MaxPoolin  multiple                 0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_76 (Conv2D)          (None, 16, 16, 64)        18496     \n",
            "                                                                 \n",
            " dropout_20 (Dropout)        (None, 8, 8, 64)          0         \n",
            "                                                                 \n",
            " conv2d_77 (Conv2D)          (None, 8, 8, 128)         73856     \n",
            "                                                                 \n",
            " flatten_20 (Flatten)        (None, 2048)              0         \n",
            "                                                                 \n",
            " dense_20 (Dense)            (None, 10)                20490     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 113,738\n",
            "Trainable params: 113,738\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.5872 - accuracy: 0.4259 - val_loss: 1.3002 - val_accuracy: 0.5379 - lr: 0.0010\n",
            "Epoch 2/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.2292 - accuracy: 0.5646 - val_loss: 1.1077 - val_accuracy: 0.6085 - lr: 0.0010\n",
            "Epoch 3/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 1.0867 - accuracy: 0.6176 - val_loss: 0.9807 - val_accuracy: 0.6628 - lr: 0.0010\n",
            "Epoch 4/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 1.0064 - accuracy: 0.6513 - val_loss: 0.8815 - val_accuracy: 0.6947 - lr: 0.0010\n",
            "Epoch 5/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.9425 - accuracy: 0.6718 - val_loss: 0.8196 - val_accuracy: 0.7177 - lr: 0.0010\n",
            "Epoch 6/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.8989 - accuracy: 0.6859 - val_loss: 0.8535 - val_accuracy: 0.7050 - lr: 0.0010\n",
            "Epoch 7/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.8677 - accuracy: 0.6996 - val_loss: 0.7953 - val_accuracy: 0.7234 - lr: 0.0010\n",
            "Epoch 8/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.8392 - accuracy: 0.7085 - val_loss: 0.8335 - val_accuracy: 0.7105 - lr: 0.0010\n",
            "Epoch 9/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.8169 - accuracy: 0.7161 - val_loss: 0.7290 - val_accuracy: 0.7509 - lr: 0.0010\n",
            "Epoch 10/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.7892 - accuracy: 0.7233 - val_loss: 0.8368 - val_accuracy: 0.7210 - lr: 0.0010\n",
            "Epoch 11/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.7258 - accuracy: 0.7469 - val_loss: 0.6947 - val_accuracy: 0.7614 - lr: 5.0000e-04\n",
            "Epoch 12/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.7062 - accuracy: 0.7546 - val_loss: 0.6979 - val_accuracy: 0.7589 - lr: 5.0000e-04\n",
            "Epoch 13/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.6979 - accuracy: 0.7573 - val_loss: 0.6868 - val_accuracy: 0.7653 - lr: 5.0000e-04\n",
            "Epoch 14/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.6805 - accuracy: 0.7639 - val_loss: 0.6816 - val_accuracy: 0.7657 - lr: 5.0000e-04\n",
            "Epoch 15/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.6749 - accuracy: 0.7651 - val_loss: 0.7310 - val_accuracy: 0.7482 - lr: 5.0000e-04\n",
            "Epoch 16/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.6668 - accuracy: 0.7691 - val_loss: 0.6641 - val_accuracy: 0.7725 - lr: 5.0000e-04\n",
            "Epoch 17/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.6598 - accuracy: 0.7711 - val_loss: 0.6637 - val_accuracy: 0.7741 - lr: 5.0000e-04\n",
            "Epoch 18/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.6551 - accuracy: 0.7721 - val_loss: 0.6734 - val_accuracy: 0.7698 - lr: 5.0000e-04\n",
            "Epoch 19/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.6418 - accuracy: 0.7751 - val_loss: 0.6478 - val_accuracy: 0.7795 - lr: 5.0000e-04\n",
            "Epoch 20/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.6364 - accuracy: 0.7788 - val_loss: 0.6181 - val_accuracy: 0.7879 - lr: 5.0000e-04\n",
            "Epoch 21/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.5904 - accuracy: 0.7946 - val_loss: 0.6247 - val_accuracy: 0.7882 - lr: 1.2500e-04\n",
            "Epoch 22/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.5784 - accuracy: 0.7985 - val_loss: 0.6129 - val_accuracy: 0.7878 - lr: 1.2500e-04\n",
            "Epoch 23/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.5751 - accuracy: 0.7989 - val_loss: 0.6074 - val_accuracy: 0.7919 - lr: 1.2500e-04\n",
            "Epoch 24/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.5685 - accuracy: 0.8023 - val_loss: 0.6025 - val_accuracy: 0.7977 - lr: 1.2500e-04\n",
            "Epoch 25/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.5742 - accuracy: 0.8005 - val_loss: 0.6160 - val_accuracy: 0.7924 - lr: 1.2500e-04\n",
            "Epoch 26/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.5676 - accuracy: 0.8041 - val_loss: 0.6136 - val_accuracy: 0.7909 - lr: 1.2500e-04\n",
            "Epoch 27/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.5636 - accuracy: 0.8003 - val_loss: 0.6176 - val_accuracy: 0.7896 - lr: 1.2500e-04\n",
            "Epoch 28/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.5643 - accuracy: 0.8028 - val_loss: 0.6179 - val_accuracy: 0.7918 - lr: 1.2500e-04\n",
            "Epoch 29/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.5586 - accuracy: 0.8053 - val_loss: 0.5963 - val_accuracy: 0.7966 - lr: 1.2500e-04\n",
            "Epoch 30/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.5578 - accuracy: 0.8070 - val_loss: 0.6032 - val_accuracy: 0.7945 - lr: 1.2500e-04\n",
            "Score for fold 1: loss of 0.6031631827354431; accuracy of 79.44999933242798%\n",
            "Model: \"sequential_21\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_78 (Conv2D)          (None, 32, 32, 32)        896       \n",
            "                                                                 \n",
            " max_pooling2d_21 (MaxPoolin  multiple                 0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_79 (Conv2D)          (None, 16, 16, 64)        18496     \n",
            "                                                                 \n",
            " dropout_21 (Dropout)        (None, 8, 8, 64)          0         \n",
            "                                                                 \n",
            " conv2d_80 (Conv2D)          (None, 8, 8, 128)         73856     \n",
            "                                                                 \n",
            " flatten_21 (Flatten)        (None, 2048)              0         \n",
            "                                                                 \n",
            " dense_21 (Dense)            (None, 10)                20490     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 113,738\n",
            "Trainable params: 113,738\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.5787 - accuracy: 0.4288 - val_loss: 1.2565 - val_accuracy: 0.5535 - lr: 0.0010\n",
            "Epoch 2/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 1.2139 - accuracy: 0.5700 - val_loss: 1.0252 - val_accuracy: 0.6324 - lr: 0.0010\n",
            "Epoch 3/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 1.0665 - accuracy: 0.6260 - val_loss: 0.9669 - val_accuracy: 0.6650 - lr: 0.0010\n",
            "Epoch 4/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.9795 - accuracy: 0.6566 - val_loss: 0.8989 - val_accuracy: 0.6867 - lr: 0.0010\n",
            "Epoch 5/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.9248 - accuracy: 0.6772 - val_loss: 0.8603 - val_accuracy: 0.7061 - lr: 0.0010\n",
            "Epoch 6/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.8849 - accuracy: 0.6914 - val_loss: 0.7866 - val_accuracy: 0.7273 - lr: 0.0010\n",
            "Epoch 7/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.8441 - accuracy: 0.7062 - val_loss: 0.7864 - val_accuracy: 0.7288 - lr: 0.0010\n",
            "Epoch 8/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.8199 - accuracy: 0.7167 - val_loss: 0.7459 - val_accuracy: 0.7416 - lr: 0.0010\n",
            "Epoch 9/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.7975 - accuracy: 0.7232 - val_loss: 0.7706 - val_accuracy: 0.7375 - lr: 0.0010\n",
            "Epoch 10/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.7799 - accuracy: 0.7272 - val_loss: 0.7799 - val_accuracy: 0.7345 - lr: 0.0010\n",
            "Epoch 11/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.7085 - accuracy: 0.7543 - val_loss: 0.7122 - val_accuracy: 0.7588 - lr: 5.0000e-04\n",
            "Epoch 12/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.6946 - accuracy: 0.7585 - val_loss: 0.6711 - val_accuracy: 0.7683 - lr: 5.0000e-04\n",
            "Epoch 13/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.6782 - accuracy: 0.7655 - val_loss: 0.6771 - val_accuracy: 0.7698 - lr: 5.0000e-04\n",
            "Epoch 14/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.6692 - accuracy: 0.7695 - val_loss: 0.6713 - val_accuracy: 0.7733 - lr: 5.0000e-04\n",
            "Epoch 15/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.6611 - accuracy: 0.7682 - val_loss: 0.7061 - val_accuracy: 0.7616 - lr: 5.0000e-04\n",
            "Epoch 16/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.6525 - accuracy: 0.7732 - val_loss: 0.6347 - val_accuracy: 0.7817 - lr: 5.0000e-04\n",
            "Epoch 17/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.6505 - accuracy: 0.7741 - val_loss: 0.6618 - val_accuracy: 0.7741 - lr: 5.0000e-04\n",
            "Epoch 18/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.6438 - accuracy: 0.7758 - val_loss: 0.6444 - val_accuracy: 0.7781 - lr: 5.0000e-04\n",
            "Epoch 19/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.6302 - accuracy: 0.7805 - val_loss: 0.6732 - val_accuracy: 0.7695 - lr: 5.0000e-04\n",
            "Epoch 20/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.6234 - accuracy: 0.7827 - val_loss: 0.6693 - val_accuracy: 0.7753 - lr: 5.0000e-04\n",
            "Epoch 21/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.5818 - accuracy: 0.7983 - val_loss: 0.6361 - val_accuracy: 0.7884 - lr: 1.2500e-04\n",
            "Epoch 22/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.5703 - accuracy: 0.8013 - val_loss: 0.6141 - val_accuracy: 0.7901 - lr: 1.2500e-04\n",
            "Epoch 23/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.5659 - accuracy: 0.8038 - val_loss: 0.6155 - val_accuracy: 0.7917 - lr: 1.2500e-04\n",
            "Epoch 24/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.5620 - accuracy: 0.8052 - val_loss: 0.6171 - val_accuracy: 0.7883 - lr: 1.2500e-04\n",
            "Epoch 25/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.5602 - accuracy: 0.8047 - val_loss: 0.6237 - val_accuracy: 0.7878 - lr: 1.2500e-04\n",
            "Epoch 26/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.5563 - accuracy: 0.8077 - val_loss: 0.6095 - val_accuracy: 0.7930 - lr: 1.2500e-04\n",
            "Epoch 27/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.5511 - accuracy: 0.8062 - val_loss: 0.6063 - val_accuracy: 0.7934 - lr: 1.2500e-04\n",
            "Epoch 28/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.5522 - accuracy: 0.8064 - val_loss: 0.6261 - val_accuracy: 0.7912 - lr: 1.2500e-04\n",
            "Epoch 29/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.5450 - accuracy: 0.8117 - val_loss: 0.6065 - val_accuracy: 0.7940 - lr: 1.2500e-04\n",
            "Epoch 30/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.5475 - accuracy: 0.8091 - val_loss: 0.6087 - val_accuracy: 0.7926 - lr: 1.2500e-04\n",
            "Score for fold 2: loss of 0.6087312698364258; accuracy of 79.25999760627747%\n",
            "Model: \"sequential_22\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_81 (Conv2D)          (None, 32, 32, 32)        896       \n",
            "                                                                 \n",
            " max_pooling2d_22 (MaxPoolin  multiple                 0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_82 (Conv2D)          (None, 16, 16, 64)        18496     \n",
            "                                                                 \n",
            " dropout_22 (Dropout)        (None, 8, 8, 64)          0         \n",
            "                                                                 \n",
            " conv2d_83 (Conv2D)          (None, 8, 8, 128)         73856     \n",
            "                                                                 \n",
            " flatten_22 (Flatten)        (None, 2048)              0         \n",
            "                                                                 \n",
            " dense_22 (Dense)            (None, 10)                20490     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 113,738\n",
            "Trainable params: 113,738\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.5825 - accuracy: 0.4264 - val_loss: 1.2746 - val_accuracy: 0.5397 - lr: 0.0010\n",
            "Epoch 2/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 1.2323 - accuracy: 0.5655 - val_loss: 1.0834 - val_accuracy: 0.6179 - lr: 0.0010\n",
            "Epoch 3/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 1.0851 - accuracy: 0.6179 - val_loss: 0.9805 - val_accuracy: 0.6589 - lr: 0.0010\n",
            "Epoch 4/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.0067 - accuracy: 0.6492 - val_loss: 0.9373 - val_accuracy: 0.6798 - lr: 0.0010\n",
            "Epoch 5/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.9460 - accuracy: 0.6695 - val_loss: 0.8667 - val_accuracy: 0.7067 - lr: 0.0010\n",
            "Epoch 6/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.9022 - accuracy: 0.6864 - val_loss: 0.7887 - val_accuracy: 0.7287 - lr: 0.0010\n",
            "Epoch 7/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.8727 - accuracy: 0.6956 - val_loss: 0.7660 - val_accuracy: 0.7347 - lr: 0.0010\n",
            "Epoch 8/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.8431 - accuracy: 0.7067 - val_loss: 0.8467 - val_accuracy: 0.7114 - lr: 0.0010\n",
            "Epoch 9/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.8217 - accuracy: 0.7138 - val_loss: 0.7956 - val_accuracy: 0.7263 - lr: 0.0010\n",
            "Epoch 10/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.8019 - accuracy: 0.7211 - val_loss: 0.7356 - val_accuracy: 0.7428 - lr: 0.0010\n",
            "Epoch 11/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.7292 - accuracy: 0.7479 - val_loss: 0.6977 - val_accuracy: 0.7585 - lr: 5.0000e-04\n",
            "Epoch 12/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.7124 - accuracy: 0.7518 - val_loss: 0.6785 - val_accuracy: 0.7667 - lr: 5.0000e-04\n",
            "Epoch 13/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.7016 - accuracy: 0.7555 - val_loss: 0.6669 - val_accuracy: 0.7713 - lr: 5.0000e-04\n",
            "Epoch 14/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.6914 - accuracy: 0.7592 - val_loss: 0.6714 - val_accuracy: 0.7691 - lr: 5.0000e-04\n",
            "Epoch 15/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.6863 - accuracy: 0.7643 - val_loss: 0.6601 - val_accuracy: 0.7731 - lr: 5.0000e-04\n",
            "Epoch 16/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.6677 - accuracy: 0.7683 - val_loss: 0.6558 - val_accuracy: 0.7763 - lr: 5.0000e-04\n",
            "Epoch 17/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.6628 - accuracy: 0.7693 - val_loss: 0.6501 - val_accuracy: 0.7799 - lr: 5.0000e-04\n",
            "Epoch 18/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.6574 - accuracy: 0.7713 - val_loss: 0.6653 - val_accuracy: 0.7743 - lr: 5.0000e-04\n",
            "Epoch 19/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.6566 - accuracy: 0.7717 - val_loss: 0.6332 - val_accuracy: 0.7841 - lr: 5.0000e-04\n",
            "Epoch 20/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.6461 - accuracy: 0.7745 - val_loss: 0.6901 - val_accuracy: 0.7700 - lr: 5.0000e-04\n",
            "Epoch 21/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.5937 - accuracy: 0.7947 - val_loss: 0.6115 - val_accuracy: 0.7933 - lr: 1.2500e-04\n",
            "Epoch 22/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.5886 - accuracy: 0.7963 - val_loss: 0.6123 - val_accuracy: 0.7900 - lr: 1.2500e-04\n",
            "Epoch 23/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.5814 - accuracy: 0.7999 - val_loss: 0.6050 - val_accuracy: 0.7949 - lr: 1.2500e-04\n",
            "Epoch 24/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.5763 - accuracy: 0.8011 - val_loss: 0.5969 - val_accuracy: 0.7963 - lr: 1.2500e-04\n",
            "Epoch 25/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.5763 - accuracy: 0.7995 - val_loss: 0.6003 - val_accuracy: 0.7953 - lr: 1.2500e-04\n",
            "Epoch 26/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.5672 - accuracy: 0.8029 - val_loss: 0.6072 - val_accuracy: 0.7937 - lr: 1.2500e-04\n",
            "Epoch 27/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.5677 - accuracy: 0.8031 - val_loss: 0.5926 - val_accuracy: 0.7996 - lr: 1.2500e-04\n",
            "Epoch 28/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.5707 - accuracy: 0.8019 - val_loss: 0.6085 - val_accuracy: 0.7952 - lr: 1.2500e-04\n",
            "Epoch 29/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.5611 - accuracy: 0.8048 - val_loss: 0.5900 - val_accuracy: 0.7996 - lr: 1.2500e-04\n",
            "Epoch 30/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.5572 - accuracy: 0.8067 - val_loss: 0.5817 - val_accuracy: 0.8035 - lr: 1.2500e-04\n",
            "Score for fold 3: loss of 0.5817480683326721; accuracy of 80.34999966621399%\n",
            "Model: \"sequential_23\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_84 (Conv2D)          (None, 32, 32, 32)        896       \n",
            "                                                                 \n",
            " max_pooling2d_23 (MaxPoolin  multiple                 0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_85 (Conv2D)          (None, 16, 16, 64)        18496     \n",
            "                                                                 \n",
            " dropout_23 (Dropout)        (None, 8, 8, 64)          0         \n",
            "                                                                 \n",
            " conv2d_86 (Conv2D)          (None, 8, 8, 128)         73856     \n",
            "                                                                 \n",
            " flatten_23 (Flatten)        (None, 2048)              0         \n",
            "                                                                 \n",
            " dense_23 (Dense)            (None, 10)                20490     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 113,738\n",
            "Trainable params: 113,738\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.5694 - accuracy: 0.4314 - val_loss: 1.2840 - val_accuracy: 0.5424 - lr: 0.0010\n",
            "Epoch 2/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 1.2171 - accuracy: 0.5695 - val_loss: 1.0269 - val_accuracy: 0.6377 - lr: 0.0010\n",
            "Epoch 3/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 1.0626 - accuracy: 0.6269 - val_loss: 0.9427 - val_accuracy: 0.6690 - lr: 0.0010\n",
            "Epoch 4/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.9844 - accuracy: 0.6568 - val_loss: 0.8358 - val_accuracy: 0.7131 - lr: 0.0010\n",
            "Epoch 5/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.9185 - accuracy: 0.6823 - val_loss: 0.8053 - val_accuracy: 0.7164 - lr: 0.0010\n",
            "Epoch 6/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.8802 - accuracy: 0.6941 - val_loss: 0.8142 - val_accuracy: 0.7186 - lr: 0.0010\n",
            "Epoch 7/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.8503 - accuracy: 0.7026 - val_loss: 0.7971 - val_accuracy: 0.7315 - lr: 0.0010\n",
            "Epoch 8/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.8249 - accuracy: 0.7142 - val_loss: 0.7345 - val_accuracy: 0.7484 - lr: 0.0010\n",
            "Epoch 9/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.7977 - accuracy: 0.7228 - val_loss: 0.7792 - val_accuracy: 0.7298 - lr: 0.0010\n",
            "Epoch 10/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.7785 - accuracy: 0.7294 - val_loss: 0.7471 - val_accuracy: 0.7437 - lr: 0.0010\n",
            "Epoch 11/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.7150 - accuracy: 0.7526 - val_loss: 0.7075 - val_accuracy: 0.7570 - lr: 5.0000e-04\n",
            "Epoch 12/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.6897 - accuracy: 0.7630 - val_loss: 0.6577 - val_accuracy: 0.7750 - lr: 5.0000e-04\n",
            "Epoch 13/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.6808 - accuracy: 0.7625 - val_loss: 0.6371 - val_accuracy: 0.7844 - lr: 5.0000e-04\n",
            "Epoch 14/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.6719 - accuracy: 0.7661 - val_loss: 0.6854 - val_accuracy: 0.7675 - lr: 5.0000e-04\n",
            "Epoch 15/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.6551 - accuracy: 0.7725 - val_loss: 0.6399 - val_accuracy: 0.7807 - lr: 5.0000e-04\n",
            "Epoch 16/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.6500 - accuracy: 0.7731 - val_loss: 0.6891 - val_accuracy: 0.7637 - lr: 5.0000e-04\n",
            "Epoch 17/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.6475 - accuracy: 0.7742 - val_loss: 0.6350 - val_accuracy: 0.7836 - lr: 5.0000e-04\n",
            "Epoch 18/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.6327 - accuracy: 0.7801 - val_loss: 0.6167 - val_accuracy: 0.7946 - lr: 5.0000e-04\n",
            "Epoch 19/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.6288 - accuracy: 0.7805 - val_loss: 0.6005 - val_accuracy: 0.7963 - lr: 5.0000e-04\n",
            "Epoch 20/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.6222 - accuracy: 0.7846 - val_loss: 0.6224 - val_accuracy: 0.7871 - lr: 5.0000e-04\n",
            "Epoch 21/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.5713 - accuracy: 0.7993 - val_loss: 0.5842 - val_accuracy: 0.8031 - lr: 1.2500e-04\n",
            "Epoch 22/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.5645 - accuracy: 0.8045 - val_loss: 0.6100 - val_accuracy: 0.7956 - lr: 1.2500e-04\n",
            "Epoch 23/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.5610 - accuracy: 0.8041 - val_loss: 0.6106 - val_accuracy: 0.7965 - lr: 1.2500e-04\n",
            "Epoch 24/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.5556 - accuracy: 0.8074 - val_loss: 0.5731 - val_accuracy: 0.8076 - lr: 1.2500e-04\n",
            "Epoch 25/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.5498 - accuracy: 0.8085 - val_loss: 0.5837 - val_accuracy: 0.7990 - lr: 1.2500e-04\n",
            "Epoch 26/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.5498 - accuracy: 0.8087 - val_loss: 0.5773 - val_accuracy: 0.8039 - lr: 1.2500e-04\n",
            "Epoch 27/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.5485 - accuracy: 0.8095 - val_loss: 0.5822 - val_accuracy: 0.8011 - lr: 1.2500e-04\n",
            "Epoch 28/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.5433 - accuracy: 0.8115 - val_loss: 0.5720 - val_accuracy: 0.8062 - lr: 1.2500e-04\n",
            "Epoch 29/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.5439 - accuracy: 0.8109 - val_loss: 0.5884 - val_accuracy: 0.8002 - lr: 1.2500e-04\n",
            "Epoch 30/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.5426 - accuracy: 0.8120 - val_loss: 0.5747 - val_accuracy: 0.8018 - lr: 1.2500e-04\n",
            "Score for fold 4: loss of 0.5746793746948242; accuracy of 80.1800012588501%\n",
            "Model: \"sequential_24\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_87 (Conv2D)          (None, 32, 32, 32)        896       \n",
            "                                                                 \n",
            " max_pooling2d_24 (MaxPoolin  multiple                 0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_88 (Conv2D)          (None, 16, 16, 64)        18496     \n",
            "                                                                 \n",
            " dropout_24 (Dropout)        (None, 8, 8, 64)          0         \n",
            "                                                                 \n",
            " conv2d_89 (Conv2D)          (None, 8, 8, 128)         73856     \n",
            "                                                                 \n",
            " flatten_24 (Flatten)        (None, 2048)              0         \n",
            "                                                                 \n",
            " dense_24 (Dense)            (None, 10)                20490     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 113,738\n",
            "Trainable params: 113,738\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.5829 - accuracy: 0.4241 - val_loss: 1.2705 - val_accuracy: 0.5488 - lr: 0.0010\n",
            "Epoch 2/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 1.2388 - accuracy: 0.5625 - val_loss: 1.0580 - val_accuracy: 0.6301 - lr: 0.0010\n",
            "Epoch 3/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.0866 - accuracy: 0.6164 - val_loss: 0.9919 - val_accuracy: 0.6563 - lr: 0.0010\n",
            "Epoch 4/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.9973 - accuracy: 0.6521 - val_loss: 0.9354 - val_accuracy: 0.6768 - lr: 0.0010\n",
            "Epoch 5/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.9518 - accuracy: 0.6690 - val_loss: 0.8621 - val_accuracy: 0.7067 - lr: 0.0010\n",
            "Epoch 6/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.9003 - accuracy: 0.6857 - val_loss: 0.8176 - val_accuracy: 0.7237 - lr: 0.0010\n",
            "Epoch 7/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.8698 - accuracy: 0.6978 - val_loss: 0.7952 - val_accuracy: 0.7266 - lr: 0.0010\n",
            "Epoch 8/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.8389 - accuracy: 0.7064 - val_loss: 0.7540 - val_accuracy: 0.7404 - lr: 0.0010\n",
            "Epoch 9/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.8171 - accuracy: 0.7172 - val_loss: 0.7792 - val_accuracy: 0.7332 - lr: 0.0010\n",
            "Epoch 10/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.8013 - accuracy: 0.7210 - val_loss: 0.7829 - val_accuracy: 0.7337 - lr: 0.0010\n",
            "Epoch 11/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.7326 - accuracy: 0.7440 - val_loss: 0.6881 - val_accuracy: 0.7671 - lr: 5.0000e-04\n",
            "Epoch 12/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.7104 - accuracy: 0.7544 - val_loss: 0.7190 - val_accuracy: 0.7585 - lr: 5.0000e-04\n",
            "Epoch 13/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.7006 - accuracy: 0.7558 - val_loss: 0.7026 - val_accuracy: 0.7625 - lr: 5.0000e-04\n",
            "Epoch 14/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.6900 - accuracy: 0.7603 - val_loss: 0.6783 - val_accuracy: 0.7701 - lr: 5.0000e-04\n",
            "Epoch 15/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.6854 - accuracy: 0.7619 - val_loss: 0.6654 - val_accuracy: 0.7730 - lr: 5.0000e-04\n",
            "Epoch 16/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.6731 - accuracy: 0.7660 - val_loss: 0.6843 - val_accuracy: 0.7719 - lr: 5.0000e-04\n",
            "Epoch 17/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.6684 - accuracy: 0.7665 - val_loss: 0.6526 - val_accuracy: 0.7792 - lr: 5.0000e-04\n",
            "Epoch 18/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.6570 - accuracy: 0.7695 - val_loss: 0.6749 - val_accuracy: 0.7724 - lr: 5.0000e-04\n",
            "Epoch 19/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.6515 - accuracy: 0.7733 - val_loss: 0.6395 - val_accuracy: 0.7844 - lr: 5.0000e-04\n",
            "Epoch 20/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.6483 - accuracy: 0.7739 - val_loss: 0.6396 - val_accuracy: 0.7838 - lr: 5.0000e-04\n",
            "Epoch 21/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.5949 - accuracy: 0.7923 - val_loss: 0.6288 - val_accuracy: 0.7890 - lr: 1.2500e-04\n",
            "Epoch 22/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.5846 - accuracy: 0.7969 - val_loss: 0.6259 - val_accuracy: 0.7894 - lr: 1.2500e-04\n",
            "Epoch 23/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.5899 - accuracy: 0.7944 - val_loss: 0.6296 - val_accuracy: 0.7879 - lr: 1.2500e-04\n",
            "Epoch 24/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.5795 - accuracy: 0.7976 - val_loss: 0.6336 - val_accuracy: 0.7876 - lr: 1.2500e-04\n",
            "Epoch 25/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.5746 - accuracy: 0.7996 - val_loss: 0.6256 - val_accuracy: 0.7871 - lr: 1.2500e-04\n",
            "Epoch 26/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.5717 - accuracy: 0.8022 - val_loss: 0.6099 - val_accuracy: 0.7937 - lr: 1.2500e-04\n",
            "Epoch 27/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.5711 - accuracy: 0.8016 - val_loss: 0.6127 - val_accuracy: 0.7932 - lr: 1.2500e-04\n",
            "Epoch 28/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.5747 - accuracy: 0.7997 - val_loss: 0.6440 - val_accuracy: 0.7840 - lr: 1.2500e-04\n",
            "Epoch 29/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.5640 - accuracy: 0.8033 - val_loss: 0.6230 - val_accuracy: 0.7933 - lr: 1.2500e-04\n",
            "Epoch 30/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.5678 - accuracy: 0.8019 - val_loss: 0.6275 - val_accuracy: 0.7905 - lr: 1.2500e-04\n",
            "Score for fold 5: loss of 0.6274961233139038; accuracy of 79.04999852180481%\n",
            "------------------------------------------------------------------------\n",
            "Score per fold\n",
            "------------------------------------------------------------------------\n",
            "> Fold 1 - Loss: 0.6031631827354431 - Accuracy: 79.44999933242798%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 2 - Loss: 0.6087312698364258 - Accuracy: 79.25999760627747%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 3 - Loss: 0.5817480683326721 - Accuracy: 80.34999966621399%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 4 - Loss: 0.5746793746948242 - Accuracy: 80.1800012588501%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 5 - Loss: 0.6274961233139038 - Accuracy: 79.04999852180481%\n",
            "------------------------------------------------------------------------\n",
            "Average scores for all folds:\n",
            "> Accuracy: 79.65799927711487 (+- 0.514331572603813)\n",
            "> Loss: 0.5991636037826538\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3 hidden layers, Narrow and Kernel size 3x3"
      ],
      "metadata": {
        "id": "Q7Vj56e3ZJu9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "acc_per_fold = []\n",
        "loss_per_fold = []\n",
        "\n",
        "with tf.device('/device:GPU:0'):\n",
        "  fold_no = 1\n",
        "  for train, valid in kfold.split(trainX, trainy):\n",
        "\n",
        "    model = CNN_Model(batch_size=32, depth=3, hidden_neurons_1=4, hidden_neurons_2=32, hidden_neurons_3=64, kernel_size_1=3, kernel_size_2=3, kernel_size_3=3, \n",
        "               dropout_rate=0.2, batch_norm=False, pooling='max', layer_activation_1='relu', layer_activation_2='relu', layer_activation_3='relu',\n",
        "               kernel_initialiser='glorot_uniform', bias_initialiser='zeros', optimiser='Adam', weight_decay=None, learning_rate=1e-3, output_activation='softmax', SGD_momentum=0) \n",
        "  \n",
        "    train_gen = tf.keras.preprocessing.image.ImageDataGenerator(width_shift_range=0.1,\n",
        "      height_shift_range=0.1,\n",
        "      horizontal_flip=True)\n",
        "    train_gen.fit(trainX[train])\n",
        "  \n",
        "    print('------------------------------------------------------------------------')\n",
        "    print(f'Training for fold {fold_no} ...')\n",
        "\n",
        "    history = model.train(\n",
        "              train_gen,\n",
        "                trainX[train],\n",
        "                  trainy[train],\n",
        "                trainX[valid],\n",
        "                  trainy[valid],\n",
        "                  lr_rate_type='step')\n",
        "\n",
        "    scores = model.evaluate(trainX[valid], trainy[valid])\n",
        "\n",
        "    print(f'Score for fold {fold_no}: loss of {scores[0]}; accuracy of {scores[1]*100}%')\n",
        "    acc_per_fold.append(scores[1] * 100)\n",
        "    loss_per_fold.append(scores[0])\n",
        "\n",
        "    fold_no+=1\n",
        "\n",
        "# == Provide average scores ==\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Score per fold')\n",
        "for i in range(0, len(acc_per_fold)):\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Average scores for all folds:')\n",
        "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
        "print(f'> Loss: {np.mean(loss_per_fold)}')"
      ],
      "metadata": {
        "id": "xWXytnepZNYN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "532c1601-8dbf-40a3-edc6-60f41ad764fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_25\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_90 (Conv2D)          (None, 32, 32, 4)         112       \n",
            "                                                                 \n",
            " max_pooling2d_25 (MaxPoolin  multiple                 0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_91 (Conv2D)          (None, 16, 16, 32)        1184      \n",
            "                                                                 \n",
            " dropout_25 (Dropout)        (None, 8, 8, 32)          0         \n",
            "                                                                 \n",
            " conv2d_92 (Conv2D)          (None, 8, 8, 64)          18496     \n",
            "                                                                 \n",
            " flatten_25 (Flatten)        (None, 1024)              0         \n",
            "                                                                 \n",
            " dense_25 (Dense)            (None, 10)                10250     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 30,042\n",
            "Trainable params: 30,042\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.7255 - accuracy: 0.3783 - val_loss: 1.4563 - val_accuracy: 0.4756 - lr: 0.0010\n",
            "Epoch 2/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.4452 - accuracy: 0.4843 - val_loss: 1.3674 - val_accuracy: 0.5076 - lr: 0.0010\n",
            "Epoch 3/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.3303 - accuracy: 0.5289 - val_loss: 1.1990 - val_accuracy: 0.5680 - lr: 0.0010\n",
            "Epoch 4/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 1.2559 - accuracy: 0.5561 - val_loss: 1.1522 - val_accuracy: 0.5895 - lr: 0.0010\n",
            "Epoch 5/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 1.2037 - accuracy: 0.5766 - val_loss: 1.0771 - val_accuracy: 0.6191 - lr: 0.0010\n",
            "Epoch 6/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.1587 - accuracy: 0.5921 - val_loss: 1.0945 - val_accuracy: 0.6104 - lr: 0.0010\n",
            "Epoch 7/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 1.1161 - accuracy: 0.6093 - val_loss: 1.0114 - val_accuracy: 0.6441 - lr: 0.0010\n",
            "Epoch 8/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.0905 - accuracy: 0.6181 - val_loss: 0.9844 - val_accuracy: 0.6555 - lr: 0.0010\n",
            "Epoch 9/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.0663 - accuracy: 0.6285 - val_loss: 1.0025 - val_accuracy: 0.6488 - lr: 0.0010\n",
            "Epoch 10/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 1.0452 - accuracy: 0.6355 - val_loss: 0.9394 - val_accuracy: 0.6723 - lr: 0.0010\n",
            "Epoch 11/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 1.0012 - accuracy: 0.6510 - val_loss: 0.9357 - val_accuracy: 0.6692 - lr: 5.0000e-04\n",
            "Epoch 12/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.9849 - accuracy: 0.6539 - val_loss: 0.8977 - val_accuracy: 0.6859 - lr: 5.0000e-04\n",
            "Epoch 13/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.9754 - accuracy: 0.6596 - val_loss: 0.9213 - val_accuracy: 0.6849 - lr: 5.0000e-04\n",
            "Epoch 14/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.9705 - accuracy: 0.6632 - val_loss: 0.9078 - val_accuracy: 0.6816 - lr: 5.0000e-04\n",
            "Epoch 15/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.9625 - accuracy: 0.6641 - val_loss: 0.8751 - val_accuracy: 0.6958 - lr: 5.0000e-04\n",
            "Epoch 16/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.9497 - accuracy: 0.6683 - val_loss: 0.9049 - val_accuracy: 0.6807 - lr: 5.0000e-04\n",
            "Epoch 17/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.9455 - accuracy: 0.6703 - val_loss: 0.8580 - val_accuracy: 0.7007 - lr: 5.0000e-04\n",
            "Epoch 18/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.9425 - accuracy: 0.6723 - val_loss: 0.8694 - val_accuracy: 0.6987 - lr: 5.0000e-04\n",
            "Epoch 19/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.9335 - accuracy: 0.6754 - val_loss: 0.8591 - val_accuracy: 0.7041 - lr: 5.0000e-04\n",
            "Epoch 20/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.9288 - accuracy: 0.6783 - val_loss: 0.9016 - val_accuracy: 0.6854 - lr: 5.0000e-04\n",
            "Epoch 21/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.9027 - accuracy: 0.6873 - val_loss: 0.8671 - val_accuracy: 0.7002 - lr: 1.2500e-04\n",
            "Epoch 22/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.8960 - accuracy: 0.6884 - val_loss: 0.8483 - val_accuracy: 0.7052 - lr: 1.2500e-04\n",
            "Epoch 23/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.8920 - accuracy: 0.6895 - val_loss: 0.8631 - val_accuracy: 0.6988 - lr: 1.2500e-04\n",
            "Epoch 24/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.8941 - accuracy: 0.6879 - val_loss: 0.8548 - val_accuracy: 0.7063 - lr: 1.2500e-04\n",
            "Epoch 25/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.8934 - accuracy: 0.6932 - val_loss: 0.8575 - val_accuracy: 0.7045 - lr: 1.2500e-04\n",
            "Epoch 26/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.8871 - accuracy: 0.6933 - val_loss: 0.8359 - val_accuracy: 0.7090 - lr: 1.2500e-04\n",
            "Epoch 27/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.8881 - accuracy: 0.6896 - val_loss: 0.8374 - val_accuracy: 0.7118 - lr: 1.2500e-04\n",
            "Epoch 28/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.8889 - accuracy: 0.6912 - val_loss: 0.8533 - val_accuracy: 0.7047 - lr: 1.2500e-04\n",
            "Epoch 29/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.8892 - accuracy: 0.6926 - val_loss: 0.8348 - val_accuracy: 0.7101 - lr: 1.2500e-04\n",
            "Epoch 30/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.8836 - accuracy: 0.6927 - val_loss: 0.8408 - val_accuracy: 0.7102 - lr: 1.2500e-04\n",
            "Score for fold 1: loss of 0.8408291935920715; accuracy of 71.02000117301941%\n",
            "Model: \"sequential_26\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_93 (Conv2D)          (None, 32, 32, 4)         112       \n",
            "                                                                 \n",
            " max_pooling2d_26 (MaxPoolin  multiple                 0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_94 (Conv2D)          (None, 16, 16, 32)        1184      \n",
            "                                                                 \n",
            " dropout_26 (Dropout)        (None, 8, 8, 32)          0         \n",
            "                                                                 \n",
            " conv2d_95 (Conv2D)          (None, 8, 8, 64)          18496     \n",
            "                                                                 \n",
            " flatten_26 (Flatten)        (None, 1024)              0         \n",
            "                                                                 \n",
            " dense_26 (Dense)            (None, 10)                10250     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 30,042\n",
            "Trainable params: 30,042\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.7056 - accuracy: 0.3858 - val_loss: 1.4707 - val_accuracy: 0.4743 - lr: 0.0010\n",
            "Epoch 2/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.4202 - accuracy: 0.4927 - val_loss: 1.3089 - val_accuracy: 0.5419 - lr: 0.0010\n",
            "Epoch 3/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 1.2972 - accuracy: 0.5407 - val_loss: 1.2365 - val_accuracy: 0.5679 - lr: 0.0010\n",
            "Epoch 4/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 1.2177 - accuracy: 0.5709 - val_loss: 1.1104 - val_accuracy: 0.6169 - lr: 0.0010\n",
            "Epoch 5/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 1.1627 - accuracy: 0.5904 - val_loss: 1.1100 - val_accuracy: 0.6123 - lr: 0.0010\n",
            "Epoch 6/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.1242 - accuracy: 0.6064 - val_loss: 1.0318 - val_accuracy: 0.6441 - lr: 0.0010\n",
            "Epoch 7/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.0966 - accuracy: 0.6160 - val_loss: 1.0394 - val_accuracy: 0.6281 - lr: 0.0010\n",
            "Epoch 8/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.0650 - accuracy: 0.6292 - val_loss: 0.9805 - val_accuracy: 0.6596 - lr: 0.0010\n",
            "Epoch 9/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 1.0496 - accuracy: 0.6324 - val_loss: 0.9882 - val_accuracy: 0.6614 - lr: 0.0010\n",
            "Epoch 10/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 1.0279 - accuracy: 0.6392 - val_loss: 0.9477 - val_accuracy: 0.6750 - lr: 0.0010\n",
            "Epoch 11/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.9760 - accuracy: 0.6598 - val_loss: 0.9561 - val_accuracy: 0.6662 - lr: 5.0000e-04\n",
            "Epoch 12/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.9708 - accuracy: 0.6626 - val_loss: 0.9024 - val_accuracy: 0.6893 - lr: 5.0000e-04\n",
            "Epoch 13/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.9583 - accuracy: 0.6636 - val_loss: 0.9084 - val_accuracy: 0.6861 - lr: 5.0000e-04\n",
            "Epoch 14/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.9498 - accuracy: 0.6711 - val_loss: 0.9063 - val_accuracy: 0.6856 - lr: 5.0000e-04\n",
            "Epoch 15/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.9459 - accuracy: 0.6721 - val_loss: 0.8989 - val_accuracy: 0.6923 - lr: 5.0000e-04\n",
            "Epoch 16/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.9363 - accuracy: 0.6753 - val_loss: 0.9484 - val_accuracy: 0.6713 - lr: 5.0000e-04\n",
            "Epoch 17/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.9371 - accuracy: 0.6746 - val_loss: 0.8949 - val_accuracy: 0.6852 - lr: 5.0000e-04\n",
            "Epoch 18/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.9314 - accuracy: 0.6755 - val_loss: 0.8843 - val_accuracy: 0.6932 - lr: 5.0000e-04\n",
            "Epoch 19/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.9228 - accuracy: 0.6787 - val_loss: 0.8945 - val_accuracy: 0.6906 - lr: 5.0000e-04\n",
            "Epoch 20/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.9130 - accuracy: 0.6809 - val_loss: 0.8696 - val_accuracy: 0.6971 - lr: 5.0000e-04\n",
            "Epoch 21/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.8893 - accuracy: 0.6909 - val_loss: 0.8579 - val_accuracy: 0.7023 - lr: 1.2500e-04\n",
            "Epoch 22/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.8798 - accuracy: 0.6931 - val_loss: 0.8509 - val_accuracy: 0.7045 - lr: 1.2500e-04\n",
            "Epoch 23/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.8763 - accuracy: 0.6952 - val_loss: 0.8621 - val_accuracy: 0.7021 - lr: 1.2500e-04\n",
            "Epoch 24/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.8792 - accuracy: 0.6912 - val_loss: 0.8847 - val_accuracy: 0.6949 - lr: 1.2500e-04\n",
            "Epoch 25/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.8771 - accuracy: 0.6944 - val_loss: 0.8605 - val_accuracy: 0.7027 - lr: 1.2500e-04\n",
            "Epoch 26/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.8767 - accuracy: 0.6944 - val_loss: 0.8669 - val_accuracy: 0.6969 - lr: 1.2500e-04\n",
            "Epoch 27/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.8732 - accuracy: 0.6969 - val_loss: 0.8587 - val_accuracy: 0.7000 - lr: 1.2500e-04\n",
            "Epoch 28/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.8724 - accuracy: 0.6971 - val_loss: 0.8609 - val_accuracy: 0.7007 - lr: 1.2500e-04\n",
            "Epoch 29/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.8712 - accuracy: 0.6942 - val_loss: 0.8589 - val_accuracy: 0.7013 - lr: 1.2500e-04\n",
            "Epoch 30/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.8687 - accuracy: 0.7011 - val_loss: 0.8634 - val_accuracy: 0.6989 - lr: 1.2500e-04\n",
            "Score for fold 2: loss of 0.8633545637130737; accuracy of 69.88999843597412%\n",
            "Model: \"sequential_27\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_96 (Conv2D)          (None, 32, 32, 4)         112       \n",
            "                                                                 \n",
            " max_pooling2d_27 (MaxPoolin  multiple                 0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_97 (Conv2D)          (None, 16, 16, 32)        1184      \n",
            "                                                                 \n",
            " dropout_27 (Dropout)        (None, 8, 8, 32)          0         \n",
            "                                                                 \n",
            " conv2d_98 (Conv2D)          (None, 8, 8, 64)          18496     \n",
            "                                                                 \n",
            " flatten_27 (Flatten)        (None, 1024)              0         \n",
            "                                                                 \n",
            " dense_27 (Dense)            (None, 10)                10250     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 30,042\n",
            "Trainable params: 30,042\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.7616 - accuracy: 0.3636 - val_loss: 1.4891 - val_accuracy: 0.4716 - lr: 0.0010\n",
            "Epoch 2/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 1.4436 - accuracy: 0.4847 - val_loss: 1.2931 - val_accuracy: 0.5474 - lr: 0.0010\n",
            "Epoch 3/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 1.2964 - accuracy: 0.5427 - val_loss: 1.2421 - val_accuracy: 0.5756 - lr: 0.0010\n",
            "Epoch 4/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 1.2129 - accuracy: 0.5748 - val_loss: 1.0980 - val_accuracy: 0.6174 - lr: 0.0010\n",
            "Epoch 5/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.1610 - accuracy: 0.5913 - val_loss: 1.0834 - val_accuracy: 0.6275 - lr: 0.0010\n",
            "Epoch 6/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 1.1165 - accuracy: 0.6092 - val_loss: 1.0459 - val_accuracy: 0.6397 - lr: 0.0010\n",
            "Epoch 7/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.0850 - accuracy: 0.6203 - val_loss: 0.9814 - val_accuracy: 0.6680 - lr: 0.0010\n",
            "Epoch 8/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 1.0576 - accuracy: 0.6299 - val_loss: 0.9905 - val_accuracy: 0.6622 - lr: 0.0010\n",
            "Epoch 9/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 1.0351 - accuracy: 0.6379 - val_loss: 0.9447 - val_accuracy: 0.6761 - lr: 0.0010\n",
            "Epoch 10/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.0192 - accuracy: 0.6435 - val_loss: 0.9391 - val_accuracy: 0.6740 - lr: 0.0010\n",
            "Epoch 11/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.9755 - accuracy: 0.6606 - val_loss: 0.9304 - val_accuracy: 0.6846 - lr: 5.0000e-04\n",
            "Epoch 12/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.9586 - accuracy: 0.6651 - val_loss: 0.8972 - val_accuracy: 0.6956 - lr: 5.0000e-04\n",
            "Epoch 13/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.9499 - accuracy: 0.6680 - val_loss: 0.9078 - val_accuracy: 0.6884 - lr: 5.0000e-04\n",
            "Epoch 14/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.9435 - accuracy: 0.6718 - val_loss: 0.9182 - val_accuracy: 0.6892 - lr: 5.0000e-04\n",
            "Epoch 15/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.9346 - accuracy: 0.6734 - val_loss: 0.8714 - val_accuracy: 0.7026 - lr: 5.0000e-04\n",
            "Epoch 16/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.9330 - accuracy: 0.6751 - val_loss: 0.9078 - val_accuracy: 0.6891 - lr: 5.0000e-04\n",
            "Epoch 17/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.9204 - accuracy: 0.6793 - val_loss: 0.8619 - val_accuracy: 0.7065 - lr: 5.0000e-04\n",
            "Epoch 18/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.9188 - accuracy: 0.6789 - val_loss: 0.9140 - val_accuracy: 0.6922 - lr: 5.0000e-04\n",
            "Epoch 19/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.9118 - accuracy: 0.6839 - val_loss: 0.8519 - val_accuracy: 0.7101 - lr: 5.0000e-04\n",
            "Epoch 20/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.9088 - accuracy: 0.6830 - val_loss: 0.9008 - val_accuracy: 0.6900 - lr: 5.0000e-04\n",
            "Epoch 21/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.8827 - accuracy: 0.6939 - val_loss: 0.8655 - val_accuracy: 0.7060 - lr: 1.2500e-04\n",
            "Epoch 22/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.8803 - accuracy: 0.6947 - val_loss: 0.8595 - val_accuracy: 0.7069 - lr: 1.2500e-04\n",
            "Epoch 23/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.8748 - accuracy: 0.6966 - val_loss: 0.8836 - val_accuracy: 0.7011 - lr: 1.2500e-04\n",
            "Epoch 24/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.8714 - accuracy: 0.6979 - val_loss: 0.8616 - val_accuracy: 0.7085 - lr: 1.2500e-04\n",
            "Epoch 25/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.8697 - accuracy: 0.6976 - val_loss: 0.8431 - val_accuracy: 0.7114 - lr: 1.2500e-04\n",
            "Epoch 26/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.8706 - accuracy: 0.6973 - val_loss: 0.8244 - val_accuracy: 0.7229 - lr: 1.2500e-04\n",
            "Epoch 27/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.8673 - accuracy: 0.7020 - val_loss: 0.8417 - val_accuracy: 0.7127 - lr: 1.2500e-04\n",
            "Epoch 28/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.8647 - accuracy: 0.6999 - val_loss: 0.8365 - val_accuracy: 0.7144 - lr: 1.2500e-04\n",
            "Epoch 29/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.8714 - accuracy: 0.6985 - val_loss: 0.8433 - val_accuracy: 0.7137 - lr: 1.2500e-04\n",
            "Epoch 30/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.8652 - accuracy: 0.7013 - val_loss: 0.8515 - val_accuracy: 0.7103 - lr: 1.2500e-04\n",
            "Score for fold 3: loss of 0.8515458703041077; accuracy of 71.03000283241272%\n",
            "Model: \"sequential_28\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_99 (Conv2D)          (None, 32, 32, 4)         112       \n",
            "                                                                 \n",
            " max_pooling2d_28 (MaxPoolin  multiple                 0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_100 (Conv2D)         (None, 16, 16, 32)        1184      \n",
            "                                                                 \n",
            " dropout_28 (Dropout)        (None, 8, 8, 32)          0         \n",
            "                                                                 \n",
            " conv2d_101 (Conv2D)         (None, 8, 8, 64)          18496     \n",
            "                                                                 \n",
            " flatten_28 (Flatten)        (None, 1024)              0         \n",
            "                                                                 \n",
            " dense_28 (Dense)            (None, 10)                10250     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 30,042\n",
            "Trainable params: 30,042\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.7235 - accuracy: 0.3762 - val_loss: 1.4467 - val_accuracy: 0.4767 - lr: 0.0010\n",
            "Epoch 2/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 1.4763 - accuracy: 0.4677 - val_loss: 1.3568 - val_accuracy: 0.5138 - lr: 0.0010\n",
            "Epoch 3/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.3652 - accuracy: 0.5097 - val_loss: 1.2422 - val_accuracy: 0.5629 - lr: 0.0010\n",
            "Epoch 4/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.2882 - accuracy: 0.5406 - val_loss: 1.1839 - val_accuracy: 0.5829 - lr: 0.0010\n",
            "Epoch 5/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.2373 - accuracy: 0.5587 - val_loss: 1.1831 - val_accuracy: 0.5845 - lr: 0.0010\n",
            "Epoch 6/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 1.1935 - accuracy: 0.5779 - val_loss: 1.0879 - val_accuracy: 0.6165 - lr: 0.0010\n",
            "Epoch 7/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 1.1589 - accuracy: 0.5909 - val_loss: 1.1386 - val_accuracy: 0.6071 - lr: 0.0010\n",
            "Epoch 8/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 1.1286 - accuracy: 0.6034 - val_loss: 1.0654 - val_accuracy: 0.6273 - lr: 0.0010\n",
            "Epoch 9/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 1.0980 - accuracy: 0.6127 - val_loss: 1.0226 - val_accuracy: 0.6422 - lr: 0.0010\n",
            "Epoch 10/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.0756 - accuracy: 0.6227 - val_loss: 1.0048 - val_accuracy: 0.6526 - lr: 0.0010\n",
            "Epoch 11/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.0380 - accuracy: 0.6373 - val_loss: 0.9763 - val_accuracy: 0.6608 - lr: 5.0000e-04\n",
            "Epoch 12/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.0170 - accuracy: 0.6436 - val_loss: 0.9365 - val_accuracy: 0.6775 - lr: 5.0000e-04\n",
            "Epoch 13/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 1.0066 - accuracy: 0.6456 - val_loss: 1.0209 - val_accuracy: 0.6503 - lr: 5.0000e-04\n",
            "Epoch 14/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.9975 - accuracy: 0.6481 - val_loss: 0.9147 - val_accuracy: 0.6878 - lr: 5.0000e-04\n",
            "Epoch 15/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.9869 - accuracy: 0.6522 - val_loss: 0.9243 - val_accuracy: 0.6788 - lr: 5.0000e-04\n",
            "Epoch 16/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.9813 - accuracy: 0.6552 - val_loss: 0.9175 - val_accuracy: 0.6873 - lr: 5.0000e-04\n",
            "Epoch 17/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.9755 - accuracy: 0.6573 - val_loss: 0.9140 - val_accuracy: 0.6863 - lr: 5.0000e-04\n",
            "Epoch 18/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.9680 - accuracy: 0.6593 - val_loss: 0.9457 - val_accuracy: 0.6716 - lr: 5.0000e-04\n",
            "Epoch 19/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.9618 - accuracy: 0.6642 - val_loss: 0.9068 - val_accuracy: 0.6857 - lr: 5.0000e-04\n",
            "Epoch 20/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.9603 - accuracy: 0.6662 - val_loss: 0.8801 - val_accuracy: 0.6988 - lr: 5.0000e-04\n",
            "Epoch 21/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.9258 - accuracy: 0.6762 - val_loss: 0.8951 - val_accuracy: 0.6921 - lr: 1.2500e-04\n",
            "Epoch 22/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.9189 - accuracy: 0.6799 - val_loss: 0.8746 - val_accuracy: 0.7003 - lr: 1.2500e-04\n",
            "Epoch 23/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.9182 - accuracy: 0.6788 - val_loss: 0.8900 - val_accuracy: 0.6929 - lr: 1.2500e-04\n",
            "Epoch 24/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.9233 - accuracy: 0.6772 - val_loss: 0.9022 - val_accuracy: 0.6881 - lr: 1.2500e-04\n",
            "Epoch 25/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.9157 - accuracy: 0.6802 - val_loss: 0.8766 - val_accuracy: 0.6963 - lr: 1.2500e-04\n",
            "Epoch 26/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.9139 - accuracy: 0.6828 - val_loss: 0.8620 - val_accuracy: 0.7026 - lr: 1.2500e-04\n",
            "Epoch 27/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.9088 - accuracy: 0.6846 - val_loss: 0.8588 - val_accuracy: 0.7021 - lr: 1.2500e-04\n",
            "Epoch 28/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.9093 - accuracy: 0.6823 - val_loss: 0.8648 - val_accuracy: 0.6993 - lr: 1.2500e-04\n",
            "Epoch 29/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.9048 - accuracy: 0.6847 - val_loss: 0.8630 - val_accuracy: 0.7056 - lr: 1.2500e-04\n",
            "Epoch 30/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.9031 - accuracy: 0.6840 - val_loss: 0.8926 - val_accuracy: 0.6932 - lr: 1.2500e-04\n",
            "Score for fold 4: loss of 0.8925544023513794; accuracy of 69.31999921798706%\n",
            "Model: \"sequential_29\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_102 (Conv2D)         (None, 32, 32, 4)         112       \n",
            "                                                                 \n",
            " max_pooling2d_29 (MaxPoolin  multiple                 0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_103 (Conv2D)         (None, 16, 16, 32)        1184      \n",
            "                                                                 \n",
            " dropout_29 (Dropout)        (None, 8, 8, 32)          0         \n",
            "                                                                 \n",
            " conv2d_104 (Conv2D)         (None, 8, 8, 64)          18496     \n",
            "                                                                 \n",
            " flatten_29 (Flatten)        (None, 1024)              0         \n",
            "                                                                 \n",
            " dense_29 (Dense)            (None, 10)                10250     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 30,042\n",
            "Trainable params: 30,042\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.7145 - accuracy: 0.3857 - val_loss: 1.4503 - val_accuracy: 0.4860 - lr: 0.0010\n",
            "Epoch 2/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 1.4395 - accuracy: 0.4870 - val_loss: 1.3169 - val_accuracy: 0.5310 - lr: 0.0010\n",
            "Epoch 3/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.3169 - accuracy: 0.5310 - val_loss: 1.1995 - val_accuracy: 0.5788 - lr: 0.0010\n",
            "Epoch 4/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 1.2518 - accuracy: 0.5573 - val_loss: 1.1537 - val_accuracy: 0.5963 - lr: 0.0010\n",
            "Epoch 5/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.2022 - accuracy: 0.5782 - val_loss: 1.0726 - val_accuracy: 0.6285 - lr: 0.0010\n",
            "Epoch 6/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 1.1536 - accuracy: 0.5955 - val_loss: 1.0595 - val_accuracy: 0.6277 - lr: 0.0010\n",
            "Epoch 7/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 1.1227 - accuracy: 0.6079 - val_loss: 1.0347 - val_accuracy: 0.6429 - lr: 0.0010\n",
            "Epoch 8/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.0968 - accuracy: 0.6150 - val_loss: 1.0130 - val_accuracy: 0.6479 - lr: 0.0010\n",
            "Epoch 9/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 1.0807 - accuracy: 0.6222 - val_loss: 1.0214 - val_accuracy: 0.6453 - lr: 0.0010\n",
            "Epoch 10/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.0579 - accuracy: 0.6295 - val_loss: 1.0078 - val_accuracy: 0.6522 - lr: 0.0010\n",
            "Epoch 11/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.0145 - accuracy: 0.6462 - val_loss: 0.9464 - val_accuracy: 0.6736 - lr: 5.0000e-04\n",
            "Epoch 12/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.9993 - accuracy: 0.6521 - val_loss: 0.9613 - val_accuracy: 0.6701 - lr: 5.0000e-04\n",
            "Epoch 13/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.9938 - accuracy: 0.6544 - val_loss: 0.9675 - val_accuracy: 0.6611 - lr: 5.0000e-04\n",
            "Epoch 14/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.9826 - accuracy: 0.6594 - val_loss: 0.9407 - val_accuracy: 0.6717 - lr: 5.0000e-04\n",
            "Epoch 15/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.9774 - accuracy: 0.6603 - val_loss: 0.9183 - val_accuracy: 0.6838 - lr: 5.0000e-04\n",
            "Epoch 16/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.9696 - accuracy: 0.6607 - val_loss: 0.9222 - val_accuracy: 0.6833 - lr: 5.0000e-04\n",
            "Epoch 17/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.9593 - accuracy: 0.6654 - val_loss: 0.9166 - val_accuracy: 0.6807 - lr: 5.0000e-04\n",
            "Epoch 18/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.9510 - accuracy: 0.6681 - val_loss: 0.9081 - val_accuracy: 0.6841 - lr: 5.0000e-04\n",
            "Epoch 19/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.9529 - accuracy: 0.6678 - val_loss: 0.8724 - val_accuracy: 0.7019 - lr: 5.0000e-04\n",
            "Epoch 20/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.9423 - accuracy: 0.6705 - val_loss: 0.8995 - val_accuracy: 0.6870 - lr: 5.0000e-04\n",
            "Epoch 21/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.9112 - accuracy: 0.6847 - val_loss: 0.8688 - val_accuracy: 0.7011 - lr: 1.2500e-04\n",
            "Epoch 22/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.9092 - accuracy: 0.6830 - val_loss: 0.8827 - val_accuracy: 0.6976 - lr: 1.2500e-04\n",
            "Epoch 23/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.9067 - accuracy: 0.6833 - val_loss: 0.8689 - val_accuracy: 0.7005 - lr: 1.2500e-04\n",
            "Epoch 24/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.9031 - accuracy: 0.6870 - val_loss: 0.8756 - val_accuracy: 0.6990 - lr: 1.2500e-04\n",
            "Epoch 25/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.8999 - accuracy: 0.6864 - val_loss: 0.8649 - val_accuracy: 0.6993 - lr: 1.2500e-04\n",
            "Epoch 26/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.9024 - accuracy: 0.6870 - val_loss: 0.8756 - val_accuracy: 0.6965 - lr: 1.2500e-04\n",
            "Epoch 27/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.8965 - accuracy: 0.6876 - val_loss: 0.8730 - val_accuracy: 0.6977 - lr: 1.2500e-04\n",
            "Epoch 28/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.8976 - accuracy: 0.6862 - val_loss: 0.8781 - val_accuracy: 0.6960 - lr: 1.2500e-04\n",
            "Epoch 29/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.8939 - accuracy: 0.6883 - val_loss: 0.8644 - val_accuracy: 0.7018 - lr: 1.2500e-04\n",
            "Epoch 30/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.8975 - accuracy: 0.6866 - val_loss: 0.8585 - val_accuracy: 0.6995 - lr: 1.2500e-04\n",
            "Score for fold 5: loss of 0.8585060834884644; accuracy of 69.9500024318695%\n",
            "------------------------------------------------------------------------\n",
            "Score per fold\n",
            "------------------------------------------------------------------------\n",
            "> Fold 1 - Loss: 0.8408291935920715 - Accuracy: 71.02000117301941%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 2 - Loss: 0.8633545637130737 - Accuracy: 69.88999843597412%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 3 - Loss: 0.8515458703041077 - Accuracy: 71.03000283241272%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 4 - Loss: 0.8925544023513794 - Accuracy: 69.31999921798706%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 5 - Loss: 0.8585060834884644 - Accuracy: 69.9500024318695%\n",
            "------------------------------------------------------------------------\n",
            "Average scores for all folds:\n",
            "> Accuracy: 70.24200081825256 (+- 0.6760898478767384)\n",
            "> Loss: 0.8613580226898193\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3 hidden layers, Wide and Kernel size 5x5"
      ],
      "metadata": {
        "id": "tNW8w6rlZNtV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "acc_per_fold = []\n",
        "loss_per_fold = []\n",
        "\n",
        "with tf.device('/device:GPU:0'):\n",
        "  fold_no = 1\n",
        "  for train, valid in kfold.split(trainX, trainy):\n",
        "\n",
        "    model = CNN_Model(batch_size=32, depth=3, hidden_neurons_1=64, hidden_neurons_2=128, hidden_neurons_3=256, kernel_size_1=5, kernel_size_2=5, kernel_size_3=5, \n",
        "               dropout_rate=0.2, batch_norm=False, pooling='max', layer_activation_1='relu', layer_activation_2='relu', layer_activation_3='relu',\n",
        "               kernel_initialiser='glorot_uniform', bias_initialiser='zeros', optimiser='Adam', weight_decay=None, learning_rate=1e-3, output_activation='softmax', SGD_momentum=0) \n",
        "  \n",
        "    train_gen = tf.keras.preprocessing.image.ImageDataGenerator(width_shift_range=0.1,\n",
        "      height_shift_range=0.1,\n",
        "      horizontal_flip=True)\n",
        "    train_gen.fit(trainX[train])\n",
        "  \n",
        "    print('------------------------------------------------------------------------')\n",
        "    print(f'Training for fold {fold_no} ...')\n",
        "\n",
        "    history = model.train(\n",
        "              train_gen,\n",
        "                trainX[train],\n",
        "                  trainy[train],\n",
        "                trainX[valid],\n",
        "                  trainy[valid],\n",
        "                  lr_rate_type='step')\n",
        "\n",
        "    scores = model.evaluate(trainX[valid], trainy[valid])\n",
        "\n",
        "    print(f'Score for fold {fold_no}: loss of {scores[0]}; accuracy of {scores[1]*100}%')\n",
        "    acc_per_fold.append(scores[1] * 100)\n",
        "    loss_per_fold.append(scores[0])\n",
        "\n",
        "    fold_no+=1\n",
        "\n",
        "# == Provide average scores ==\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Score per fold')\n",
        "for i in range(0, len(acc_per_fold)):\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Average scores for all folds:')\n",
        "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
        "print(f'> Loss: {np.mean(loss_per_fold)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "2hkxvEEmZRFs",
        "outputId": "6163bc4f-c720-4005-bc4e-d9cca15fcea7"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 32, 32, 64)        4864      \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  multiple                 0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 16, 16, 128)       204928    \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 8, 8, 128)         0         \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 8, 8, 256)         819456    \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 4096)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 10)                40970     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,070,218\n",
            "Trainable params: 1,070,218\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/30\n",
            "1250/1250 [==============================] - 28s 14ms/step - loss: 1.5766 - accuracy: 0.4272 - val_loss: 1.2834 - val_accuracy: 0.5476 - lr: 0.0010\n",
            "Epoch 2/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.2440 - accuracy: 0.5605 - val_loss: 1.0998 - val_accuracy: 0.6088 - lr: 0.0010\n",
            "Epoch 3/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.1031 - accuracy: 0.6104 - val_loss: 0.9772 - val_accuracy: 0.6586 - lr: 0.0010\n",
            "Epoch 4/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 1.0016 - accuracy: 0.6453 - val_loss: 0.9171 - val_accuracy: 0.6760 - lr: 0.0010\n",
            "Epoch 5/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.9337 - accuracy: 0.6727 - val_loss: 0.8409 - val_accuracy: 0.7014 - lr: 0.0010\n",
            "Epoch 6/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.8866 - accuracy: 0.6914 - val_loss: 0.8215 - val_accuracy: 0.7153 - lr: 0.0010\n",
            "Epoch 7/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.8453 - accuracy: 0.7059 - val_loss: 0.8456 - val_accuracy: 0.7063 - lr: 0.0010\n",
            "Epoch 8/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.8074 - accuracy: 0.7185 - val_loss: 0.7673 - val_accuracy: 0.7331 - lr: 0.0010\n",
            "Epoch 9/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.7823 - accuracy: 0.7273 - val_loss: 0.8017 - val_accuracy: 0.7247 - lr: 0.0010\n",
            "Epoch 10/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.7638 - accuracy: 0.7340 - val_loss: 0.7550 - val_accuracy: 0.7363 - lr: 0.0010\n",
            "Epoch 11/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.6613 - accuracy: 0.7671 - val_loss: 0.6943 - val_accuracy: 0.7584 - lr: 5.0000e-04\n",
            "Epoch 12/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.6416 - accuracy: 0.7765 - val_loss: 0.6752 - val_accuracy: 0.7676 - lr: 5.0000e-04\n",
            "Epoch 13/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.6258 - accuracy: 0.7826 - val_loss: 0.6654 - val_accuracy: 0.7717 - lr: 5.0000e-04\n",
            "Epoch 14/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.6017 - accuracy: 0.7915 - val_loss: 0.6551 - val_accuracy: 0.7794 - lr: 5.0000e-04\n",
            "Epoch 15/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.5976 - accuracy: 0.7896 - val_loss: 0.6844 - val_accuracy: 0.7665 - lr: 5.0000e-04\n",
            "Epoch 16/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.5757 - accuracy: 0.7990 - val_loss: 0.6560 - val_accuracy: 0.7742 - lr: 5.0000e-04\n",
            "Epoch 17/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.5717 - accuracy: 0.8001 - val_loss: 0.6670 - val_accuracy: 0.7752 - lr: 5.0000e-04\n",
            "Epoch 18/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.5550 - accuracy: 0.8068 - val_loss: 0.6365 - val_accuracy: 0.7850 - lr: 5.0000e-04\n",
            "Epoch 19/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.5477 - accuracy: 0.8060 - val_loss: 0.6521 - val_accuracy: 0.7838 - lr: 5.0000e-04\n",
            "Epoch 20/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.5386 - accuracy: 0.8127 - val_loss: 0.6613 - val_accuracy: 0.7793 - lr: 5.0000e-04\n",
            "Epoch 21/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.4697 - accuracy: 0.8350 - val_loss: 0.5988 - val_accuracy: 0.7997 - lr: 1.2500e-04\n",
            "Epoch 22/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.4592 - accuracy: 0.8397 - val_loss: 0.5988 - val_accuracy: 0.8007 - lr: 1.2500e-04\n",
            "Epoch 23/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.4519 - accuracy: 0.8421 - val_loss: 0.6028 - val_accuracy: 0.8000 - lr: 1.2500e-04\n",
            "Epoch 24/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.4440 - accuracy: 0.8436 - val_loss: 0.5874 - val_accuracy: 0.8009 - lr: 1.2500e-04\n",
            "Epoch 25/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.4399 - accuracy: 0.8459 - val_loss: 0.5950 - val_accuracy: 0.8019 - lr: 1.2500e-04\n",
            "Epoch 26/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.4358 - accuracy: 0.8467 - val_loss: 0.6082 - val_accuracy: 0.8005 - lr: 1.2500e-04\n",
            "Epoch 27/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.4319 - accuracy: 0.8482 - val_loss: 0.5946 - val_accuracy: 0.8020 - lr: 1.2500e-04\n",
            "Epoch 28/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.4319 - accuracy: 0.8476 - val_loss: 0.6026 - val_accuracy: 0.7991 - lr: 1.2500e-04\n",
            "Epoch 29/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.4254 - accuracy: 0.8510 - val_loss: 0.5984 - val_accuracy: 0.8008 - lr: 1.2500e-04\n",
            "Epoch 30/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.4183 - accuracy: 0.8519 - val_loss: 0.5971 - val_accuracy: 0.8018 - lr: 1.2500e-04\n",
            "Score for fold 1: loss of 0.5970512628555298; accuracy of 80.1800012588501%\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_3 (Conv2D)           (None, 32, 32, 64)        4864      \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  multiple                 0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_4 (Conv2D)           (None, 16, 16, 128)       204928    \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 8, 8, 128)         0         \n",
            "                                                                 \n",
            " conv2d_5 (Conv2D)           (None, 8, 8, 256)         819456    \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 4096)              0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 10)                40970     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,070,218\n",
            "Trainable params: 1,070,218\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.5986 - accuracy: 0.4162 - val_loss: 1.2883 - val_accuracy: 0.5382 - lr: 0.0010\n",
            "Epoch 2/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.2324 - accuracy: 0.5633 - val_loss: 1.0658 - val_accuracy: 0.6233 - lr: 0.0010\n",
            "Epoch 3/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.0758 - accuracy: 0.6206 - val_loss: 0.9356 - val_accuracy: 0.6713 - lr: 0.0010\n",
            "Epoch 4/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.9818 - accuracy: 0.6554 - val_loss: 0.8693 - val_accuracy: 0.6976 - lr: 0.0010\n",
            "Epoch 5/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.9098 - accuracy: 0.6820 - val_loss: 0.8498 - val_accuracy: 0.7043 - lr: 0.0010\n",
            "Epoch 6/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.8590 - accuracy: 0.7009 - val_loss: 0.7913 - val_accuracy: 0.7282 - lr: 0.0010\n",
            "Epoch 7/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.8236 - accuracy: 0.7131 - val_loss: 0.7438 - val_accuracy: 0.7427 - lr: 0.0010\n",
            "Epoch 8/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.7951 - accuracy: 0.7227 - val_loss: 0.8105 - val_accuracy: 0.7204 - lr: 0.0010\n",
            "Epoch 9/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.7757 - accuracy: 0.7318 - val_loss: 0.7262 - val_accuracy: 0.7487 - lr: 0.0010\n",
            "Epoch 10/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.7475 - accuracy: 0.7433 - val_loss: 0.7564 - val_accuracy: 0.7436 - lr: 0.0010\n",
            "Epoch 11/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.6583 - accuracy: 0.7725 - val_loss: 0.6771 - val_accuracy: 0.7662 - lr: 5.0000e-04\n",
            "Epoch 12/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.6306 - accuracy: 0.7824 - val_loss: 0.6878 - val_accuracy: 0.7659 - lr: 5.0000e-04\n",
            "Epoch 13/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.6117 - accuracy: 0.7855 - val_loss: 0.6536 - val_accuracy: 0.7766 - lr: 5.0000e-04\n",
            "Epoch 14/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.5982 - accuracy: 0.7892 - val_loss: 0.6717 - val_accuracy: 0.7729 - lr: 5.0000e-04\n",
            "Epoch 15/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.5876 - accuracy: 0.7955 - val_loss: 0.6803 - val_accuracy: 0.7698 - lr: 5.0000e-04\n",
            "Epoch 16/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.5730 - accuracy: 0.7989 - val_loss: 0.6690 - val_accuracy: 0.7745 - lr: 5.0000e-04\n",
            "Epoch 17/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.5597 - accuracy: 0.8052 - val_loss: 0.6376 - val_accuracy: 0.7854 - lr: 5.0000e-04\n",
            "Epoch 18/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.5497 - accuracy: 0.8064 - val_loss: 0.6276 - val_accuracy: 0.7903 - lr: 5.0000e-04\n",
            "Epoch 19/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.5444 - accuracy: 0.8115 - val_loss: 0.6295 - val_accuracy: 0.7876 - lr: 5.0000e-04\n",
            "Epoch 20/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.5335 - accuracy: 0.8126 - val_loss: 0.6249 - val_accuracy: 0.7904 - lr: 5.0000e-04\n",
            "Epoch 21/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.4732 - accuracy: 0.8360 - val_loss: 0.5961 - val_accuracy: 0.8024 - lr: 1.2500e-04\n",
            "Epoch 22/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.4572 - accuracy: 0.8414 - val_loss: 0.5941 - val_accuracy: 0.8026 - lr: 1.2500e-04\n",
            "Epoch 23/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.4545 - accuracy: 0.8424 - val_loss: 0.6006 - val_accuracy: 0.8031 - lr: 1.2500e-04\n",
            "Epoch 24/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.4403 - accuracy: 0.8457 - val_loss: 0.5889 - val_accuracy: 0.8040 - lr: 1.2500e-04\n",
            "Epoch 25/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.4339 - accuracy: 0.8482 - val_loss: 0.5906 - val_accuracy: 0.8037 - lr: 1.2500e-04\n",
            "Epoch 26/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.4355 - accuracy: 0.8483 - val_loss: 0.5829 - val_accuracy: 0.8049 - lr: 1.2500e-04\n",
            "Epoch 27/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.4309 - accuracy: 0.8487 - val_loss: 0.5762 - val_accuracy: 0.8059 - lr: 1.2500e-04\n",
            "Epoch 28/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.4292 - accuracy: 0.8499 - val_loss: 0.5858 - val_accuracy: 0.8054 - lr: 1.2500e-04\n",
            "Epoch 29/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.4165 - accuracy: 0.8541 - val_loss: 0.5961 - val_accuracy: 0.8068 - lr: 1.2500e-04\n",
            "Epoch 30/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.4195 - accuracy: 0.8542 - val_loss: 0.6014 - val_accuracy: 0.8022 - lr: 1.2500e-04\n",
            "Score for fold 2: loss of 0.6014094948768616; accuracy of 80.22000193595886%\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_6 (Conv2D)           (None, 32, 32, 64)        4864      \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPooling  multiple                 0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_7 (Conv2D)           (None, 16, 16, 128)       204928    \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 8, 8, 128)         0         \n",
            "                                                                 \n",
            " conv2d_8 (Conv2D)           (None, 8, 8, 256)         819456    \n",
            "                                                                 \n",
            " flatten_2 (Flatten)         (None, 4096)              0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 10)                40970     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,070,218\n",
            "Trainable params: 1,070,218\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.5993 - accuracy: 0.4192 - val_loss: 1.3206 - val_accuracy: 0.5271 - lr: 0.0010\n",
            "Epoch 2/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 1.2440 - accuracy: 0.5588 - val_loss: 1.0551 - val_accuracy: 0.6312 - lr: 0.0010\n",
            "Epoch 3/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 1.0960 - accuracy: 0.6132 - val_loss: 0.9416 - val_accuracy: 0.6736 - lr: 0.0010\n",
            "Epoch 4/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.9937 - accuracy: 0.6512 - val_loss: 0.8587 - val_accuracy: 0.7033 - lr: 0.0010\n",
            "Epoch 5/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.9251 - accuracy: 0.6756 - val_loss: 0.8973 - val_accuracy: 0.6949 - lr: 0.0010\n",
            "Epoch 6/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.8802 - accuracy: 0.6922 - val_loss: 0.8800 - val_accuracy: 0.6962 - lr: 0.0010\n",
            "Epoch 7/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.8425 - accuracy: 0.7079 - val_loss: 0.8021 - val_accuracy: 0.7178 - lr: 0.0010\n",
            "Epoch 8/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.8108 - accuracy: 0.7161 - val_loss: 0.7959 - val_accuracy: 0.7284 - lr: 0.0010\n",
            "Epoch 9/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.7884 - accuracy: 0.7257 - val_loss: 0.7403 - val_accuracy: 0.7451 - lr: 0.0010\n",
            "Epoch 10/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.7611 - accuracy: 0.7363 - val_loss: 0.7318 - val_accuracy: 0.7530 - lr: 0.0010\n",
            "Epoch 11/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.6774 - accuracy: 0.7645 - val_loss: 0.6877 - val_accuracy: 0.7683 - lr: 5.0000e-04\n",
            "Epoch 12/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.6476 - accuracy: 0.7750 - val_loss: 0.6869 - val_accuracy: 0.7648 - lr: 5.0000e-04\n",
            "Epoch 13/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.6269 - accuracy: 0.7813 - val_loss: 0.6811 - val_accuracy: 0.7688 - lr: 5.0000e-04\n",
            "Epoch 14/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.6156 - accuracy: 0.7851 - val_loss: 0.6712 - val_accuracy: 0.7703 - lr: 5.0000e-04\n",
            "Epoch 15/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.6094 - accuracy: 0.7876 - val_loss: 0.6913 - val_accuracy: 0.7645 - lr: 5.0000e-04\n",
            "Epoch 16/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.5868 - accuracy: 0.7954 - val_loss: 0.7119 - val_accuracy: 0.7633 - lr: 5.0000e-04\n",
            "Epoch 17/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.5836 - accuracy: 0.7973 - val_loss: 0.6457 - val_accuracy: 0.7812 - lr: 5.0000e-04\n",
            "Epoch 18/30\n",
            "1243/1250 [============================>.] - ETA: 0s - loss: 0.5676 - accuracy: 0.8010"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17312/3367054595.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'Training for fold {fold_no} ...'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m     history = model.train(\n\u001b[0m\u001b[0;32m     21\u001b[0m               \u001b[0mtrain_gen\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m                 \u001b[0mtrainX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17312/390019104.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, train_gen, train_x, train_y, test_x, test_y, lr_rate, lr_rate_type)\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_gen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtensorboard_callback\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcallback\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 165\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_gen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtensorboard_callback\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcallback\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m#  self.model.fit(train_x, train_y, validation_data=(test_x, test_y) , epochs=25, verbose=1)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    166\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_gen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtensorboard_callback\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1382\u001b[0m                 _r=1):\n\u001b[0;32m   1383\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1384\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1385\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1386\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    913\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    914\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 915\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    945\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    946\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 947\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    948\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2954\u001b[0m       (graph_function,\n\u001b[0;32m   2955\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2956\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   2957\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   2958\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1851\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1852\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1853\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1854\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1855\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    498\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 499\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    500\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    501\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3 hidden layers, Middle and Kernel size 5x5"
      ],
      "metadata": {
        "id": "OSyAOHjeZexM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "acc_per_fold = []\n",
        "loss_per_fold = []\n",
        "\n",
        "with tf.device('/device:GPU:0'):\n",
        "  fold_no = 1\n",
        "  for train, valid in kfold.split(trainX, trainy):\n",
        "\n",
        "    model = CNN_Model(batch_size=32, depth=3, hidden_neurons_1=32, hidden_neurons_2=64, hidden_neurons_3=128, kernel_size_1=5, kernel_size_2=5, kernel_size_3=5, \n",
        "               dropout_rate=0.2, batch_norm=False, pooling='max', layer_activation_1='relu', layer_activation_2='relu', layer_activation_3='relu',\n",
        "               kernel_initialiser='glorot_uniform', bias_initialiser='zeros', optimiser='Adam', weight_decay=None, learning_rate=1e-3, output_activation='softmax', SGD_momentum=0) \n",
        "  \n",
        "    train_gen = tf.keras.preprocessing.image.ImageDataGenerator(width_shift_range=0.1,\n",
        "      height_shift_range=0.1,\n",
        "      horizontal_flip=True)\n",
        "    train_gen.fit(trainX[train])\n",
        "  \n",
        "    print('------------------------------------------------------------------------')\n",
        "    print(f'Training for fold {fold_no} ...')\n",
        "\n",
        "    history = model.train(\n",
        "              train_gen,\n",
        "                trainX[train],\n",
        "                  trainy[train],\n",
        "                trainX[valid],\n",
        "                  trainy[valid],\n",
        "                  lr_rate_type='step')\n",
        "\n",
        "    scores = model.evaluate(trainX[valid], trainy[valid])\n",
        "\n",
        "    print(f'Score for fold {fold_no}: loss of {scores[0]}; accuracy of {scores[1]*100}%')\n",
        "    acc_per_fold.append(scores[1] * 100)\n",
        "    loss_per_fold.append(scores[0])\n",
        "\n",
        "    fold_no+=1\n",
        "\n",
        "# == Provide average scores ==\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Score per fold')\n",
        "for i in range(0, len(acc_per_fold)):\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Average scores for all folds:')\n",
        "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
        "print(f'> Loss: {np.mean(loss_per_fold)}')"
      ],
      "metadata": {
        "id": "5kydbbZAZfcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3 hidden layers, Narrow and Kernel size 5x5"
      ],
      "metadata": {
        "id": "J1RJ93xNZs5N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "acc_per_fold = []\n",
        "loss_per_fold = []\n",
        "\n",
        "with tf.device('/device:GPU:0'):\n",
        "  fold_no = 1\n",
        "  for train, valid in kfold.split(trainX, trainy):\n",
        "\n",
        "    model = CNN_Model(batch_size=32, depth=3, hidden_neurons_1=4, hidden_neurons_2=32, hidden_neurons_3=64, kernel_size_1=5, kernel_size_2=5, kernel_size_3=5, \n",
        "               dropout_rate=0.2, batch_norm=False, pooling='max', layer_activation_1='relu', layer_activation_2='relu', layer_activation_3='relu',\n",
        "               kernel_initialiser='glorot_uniform', bias_initialiser='zeros', optimiser='Adam', weight_decay=None, learning_rate=1e-3, output_activation='softmax', SGD_momentum=0) \n",
        "  \n",
        "    train_gen = tf.keras.preprocessing.image.ImageDataGenerator(width_shift_range=0.1,\n",
        "      height_shift_range=0.1,\n",
        "      horizontal_flip=True)\n",
        "    train_gen.fit(trainX[train])\n",
        "  \n",
        "    print('------------------------------------------------------------------------')\n",
        "    print(f'Training for fold {fold_no} ...')\n",
        "\n",
        "    history = model.train(\n",
        "              train_gen,\n",
        "                trainX[train],\n",
        "                  trainy[train],\n",
        "                trainX[valid],\n",
        "                  trainy[valid],\n",
        "                  lr_rate_type='step')\n",
        "\n",
        "    scores = model.evaluate(trainX[valid], trainy[valid])\n",
        "\n",
        "    print(f'Score for fold {fold_no}: loss of {scores[0]}; accuracy of {scores[1]*100}%')\n",
        "    acc_per_fold.append(scores[1] * 100)\n",
        "    loss_per_fold.append(scores[0])\n",
        "\n",
        "    fold_no+=1\n",
        "\n",
        "# == Provide average scores ==\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Score per fold')\n",
        "for i in range(0, len(acc_per_fold)):\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Average scores for all folds:')\n",
        "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
        "print(f'> Loss: {np.mean(loss_per_fold)}')"
      ],
      "metadata": {
        "id": "TDbXAqaWZr8n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3 hidden layers, Wide and Kernel size 7x7"
      ],
      "metadata": {
        "id": "eH-YkkCJZzCi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "acc_per_fold = []\n",
        "loss_per_fold = []\n",
        "\n",
        "with tf.device('/device:GPU:0'):\n",
        "  fold_no = 1\n",
        "  for train, valid in kfold.split(trainX, trainy):\n",
        "\n",
        "    model = CNN_Model(batch_size=32, depth=3, hidden_neurons_1=64, hidden_neurons_2=128, hidden_neurons_3=256, kernel_size_1=7, kernel_size_2=7, kernel_size_3=7, \n",
        "               dropout_rate=0.2, batch_norm=False, pooling='max', layer_activation_1='relu', layer_activation_2='relu', layer_activation_3='relu',\n",
        "               kernel_initialiser='glorot_uniform', bias_initialiser='zeros', optimiser='Adam', weight_decay=None, learning_rate=1e-3, output_activation='softmax', SGD_momentum=0) \n",
        "  \n",
        "    train_gen = tf.keras.preprocessing.image.ImageDataGenerator(width_shift_range=0.1,\n",
        "      height_shift_range=0.1,\n",
        "      horizontal_flip=True)\n",
        "    train_gen.fit(trainX[train])\n",
        "  \n",
        "    print('------------------------------------------------------------------------')\n",
        "    print(f'Training for fold {fold_no} ...')\n",
        "\n",
        "    history = model.train(\n",
        "              train_gen,\n",
        "                trainX[train],\n",
        "                  trainy[train],\n",
        "                trainX[valid],\n",
        "                  trainy[valid],\n",
        "                  lr_rate_type='step')\n",
        "\n",
        "    scores = model.evaluate(trainX[valid], trainy[valid])\n",
        "\n",
        "    print(f'Score for fold {fold_no}: loss of {scores[0]}; accuracy of {scores[1]*100}%')\n",
        "    acc_per_fold.append(scores[1] * 100)\n",
        "    loss_per_fold.append(scores[0])\n",
        "\n",
        "    fold_no+=1\n",
        "\n",
        "# == Provide average scores ==\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Score per fold')\n",
        "for i in range(0, len(acc_per_fold)):\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Average scores for all folds:')\n",
        "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
        "print(f'> Loss: {np.mean(loss_per_fold)}')"
      ],
      "metadata": {
        "id": "yIF8kjQJZ_vk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3 hidden layers, Middle and Kernel size 7x7"
      ],
      "metadata": {
        "id": "n8UTt4CeZ7Ut"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "acc_per_fold = []\n",
        "loss_per_fold = []\n",
        "\n",
        "with tf.device('/device:GPU:0'):\n",
        "  fold_no = 1\n",
        "  for train, valid in kfold.split(trainX, trainy):\n",
        "\n",
        "    model = CNN_Model(batch_size=32, depth=3, hidden_neurons_1=32, hidden_neurons_2=64, hidden_neurons_3=128, kernel_size_1=7, kernel_size_2=7, kernel_size_3=7, \n",
        "               dropout_rate=0.4, batch_norm=False, pooling='max', layer_activation_1='relu', layer_activation_2='relu', layer_activation_3='relu',\n",
        "               kernel_initialiser='glorot_uniform', bias_initialiser='zeros', optimiser='Adam', weight_decay=None, learning_rate=1e-3, output_activation='softmax', SGD_momentum=0, SGD_momentum=0) \n",
        "  \n",
        "    train_gen = tf.keras.preprocessing.image.ImageDataGenerator(width_shift_range=0.1,\n",
        "      height_shift_range=0.1,\n",
        "      horizontal_flip=True)\n",
        "    train_gen.fit(trainX[train])\n",
        "  \n",
        "    print('------------------------------------------------------------------------')\n",
        "    print(f'Training for fold {fold_no} ...')\n",
        "\n",
        "    history = model.train(\n",
        "              train_gen,\n",
        "                trainX[train],\n",
        "                  trainy[train],\n",
        "                trainX[valid],\n",
        "                  trainy[valid],\n",
        "                  lr_rate_type='step')\n",
        "\n",
        "    scores = model.evaluate(trainX[valid], trainy[valid])\n",
        "\n",
        "    print(f'Score for fold {fold_no}: loss of {scores[0]}; accuracy of {scores[1]*100}%')\n",
        "    acc_per_fold.append(scores[1] * 100)\n",
        "    loss_per_fold.append(scores[0])\n",
        "\n",
        "    fold_no+=1\n",
        "\n",
        "# == Provide average scores ==\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Score per fold')\n",
        "for i in range(0, len(acc_per_fold)):\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Average scores for all folds:')\n",
        "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
        "print(f'> Loss: {np.mean(loss_per_fold)}')"
      ],
      "metadata": {
        "id": "VQdxs9YdLWit"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3 hidden layers, Narrow and Kernel size 7x7"
      ],
      "metadata": {
        "id": "EiuXhVYrZ9Xf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "acc_per_fold = []\n",
        "loss_per_fold = []\n",
        "\n",
        "with tf.device('/device:GPU:0'):\n",
        "  fold_no = 1\n",
        "  for train, valid in kfold.split(trainX, trainy):\n",
        "\n",
        "    model = CNN_Model(batch_size=32, depth=3, hidden_neurons_1=4, hidden_neurons_2=32, hidden_neurons_3=64, kernel_size_1=7, kernel_size_2=7, kernel_size_3=7, \n",
        "               dropout_rate=0.2, batch_norm=False, pooling='max', layer_activation_1='relu', layer_activation_2='relu', layer_activation_3='relu',\n",
        "               kernel_initialiser='glorot_uniform', bias_initialiser='zeros', optimiser='Adam', weight_decay=None, learning_rate=1e-3, output_activation='softmax', SGD_momentum=0) \n",
        "  \n",
        "    train_gen = tf.keras.preprocessing.image.ImageDataGenerator(width_shift_range=0.1,\n",
        "      height_shift_range=0.1,\n",
        "      horizontal_flip=True)\n",
        "    train_gen.fit(trainX[train])\n",
        "  \n",
        "    print('------------------------------------------------------------------------')\n",
        "    print(f'Training for fold {fold_no} ...')\n",
        "\n",
        "    history = model.train(\n",
        "              train_gen,\n",
        "                trainX[train],\n",
        "                  trainy[train],\n",
        "                trainX[valid],\n",
        "                  trainy[valid],\n",
        "                  lr_rate_type='step')\n",
        "\n",
        "    scores = model.evaluate(trainX[valid], trainy[valid])\n",
        "\n",
        "    print(f'Score for fold {fold_no}: loss of {scores[0]}; accuracy of {scores[1]*100}%')\n",
        "    acc_per_fold.append(scores[1] * 100)\n",
        "    loss_per_fold.append(scores[0])\n",
        "\n",
        "    fold_no+=1\n",
        "\n",
        "# == Provide average scores ==\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Score per fold')\n",
        "for i in range(0, len(acc_per_fold)):\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Average scores for all folds:')\n",
        "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
        "print(f'> Loss: {np.mean(loss_per_fold)}')"
      ],
      "metadata": {
        "id": "fD6jSRJxaFkt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Max pooling"
      ],
      "metadata": {
        "id": "xcHCA4VWnzth"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "acc_per_fold = []\n",
        "loss_per_fold = []\n",
        "\n",
        "with tf.device('/device:GPU:0'):\n",
        "  fold_no = 1\n",
        "  for train, valid in kfold.split(trainX, trainy):\n",
        "\n",
        "    model = CNN_Model(batch_size=32, depth=3, hidden_neurons_1=64, hidden_neurons_2=128, hidden_neurons_3=256, kernel_size_1=3, kernel_size_2=3, kernel_size_3=3, \n",
        "               dropout_rate=0.2, batch_norm=False, pooling='max', layer_activation_1='relu', layer_activation_2='relu', layer_activation_3='relu',\n",
        "               kernel_initialiser='glorot_uniform', bias_initialiser='zeros', optimiser='Adam', weight_decay=None, learning_rate=1e-3, output_activation='softmax', SGD_momentum=0) \n",
        "  \n",
        "    train_gen = tf.keras.preprocessing.image.ImageDataGenerator(width_shift_range=0.1,\n",
        "      height_shift_range=0.1,\n",
        "      horizontal_flip=True)\n",
        "    train_gen.fit(trainX[train])\n",
        "  \n",
        "    print('------------------------------------------------------------------------')\n",
        "    print(f'Training for fold {fold_no} ...')\n",
        "\n",
        "    history = model.train(\n",
        "              train_gen,\n",
        "                trainX[train],\n",
        "                  trainy[train],\n",
        "                trainX[valid],\n",
        "                  trainy[valid],\n",
        "                  lr_rate_type='step')\n",
        "\n",
        "    scores = model.evaluate(trainX[valid], trainy[valid])\n",
        "\n",
        "    print(f'Score for fold {fold_no}: loss of {scores[0]}; accuracy of {scores[1]*100}%')\n",
        "    acc_per_fold.append(scores[1] * 100)\n",
        "    loss_per_fold.append(scores[0])\n",
        "\n",
        "    fold_no+=1\n",
        "\n",
        "# == Provide average scores ==\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Score per fold')\n",
        "for i in range(0, len(acc_per_fold)):\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Average scores for all folds:')\n",
        "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
        "print(f'> Loss: {np.mean(loss_per_fold)}')"
      ],
      "metadata": {
        "id": "emRQJwkln9Ka",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "833470e9-3e95-4e3b-9a8d-274a807193b8"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_9 (Conv2D)           (None, 32, 32, 64)        1792      \n",
            "                                                                 \n",
            " max_pooling2d_3 (MaxPooling  multiple                 0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_10 (Conv2D)          (None, 16, 16, 128)       73856     \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 8, 8, 128)         0         \n",
            "                                                                 \n",
            " conv2d_11 (Conv2D)          (None, 8, 8, 256)         295168    \n",
            "                                                                 \n",
            " flatten_3 (Flatten)         (None, 4096)              0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 10)                40970     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 411,786\n",
            "Trainable params: 411,786\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/30\n",
            "1250/1250 [==============================] - 19s 14ms/step - loss: 1.5587 - accuracy: 0.4349 - val_loss: 1.3060 - val_accuracy: 0.5382 - lr: 0.0010\n",
            "Epoch 2/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.1780 - accuracy: 0.5848 - val_loss: 0.9925 - val_accuracy: 0.6554 - lr: 0.0010\n",
            "Epoch 3/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.0261 - accuracy: 0.6419 - val_loss: 0.9025 - val_accuracy: 0.6822 - lr: 0.0010\n",
            "Epoch 4/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.9324 - accuracy: 0.6752 - val_loss: 0.8546 - val_accuracy: 0.7096 - lr: 0.0010\n",
            "Epoch 5/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.8734 - accuracy: 0.6957 - val_loss: 0.8404 - val_accuracy: 0.7073 - lr: 0.0010\n",
            "Epoch 6/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.8299 - accuracy: 0.7121 - val_loss: 0.7595 - val_accuracy: 0.7388 - lr: 0.0010\n",
            "Epoch 7/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.7896 - accuracy: 0.7241 - val_loss: 0.7025 - val_accuracy: 0.7627 - lr: 0.0010\n",
            "Epoch 8/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.7657 - accuracy: 0.7350 - val_loss: 0.7812 - val_accuracy: 0.7380 - lr: 0.0010\n",
            "Epoch 9/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.7466 - accuracy: 0.7419 - val_loss: 0.7181 - val_accuracy: 0.7538 - lr: 0.0010\n",
            "Epoch 10/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.7180 - accuracy: 0.7513 - val_loss: 0.6875 - val_accuracy: 0.7677 - lr: 0.0010\n",
            "Epoch 11/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.6356 - accuracy: 0.7814 - val_loss: 0.6528 - val_accuracy: 0.7803 - lr: 5.0000e-04\n",
            "Epoch 12/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.6169 - accuracy: 0.7882 - val_loss: 0.6379 - val_accuracy: 0.7838 - lr: 5.0000e-04\n",
            "Epoch 13/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.6013 - accuracy: 0.7911 - val_loss: 0.6164 - val_accuracy: 0.7923 - lr: 5.0000e-04\n",
            "Epoch 14/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.5877 - accuracy: 0.7956 - val_loss: 0.6090 - val_accuracy: 0.7982 - lr: 5.0000e-04\n",
            "Epoch 15/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.5798 - accuracy: 0.7994 - val_loss: 0.5974 - val_accuracy: 0.7979 - lr: 5.0000e-04\n",
            "Epoch 16/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.5642 - accuracy: 0.8049 - val_loss: 0.5863 - val_accuracy: 0.8014 - lr: 5.0000e-04\n",
            "Epoch 17/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.5541 - accuracy: 0.8081 - val_loss: 0.6085 - val_accuracy: 0.7934 - lr: 5.0000e-04\n",
            "Epoch 18/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.5454 - accuracy: 0.8130 - val_loss: 0.5975 - val_accuracy: 0.8012 - lr: 5.0000e-04\n",
            "Epoch 19/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.5370 - accuracy: 0.8123 - val_loss: 0.5879 - val_accuracy: 0.8061 - lr: 5.0000e-04\n",
            "Epoch 20/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.5240 - accuracy: 0.8190 - val_loss: 0.5907 - val_accuracy: 0.8038 - lr: 5.0000e-04\n",
            "Epoch 21/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.4768 - accuracy: 0.8320 - val_loss: 0.5572 - val_accuracy: 0.8135 - lr: 1.2500e-04\n",
            "Epoch 22/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.4662 - accuracy: 0.8372 - val_loss: 0.5463 - val_accuracy: 0.8191 - lr: 1.2500e-04\n",
            "Epoch 23/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.4636 - accuracy: 0.8374 - val_loss: 0.5609 - val_accuracy: 0.8126 - lr: 1.2500e-04\n",
            "Epoch 24/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.4562 - accuracy: 0.8406 - val_loss: 0.5811 - val_accuracy: 0.8068 - lr: 1.2500e-04\n",
            "Epoch 25/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.4501 - accuracy: 0.8438 - val_loss: 0.5673 - val_accuracy: 0.8119 - lr: 1.2500e-04\n",
            "Epoch 26/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.4483 - accuracy: 0.8448 - val_loss: 0.5529 - val_accuracy: 0.8194 - lr: 1.2500e-04\n",
            "Epoch 27/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.4414 - accuracy: 0.8461 - val_loss: 0.5452 - val_accuracy: 0.8238 - lr: 1.2500e-04\n",
            "Epoch 28/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.4440 - accuracy: 0.8450 - val_loss: 0.5624 - val_accuracy: 0.8167 - lr: 1.2500e-04\n",
            "Epoch 29/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.4382 - accuracy: 0.8464 - val_loss: 0.5675 - val_accuracy: 0.8149 - lr: 1.2500e-04\n",
            "Epoch 30/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.4360 - accuracy: 0.8487 - val_loss: 0.5388 - val_accuracy: 0.8217 - lr: 1.2500e-04\n",
            "Score for fold 1: loss of 0.5388430953025818; accuracy of 82.16999769210815%\n",
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_12 (Conv2D)          (None, 32, 32, 64)        1792      \n",
            "                                                                 \n",
            " max_pooling2d_4 (MaxPooling  multiple                 0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_13 (Conv2D)          (None, 16, 16, 128)       73856     \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 8, 8, 128)         0         \n",
            "                                                                 \n",
            " conv2d_14 (Conv2D)          (None, 8, 8, 256)         295168    \n",
            "                                                                 \n",
            " flatten_4 (Flatten)         (None, 4096)              0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 10)                40970     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 411,786\n",
            "Trainable params: 411,786\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.5473 - accuracy: 0.4367 - val_loss: 1.1754 - val_accuracy: 0.5826 - lr: 0.0010\n",
            "Epoch 2/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.1542 - accuracy: 0.5900 - val_loss: 0.9948 - val_accuracy: 0.6665 - lr: 0.0010\n",
            "Epoch 3/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.0058 - accuracy: 0.6482 - val_loss: 0.8708 - val_accuracy: 0.6990 - lr: 0.0010\n",
            "Epoch 4/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.9256 - accuracy: 0.6798 - val_loss: 0.8677 - val_accuracy: 0.7067 - lr: 0.0010\n",
            "Epoch 5/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.8659 - accuracy: 0.6996 - val_loss: 0.7718 - val_accuracy: 0.7405 - lr: 0.0010\n",
            "Epoch 6/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.8172 - accuracy: 0.7159 - val_loss: 0.7751 - val_accuracy: 0.7327 - lr: 0.0010\n",
            "Epoch 7/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.7877 - accuracy: 0.7263 - val_loss: 0.7348 - val_accuracy: 0.7512 - lr: 0.0010\n",
            "Epoch 8/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.7635 - accuracy: 0.7345 - val_loss: 0.7366 - val_accuracy: 0.7462 - lr: 0.0010\n",
            "Epoch 9/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.7306 - accuracy: 0.7467 - val_loss: 0.7072 - val_accuracy: 0.7609 - lr: 0.0010\n",
            "Epoch 10/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.7084 - accuracy: 0.7516 - val_loss: 0.7014 - val_accuracy: 0.7598 - lr: 0.0010\n",
            "Epoch 11/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.6277 - accuracy: 0.7792 - val_loss: 0.6305 - val_accuracy: 0.7906 - lr: 5.0000e-04\n",
            "Epoch 12/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.6033 - accuracy: 0.7897 - val_loss: 0.6216 - val_accuracy: 0.7903 - lr: 5.0000e-04\n",
            "Epoch 13/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.5909 - accuracy: 0.7956 - val_loss: 0.6326 - val_accuracy: 0.7914 - lr: 5.0000e-04\n",
            "Epoch 14/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.5821 - accuracy: 0.7970 - val_loss: 0.6134 - val_accuracy: 0.7908 - lr: 5.0000e-04\n",
            "Epoch 15/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.5655 - accuracy: 0.8043 - val_loss: 0.6307 - val_accuracy: 0.7866 - lr: 5.0000e-04\n",
            "Epoch 16/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.5611 - accuracy: 0.8045 - val_loss: 0.5968 - val_accuracy: 0.8051 - lr: 5.0000e-04\n",
            "Epoch 17/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.5509 - accuracy: 0.8099 - val_loss: 0.6497 - val_accuracy: 0.7858 - lr: 5.0000e-04\n",
            "Epoch 18/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.5410 - accuracy: 0.8104 - val_loss: 0.6369 - val_accuracy: 0.7863 - lr: 5.0000e-04\n",
            "Epoch 19/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.5251 - accuracy: 0.8169 - val_loss: 0.6075 - val_accuracy: 0.8000 - lr: 5.0000e-04\n",
            "Epoch 20/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.5260 - accuracy: 0.8156 - val_loss: 0.6003 - val_accuracy: 0.8019 - lr: 5.0000e-04\n",
            "Epoch 21/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.4747 - accuracy: 0.8327 - val_loss: 0.5523 - val_accuracy: 0.8169 - lr: 1.2500e-04\n",
            "Epoch 22/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.4557 - accuracy: 0.8406 - val_loss: 0.5545 - val_accuracy: 0.8159 - lr: 1.2500e-04\n",
            "Epoch 23/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.4564 - accuracy: 0.8388 - val_loss: 0.5428 - val_accuracy: 0.8205 - lr: 1.2500e-04\n",
            "Epoch 24/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.4514 - accuracy: 0.8422 - val_loss: 0.5490 - val_accuracy: 0.8176 - lr: 1.2500e-04\n",
            "Epoch 25/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.4455 - accuracy: 0.8440 - val_loss: 0.5481 - val_accuracy: 0.8169 - lr: 1.2500e-04\n",
            "Epoch 26/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.4425 - accuracy: 0.8455 - val_loss: 0.5430 - val_accuracy: 0.8211 - lr: 1.2500e-04\n",
            "Epoch 27/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.4400 - accuracy: 0.8461 - val_loss: 0.5432 - val_accuracy: 0.8186 - lr: 1.2500e-04\n",
            "Epoch 28/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.4363 - accuracy: 0.8475 - val_loss: 0.5590 - val_accuracy: 0.8161 - lr: 1.2500e-04\n",
            "Epoch 29/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.4281 - accuracy: 0.8497 - val_loss: 0.5515 - val_accuracy: 0.8214 - lr: 1.2500e-04\n",
            "Epoch 30/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.4272 - accuracy: 0.8523 - val_loss: 0.5501 - val_accuracy: 0.8196 - lr: 1.2500e-04\n",
            "Score for fold 2: loss of 0.5500883460044861; accuracy of 81.9599986076355%\n",
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_15 (Conv2D)          (None, 32, 32, 64)        1792      \n",
            "                                                                 \n",
            " max_pooling2d_5 (MaxPooling  multiple                 0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_16 (Conv2D)          (None, 16, 16, 128)       73856     \n",
            "                                                                 \n",
            " dropout_5 (Dropout)         (None, 8, 8, 128)         0         \n",
            "                                                                 \n",
            " conv2d_17 (Conv2D)          (None, 8, 8, 256)         295168    \n",
            "                                                                 \n",
            " flatten_5 (Flatten)         (None, 4096)              0         \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 10)                40970     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 411,786\n",
            "Trainable params: 411,786\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.5269 - accuracy: 0.4497 - val_loss: 1.2820 - val_accuracy: 0.5592 - lr: 0.0010\n",
            "Epoch 2/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 1.1352 - accuracy: 0.6005 - val_loss: 1.0193 - val_accuracy: 0.6497 - lr: 0.0010\n",
            "Epoch 3/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.9997 - accuracy: 0.6486 - val_loss: 0.8717 - val_accuracy: 0.6993 - lr: 0.0010\n",
            "Epoch 4/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.9139 - accuracy: 0.6815 - val_loss: 0.7963 - val_accuracy: 0.7263 - lr: 0.0010\n",
            "Epoch 5/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.8555 - accuracy: 0.7022 - val_loss: 0.8047 - val_accuracy: 0.7288 - lr: 0.0010\n",
            "Epoch 6/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.8135 - accuracy: 0.7168 - val_loss: 0.7581 - val_accuracy: 0.7371 - lr: 0.0010\n",
            "Epoch 7/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.7768 - accuracy: 0.7304 - val_loss: 0.7563 - val_accuracy: 0.7475 - lr: 0.0010\n",
            "Epoch 8/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.7521 - accuracy: 0.7395 - val_loss: 0.7243 - val_accuracy: 0.7552 - lr: 0.0010\n",
            "Epoch 9/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.7249 - accuracy: 0.7485 - val_loss: 0.7496 - val_accuracy: 0.7432 - lr: 0.0010\n",
            "Epoch 10/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.7069 - accuracy: 0.7555 - val_loss: 0.7014 - val_accuracy: 0.7617 - lr: 0.0010\n",
            "Epoch 11/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.6183 - accuracy: 0.7855 - val_loss: 0.6350 - val_accuracy: 0.7869 - lr: 5.0000e-04\n",
            "Epoch 12/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.5945 - accuracy: 0.7936 - val_loss: 0.6342 - val_accuracy: 0.7815 - lr: 5.0000e-04\n",
            "Epoch 13/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.5806 - accuracy: 0.7968 - val_loss: 0.6414 - val_accuracy: 0.7823 - lr: 5.0000e-04\n",
            "Epoch 14/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.5695 - accuracy: 0.8029 - val_loss: 0.6168 - val_accuracy: 0.7959 - lr: 5.0000e-04\n",
            "Epoch 15/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.5593 - accuracy: 0.8069 - val_loss: 0.6130 - val_accuracy: 0.7960 - lr: 5.0000e-04\n",
            "Epoch 16/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.5509 - accuracy: 0.8087 - val_loss: 0.6067 - val_accuracy: 0.7964 - lr: 5.0000e-04\n",
            "Epoch 17/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.5402 - accuracy: 0.8109 - val_loss: 0.5995 - val_accuracy: 0.8012 - lr: 5.0000e-04\n",
            "Epoch 18/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.5297 - accuracy: 0.8154 - val_loss: 0.6196 - val_accuracy: 0.7945 - lr: 5.0000e-04\n",
            "Epoch 19/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.5239 - accuracy: 0.8164 - val_loss: 0.5746 - val_accuracy: 0.8073 - lr: 5.0000e-04\n",
            "Epoch 20/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.5137 - accuracy: 0.8200 - val_loss: 0.6183 - val_accuracy: 0.7952 - lr: 5.0000e-04\n",
            "Epoch 21/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.4542 - accuracy: 0.8393 - val_loss: 0.5537 - val_accuracy: 0.8160 - lr: 1.2500e-04\n",
            "Epoch 22/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.4462 - accuracy: 0.8447 - val_loss: 0.5594 - val_accuracy: 0.8120 - lr: 1.2500e-04\n",
            "Epoch 23/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.4407 - accuracy: 0.8457 - val_loss: 0.5666 - val_accuracy: 0.8136 - lr: 1.2500e-04\n",
            "Epoch 24/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.4355 - accuracy: 0.8471 - val_loss: 0.5629 - val_accuracy: 0.8154 - lr: 1.2500e-04\n",
            "Epoch 25/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.4278 - accuracy: 0.8505 - val_loss: 0.5584 - val_accuracy: 0.8151 - lr: 1.2500e-04\n",
            "Epoch 26/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.4291 - accuracy: 0.8485 - val_loss: 0.5534 - val_accuracy: 0.8165 - lr: 1.2500e-04\n",
            "Epoch 27/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.4202 - accuracy: 0.8530 - val_loss: 0.5520 - val_accuracy: 0.8169 - lr: 1.2500e-04\n",
            "Epoch 28/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.4233 - accuracy: 0.8526 - val_loss: 0.5538 - val_accuracy: 0.8168 - lr: 1.2500e-04\n",
            "Epoch 29/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.4176 - accuracy: 0.8530 - val_loss: 0.5388 - val_accuracy: 0.8203 - lr: 1.2500e-04\n",
            "Epoch 30/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.4074 - accuracy: 0.8553 - val_loss: 0.5557 - val_accuracy: 0.8165 - lr: 1.2500e-04\n",
            "Score for fold 3: loss of 0.5556604862213135; accuracy of 81.65000081062317%\n",
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_18 (Conv2D)          (None, 32, 32, 64)        1792      \n",
            "                                                                 \n",
            " max_pooling2d_6 (MaxPooling  multiple                 0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_19 (Conv2D)          (None, 16, 16, 128)       73856     \n",
            "                                                                 \n",
            " dropout_6 (Dropout)         (None, 8, 8, 128)         0         \n",
            "                                                                 \n",
            " conv2d_20 (Conv2D)          (None, 8, 8, 256)         295168    \n",
            "                                                                 \n",
            " flatten_6 (Flatten)         (None, 4096)              0         \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 10)                40970     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 411,786\n",
            "Trainable params: 411,786\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.5600 - accuracy: 0.4353 - val_loss: 1.1872 - val_accuracy: 0.5895 - lr: 0.0010\n",
            "Epoch 2/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.1709 - accuracy: 0.5855 - val_loss: 1.0729 - val_accuracy: 0.6218 - lr: 0.0010\n",
            "Epoch 3/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.0264 - accuracy: 0.6389 - val_loss: 0.8557 - val_accuracy: 0.6990 - lr: 0.0010\n",
            "Epoch 4/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.9384 - accuracy: 0.6707 - val_loss: 0.8094 - val_accuracy: 0.7187 - lr: 0.0010\n",
            "Epoch 5/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.8761 - accuracy: 0.6956 - val_loss: 0.7915 - val_accuracy: 0.7279 - lr: 0.0010\n",
            "Epoch 6/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.8308 - accuracy: 0.7111 - val_loss: 0.8113 - val_accuracy: 0.7207 - lr: 0.0010\n",
            "Epoch 7/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.7993 - accuracy: 0.7245 - val_loss: 0.7306 - val_accuracy: 0.7511 - lr: 0.0010\n",
            "Epoch 8/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.7639 - accuracy: 0.7337 - val_loss: 0.7866 - val_accuracy: 0.7336 - lr: 0.0010\n",
            "Epoch 9/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.7494 - accuracy: 0.7409 - val_loss: 0.7263 - val_accuracy: 0.7543 - lr: 0.0010\n",
            "Epoch 10/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.7251 - accuracy: 0.7456 - val_loss: 0.7089 - val_accuracy: 0.7612 - lr: 0.0010\n",
            "Epoch 11/30\n",
            "1250/1250 [==============================] - 16s 12ms/step - loss: 0.6428 - accuracy: 0.7784 - val_loss: 0.6557 - val_accuracy: 0.7777 - lr: 5.0000e-04\n",
            "Epoch 12/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.6249 - accuracy: 0.7842 - val_loss: 0.6260 - val_accuracy: 0.7827 - lr: 5.0000e-04\n",
            "Epoch 13/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.6080 - accuracy: 0.7887 - val_loss: 0.6109 - val_accuracy: 0.7903 - lr: 5.0000e-04\n",
            "Epoch 14/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 0.5936 - accuracy: 0.7922 - val_loss: 0.6332 - val_accuracy: 0.7855 - lr: 5.0000e-04\n",
            "Epoch 15/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.5801 - accuracy: 0.7994 - val_loss: 0.6222 - val_accuracy: 0.7897 - lr: 5.0000e-04\n",
            "Epoch 16/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.5707 - accuracy: 0.7986 - val_loss: 0.5841 - val_accuracy: 0.8019 - lr: 5.0000e-04\n",
            "Epoch 17/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.5617 - accuracy: 0.8043 - val_loss: 0.6227 - val_accuracy: 0.7937 - lr: 5.0000e-04\n",
            "Epoch 18/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.5580 - accuracy: 0.8033 - val_loss: 0.6114 - val_accuracy: 0.7924 - lr: 5.0000e-04\n",
            "Epoch 19/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.5508 - accuracy: 0.8092 - val_loss: 0.6056 - val_accuracy: 0.7985 - lr: 5.0000e-04\n",
            "Epoch 20/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 0.5391 - accuracy: 0.8109 - val_loss: 0.6154 - val_accuracy: 0.7949 - lr: 5.0000e-04\n",
            "Epoch 21/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 0.4815 - accuracy: 0.8346 - val_loss: 0.5705 - val_accuracy: 0.8102 - lr: 1.2500e-04\n",
            "Epoch 22/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.4810 - accuracy: 0.8317 - val_loss: 0.5666 - val_accuracy: 0.8076 - lr: 1.2500e-04\n",
            "Epoch 23/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.4623 - accuracy: 0.8388 - val_loss: 0.5598 - val_accuracy: 0.8101 - lr: 1.2500e-04\n",
            "Epoch 24/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.4643 - accuracy: 0.8403 - val_loss: 0.5725 - val_accuracy: 0.8078 - lr: 1.2500e-04\n",
            "Epoch 25/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.4560 - accuracy: 0.8413 - val_loss: 0.5673 - val_accuracy: 0.8089 - lr: 1.2500e-04\n",
            "Epoch 26/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.4591 - accuracy: 0.8403 - val_loss: 0.5705 - val_accuracy: 0.8093 - lr: 1.2500e-04\n",
            "Epoch 27/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.4529 - accuracy: 0.8422 - val_loss: 0.5776 - val_accuracy: 0.8072 - lr: 1.2500e-04\n",
            "Epoch 28/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.4514 - accuracy: 0.8432 - val_loss: 0.5629 - val_accuracy: 0.8110 - lr: 1.2500e-04\n",
            "Epoch 29/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.4449 - accuracy: 0.8461 - val_loss: 0.5555 - val_accuracy: 0.8148 - lr: 1.2500e-04\n",
            "Epoch 30/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.4400 - accuracy: 0.8457 - val_loss: 0.5632 - val_accuracy: 0.8101 - lr: 1.2500e-04\n",
            "Score for fold 4: loss of 0.5632339119911194; accuracy of 81.01000189781189%\n",
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_21 (Conv2D)          (None, 32, 32, 64)        1792      \n",
            "                                                                 \n",
            " max_pooling2d_7 (MaxPooling  multiple                 0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_22 (Conv2D)          (None, 16, 16, 128)       73856     \n",
            "                                                                 \n",
            " dropout_7 (Dropout)         (None, 8, 8, 128)         0         \n",
            "                                                                 \n",
            " conv2d_23 (Conv2D)          (None, 8, 8, 256)         295168    \n",
            "                                                                 \n",
            " flatten_7 (Flatten)         (None, 4096)              0         \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 10)                40970     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 411,786\n",
            "Trainable params: 411,786\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 1.5435 - accuracy: 0.4391 - val_loss: 1.1631 - val_accuracy: 0.5880 - lr: 0.0010\n",
            "Epoch 2/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 1.1643 - accuracy: 0.5887 - val_loss: 1.0616 - val_accuracy: 0.6350 - lr: 0.0010\n",
            "Epoch 3/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.0116 - accuracy: 0.6479 - val_loss: 0.9179 - val_accuracy: 0.6794 - lr: 0.0010\n",
            "Epoch 4/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 0.9301 - accuracy: 0.6760 - val_loss: 0.8483 - val_accuracy: 0.7071 - lr: 0.0010\n",
            "Epoch 5/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.8675 - accuracy: 0.6978 - val_loss: 0.7750 - val_accuracy: 0.7358 - lr: 0.0010\n",
            "Epoch 6/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.8219 - accuracy: 0.7128 - val_loss: 0.8167 - val_accuracy: 0.7173 - lr: 0.0010\n",
            "Epoch 7/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.7875 - accuracy: 0.7264 - val_loss: 0.7414 - val_accuracy: 0.7455 - lr: 0.0010\n",
            "Epoch 8/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.7613 - accuracy: 0.7355 - val_loss: 0.7653 - val_accuracy: 0.7395 - lr: 0.0010\n",
            "Epoch 9/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.7347 - accuracy: 0.7449 - val_loss: 0.6928 - val_accuracy: 0.7609 - lr: 0.0010\n",
            "Epoch 10/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.7103 - accuracy: 0.7503 - val_loss: 0.6819 - val_accuracy: 0.7664 - lr: 0.0010\n",
            "Epoch 11/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.6333 - accuracy: 0.7815 - val_loss: 0.6334 - val_accuracy: 0.7886 - lr: 5.0000e-04\n",
            "Epoch 12/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.6123 - accuracy: 0.7863 - val_loss: 0.6502 - val_accuracy: 0.7800 - lr: 5.0000e-04\n",
            "Epoch 13/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 0.5984 - accuracy: 0.7923 - val_loss: 0.6065 - val_accuracy: 0.7922 - lr: 5.0000e-04\n",
            "Epoch 14/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 0.5824 - accuracy: 0.7988 - val_loss: 0.5776 - val_accuracy: 0.8020 - lr: 5.0000e-04\n",
            "Epoch 15/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.5690 - accuracy: 0.8027 - val_loss: 0.6040 - val_accuracy: 0.7974 - lr: 5.0000e-04\n",
            "Epoch 16/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.5622 - accuracy: 0.8042 - val_loss: 0.5901 - val_accuracy: 0.7993 - lr: 5.0000e-04\n",
            "Epoch 17/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.5498 - accuracy: 0.8098 - val_loss: 0.5916 - val_accuracy: 0.8020 - lr: 5.0000e-04\n",
            "Epoch 18/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.5449 - accuracy: 0.8099 - val_loss: 0.5783 - val_accuracy: 0.8033 - lr: 5.0000e-04\n",
            "Epoch 19/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.5313 - accuracy: 0.8145 - val_loss: 0.5941 - val_accuracy: 0.8021 - lr: 5.0000e-04\n",
            "Epoch 20/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 0.5260 - accuracy: 0.8163 - val_loss: 0.5986 - val_accuracy: 0.8011 - lr: 5.0000e-04\n",
            "Epoch 21/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.4711 - accuracy: 0.8344 - val_loss: 0.5376 - val_accuracy: 0.8189 - lr: 1.2500e-04\n",
            "Epoch 22/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.4545 - accuracy: 0.8420 - val_loss: 0.5374 - val_accuracy: 0.8199 - lr: 1.2500e-04\n",
            "Epoch 23/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.4526 - accuracy: 0.8433 - val_loss: 0.5483 - val_accuracy: 0.8155 - lr: 1.2500e-04\n",
            "Epoch 24/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.4437 - accuracy: 0.8440 - val_loss: 0.5453 - val_accuracy: 0.8171 - lr: 1.2500e-04\n",
            "Epoch 25/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.4408 - accuracy: 0.8460 - val_loss: 0.5470 - val_accuracy: 0.8198 - lr: 1.2500e-04\n",
            "Epoch 26/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 0.4412 - accuracy: 0.8463 - val_loss: 0.5610 - val_accuracy: 0.8145 - lr: 1.2500e-04\n",
            "Epoch 27/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.4356 - accuracy: 0.8484 - val_loss: 0.5327 - val_accuracy: 0.8201 - lr: 1.2500e-04\n",
            "Epoch 28/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.4347 - accuracy: 0.8475 - val_loss: 0.5436 - val_accuracy: 0.8217 - lr: 1.2500e-04\n",
            "Epoch 29/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.4315 - accuracy: 0.8486 - val_loss: 0.5322 - val_accuracy: 0.8240 - lr: 1.2500e-04\n",
            "Epoch 30/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.4250 - accuracy: 0.8504 - val_loss: 0.5327 - val_accuracy: 0.8241 - lr: 1.2500e-04\n",
            "Score for fold 5: loss of 0.5326509475708008; accuracy of 82.41000175476074%\n",
            "------------------------------------------------------------------------\n",
            "Score per fold\n",
            "------------------------------------------------------------------------\n",
            "> Fold 1 - Loss: 0.5388430953025818 - Accuracy: 82.16999769210815%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 2 - Loss: 0.5500883460044861 - Accuracy: 81.9599986076355%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 3 - Loss: 0.5556604862213135 - Accuracy: 81.65000081062317%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 4 - Loss: 0.5632339119911194 - Accuracy: 81.01000189781189%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 5 - Loss: 0.5326509475708008 - Accuracy: 82.41000175476074%\n",
            "------------------------------------------------------------------------\n",
            "Average scores for all folds:\n",
            "> Accuracy: 81.84000015258789 (+- 0.484395847352368)\n",
            "> Loss: 0.5480953574180603\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Average pooling"
      ],
      "metadata": {
        "id": "M9YQFRM_n5v3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "acc_per_fold = []\n",
        "loss_per_fold = []\n",
        "\n",
        "with tf.device('/device:GPU:0'):\n",
        "  fold_no = 1\n",
        "  for train, valid in kfold.split(trainX, trainy):\n",
        "\n",
        "    model = CNN_Model(batch_size=32, depth=3, hidden_neurons_1=64, hidden_neurons_2=128, hidden_neurons_3=256, kernel_size_1=3, kernel_size_2=3, kernel_size_3=3, \n",
        "               dropout_rate=0.2, batch_norm=False, pooling='average', layer_activation_1='relu', layer_activation_2='relu', layer_activation_3='relu',\n",
        "               kernel_initialiser='glorot_uniform', bias_initialiser='zeros', optimiser='Adam', weight_decay=None, learning_rate=1e-3, output_activation='softmax', SGD_momentum=0) \n",
        "  \n",
        "    train_gen = tf.keras.preprocessing.image.ImageDataGenerator(width_shift_range=0.1,\n",
        "      height_shift_range=0.1,\n",
        "      horizontal_flip=True)\n",
        "    train_gen.fit(trainX[train])\n",
        "  \n",
        "    print('------------------------------------------------------------------------')\n",
        "    print(f'Training for fold {fold_no} ...')\n",
        "\n",
        "    history = model.train(\n",
        "              train_gen,\n",
        "                trainX[train],\n",
        "                  trainy[train],\n",
        "                trainX[valid],\n",
        "                  trainy[valid],\n",
        "                  lr_rate_type='step')\n",
        "\n",
        "    scores = model.evaluate(trainX[valid], trainy[valid])\n",
        "\n",
        "    print(f'Score for fold {fold_no}: loss of {scores[0]}; accuracy of {scores[1]*100}%')\n",
        "    acc_per_fold.append(scores[1] * 100)\n",
        "    loss_per_fold.append(scores[0])\n",
        "\n",
        "    fold_no+=1\n",
        "\n",
        "# == Provide average scores ==\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Score per fold')\n",
        "for i in range(0, len(acc_per_fold)):\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Average scores for all folds:')\n",
        "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
        "print(f'> Loss: {np.mean(loss_per_fold)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "NioJDGgmn9nj",
        "outputId": "d6d66ad2-ccee-46bf-cbf6-0188e6d976d8"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_8\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_24 (Conv2D)          (None, 32, 32, 64)        1792      \n",
            "                                                                 \n",
            " average_pooling2d (AverageP  multiple                 0         \n",
            " ooling2D)                                                       \n",
            "                                                                 \n",
            " conv2d_25 (Conv2D)          (None, 16, 16, 128)       73856     \n",
            "                                                                 \n",
            " dropout_8 (Dropout)         (None, 8, 8, 128)         0         \n",
            "                                                                 \n",
            " conv2d_26 (Conv2D)          (None, 8, 8, 256)         295168    \n",
            "                                                                 \n",
            " flatten_8 (Flatten)         (None, 4096)              0         \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 10)                40970     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 411,786\n",
            "Trainable params: 411,786\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/30\n",
            "1215/1250 [============================>.] - ETA: 0s - loss: 1.6283 - accuracy: 0.4100"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17312/2938406568.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'Training for fold {fold_no} ...'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m     history = model.train(\n\u001b[0m\u001b[0;32m     21\u001b[0m               \u001b[0mtrain_gen\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m                 \u001b[0mtrainX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17312/390019104.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, train_gen, train_x, train_y, test_x, test_y, lr_rate, lr_rate_type)\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_gen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtensorboard_callback\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcallback\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 165\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_gen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtensorboard_callback\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcallback\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m#  self.model.fit(train_x, train_y, validation_data=(test_x, test_y) , epochs=25, verbose=1)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    166\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_gen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtensorboard_callback\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1382\u001b[0m                 _r=1):\n\u001b[0;32m   1383\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1384\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1385\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1386\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    913\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    914\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 915\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    945\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    946\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 947\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    948\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2954\u001b[0m       (graph_function,\n\u001b[0;32m   2955\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2956\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   2957\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   2958\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1851\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1852\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1853\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1854\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1855\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    498\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 499\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    500\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    501\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### No pooling"
      ],
      "metadata": {
        "id": "8L7ijjBen-EC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "acc_per_fold = []\n",
        "loss_per_fold = []\n",
        "\n",
        "with tf.device('/device:GPU:0'):\n",
        "  fold_no = 1\n",
        "  for train, valid in kfold.split(trainX, trainy):\n",
        "\n",
        "    model = CNN_Model(batch_size=32, depth=3, hidden_neurons_1=64, hidden_neurons_2=128, hidden_neurons_3=256, kernel_size_1=3, kernel_size_2=3, kernel_size_3=3, \n",
        "               dropout_rate=0.2, batch_norm=False, pooling='None', layer_activation_1='relu', layer_activation_2='relu', layer_activation_3='relu',\n",
        "               kernel_initialiser='glorot_uniform', bias_initialiser='zeros', optimiser='Adam', weight_decay=None, learning_rate=1e-3, output_activation='softmax', SGD_momentum=0) \n",
        "  \n",
        "    train_gen = tf.keras.preprocessing.image.ImageDataGenerator(width_shift_range=0.1,\n",
        "      height_shift_range=0.1,\n",
        "      horizontal_flip=True)\n",
        "    train_gen.fit(trainX[train])\n",
        "  \n",
        "    print('------------------------------------------------------------------------')\n",
        "    print(f'Training for fold {fold_no} ...')\n",
        "\n",
        "    history = model.train(\n",
        "              train_gen,\n",
        "                trainX[train],\n",
        "                  trainy[train],\n",
        "                trainX[valid],\n",
        "                  trainy[valid],\n",
        "                  lr_rate_type='step')\n",
        "\n",
        "    scores = model.evaluate(trainX[valid], trainy[valid])\n",
        "\n",
        "    print(f'Score for fold {fold_no}: loss of {scores[0]}; accuracy of {scores[1]*100}%')\n",
        "    acc_per_fold.append(scores[1] * 100)\n",
        "    loss_per_fold.append(scores[0])\n",
        "\n",
        "    fold_no+=1\n",
        "\n",
        "# == Provide average scores ==\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Score per fold')\n",
        "for i in range(0, len(acc_per_fold)):\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Average scores for all folds:')\n",
        "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
        "print(f'> Loss: {np.mean(loss_per_fold)}')"
      ],
      "metadata": {
        "id": "DLL7J0gBo0l5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Batch normalisation, batch sizes and dropout rate"
      ],
      "metadata": {
        "id": "E9aDAnooY-An"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Drop out rate 0"
      ],
      "metadata": {
        "id": "VQ4pPns7rC4H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "acc_per_fold = []\n",
        "loss_per_fold = []\n",
        "\n",
        "with tf.device('/device:GPU:0'):\n",
        "  fold_no = 1\n",
        "  for train, valid in kfold.split(trainX, trainy):\n",
        "\n",
        "    model = CNN_Model(batch_size=32, depth=3, hidden_neurons_1=64, hidden_neurons_2=128, hidden_neurons_3=256, kernel_size_1=3, kernel_size_2=3, kernel_size_3=3, \n",
        "               dropout_rate=0, batch_norm=False, pooling='max', layer_activation_1='relu', layer_activation_2='relu', layer_activation_3='relu',\n",
        "               kernel_initialiser='glorot_uniform', bias_initialiser='zeros', optimiser='Adam', weight_decay=None, learning_rate=1e-3, output_activation='softmax', SGD_momentum=0) \n",
        "  \n",
        "    train_gen = tf.keras.preprocessing.image.ImageDataGenerator(width_shift_range=0.1,\n",
        "      height_shift_range=0.1,\n",
        "      horizontal_flip=True)\n",
        "    train_gen.fit(trainX[train])\n",
        "  \n",
        "    print('------------------------------------------------------------------------')\n",
        "    print(f'Training for fold {fold_no} ...')\n",
        "\n",
        "    history = model.train(\n",
        "              train_gen,\n",
        "                trainX[train],\n",
        "                  trainy[train],\n",
        "                trainX[valid],\n",
        "                  trainy[valid],\n",
        "                  lr_rate_type='step')\n",
        "\n",
        "    scores = model.evaluate(trainX[valid], trainy[valid])\n",
        "\n",
        "    print(f'Score for fold {fold_no}: loss of {scores[0]}; accuracy of {scores[1]*100}%')\n",
        "    acc_per_fold.append(scores[1] * 100)\n",
        "    loss_per_fold.append(scores[0])\n",
        "\n",
        "    fold_no+=1\n",
        "\n",
        "# == Provide average scores ==\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Score per fold')\n",
        "for i in range(0, len(acc_per_fold)):\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Average scores for all folds:')\n",
        "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
        "print(f'> Loss: {np.mean(loss_per_fold)}')"
      ],
      "metadata": {
        "id": "mIahpLnvZGgO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b61046d0-4855-4165-92e7-d4ad6ccb206e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_9\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_27 (Conv2D)          (None, 32, 32, 64)        1792      \n",
            "                                                                 \n",
            " max_pooling2d_8 (MaxPooling  multiple                 0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_28 (Conv2D)          (None, 16, 16, 128)       73856     \n",
            "                                                                 \n",
            " dropout_9 (Dropout)         (None, 8, 8, 128)         0         \n",
            "                                                                 \n",
            " conv2d_29 (Conv2D)          (None, 8, 8, 256)         295168    \n",
            "                                                                 \n",
            " flatten_9 (Flatten)         (None, 4096)              0         \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 10)                40970     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 411,786\n",
            "Trainable params: 411,786\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 1.5076 - accuracy: 0.4559 - val_loss: 1.1114 - val_accuracy: 0.6120 - lr: 0.0010\n",
            "Epoch 2/30\n",
            "1250/1250 [==============================] - 20s 16ms/step - loss: 1.1131 - accuracy: 0.6097 - val_loss: 0.9531 - val_accuracy: 0.6692 - lr: 0.0010\n",
            "Epoch 3/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 0.9664 - accuracy: 0.6627 - val_loss: 0.8001 - val_accuracy: 0.7282 - lr: 0.0010\n",
            "Epoch 4/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.8705 - accuracy: 0.6979 - val_loss: 0.8177 - val_accuracy: 0.7144 - lr: 0.0010\n",
            "Epoch 5/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.8058 - accuracy: 0.7199 - val_loss: 0.8318 - val_accuracy: 0.7158 - lr: 0.0010\n",
            "Epoch 6/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.7658 - accuracy: 0.7347 - val_loss: 0.6834 - val_accuracy: 0.7649 - lr: 0.0010\n",
            "Epoch 7/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.7231 - accuracy: 0.7505 - val_loss: 0.7112 - val_accuracy: 0.7506 - lr: 0.0010\n",
            "Epoch 8/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.6940 - accuracy: 0.7576 - val_loss: 0.7316 - val_accuracy: 0.7531 - lr: 0.0010\n",
            "Epoch 9/30\n",
            "1250/1250 [==============================] - 20s 16ms/step - loss: 0.6649 - accuracy: 0.7703 - val_loss: 0.6796 - val_accuracy: 0.7677 - lr: 0.0010\n",
            "Epoch 10/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 0.6457 - accuracy: 0.7782 - val_loss: 0.7038 - val_accuracy: 0.7640 - lr: 0.0010\n",
            "Epoch 11/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.5575 - accuracy: 0.8076 - val_loss: 0.6231 - val_accuracy: 0.7902 - lr: 5.0000e-04\n",
            "Epoch 12/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.5335 - accuracy: 0.8154 - val_loss: 0.6084 - val_accuracy: 0.7976 - lr: 5.0000e-04\n",
            "Epoch 13/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.5165 - accuracy: 0.8219 - val_loss: 0.6090 - val_accuracy: 0.7945 - lr: 5.0000e-04\n",
            "Epoch 14/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.5015 - accuracy: 0.8255 - val_loss: 0.5960 - val_accuracy: 0.8008 - lr: 5.0000e-04\n",
            "Epoch 15/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.4889 - accuracy: 0.8291 - val_loss: 0.5901 - val_accuracy: 0.7989 - lr: 5.0000e-04\n",
            "Epoch 16/30\n",
            "1250/1250 [==============================] - 22s 18ms/step - loss: 0.4752 - accuracy: 0.8370 - val_loss: 0.5810 - val_accuracy: 0.8039 - lr: 5.0000e-04\n",
            "Epoch 17/30\n",
            "1250/1250 [==============================] - 21s 17ms/step - loss: 0.4716 - accuracy: 0.8353 - val_loss: 0.5813 - val_accuracy: 0.8056 - lr: 5.0000e-04\n",
            "Epoch 18/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.4549 - accuracy: 0.8414 - val_loss: 0.6057 - val_accuracy: 0.7992 - lr: 5.0000e-04\n",
            "Epoch 19/30\n",
            "1250/1250 [==============================] - 20s 16ms/step - loss: 0.4456 - accuracy: 0.8437 - val_loss: 0.5865 - val_accuracy: 0.8079 - lr: 5.0000e-04\n",
            "Epoch 20/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.4443 - accuracy: 0.8482 - val_loss: 0.5986 - val_accuracy: 0.8049 - lr: 5.0000e-04\n",
            "Epoch 21/30\n",
            "1250/1250 [==============================] - 20s 16ms/step - loss: 0.3805 - accuracy: 0.8684 - val_loss: 0.5628 - val_accuracy: 0.8173 - lr: 1.2500e-04\n",
            "Epoch 22/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.3668 - accuracy: 0.8709 - val_loss: 0.5618 - val_accuracy: 0.8165 - lr: 1.2500e-04\n",
            "Epoch 23/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.3614 - accuracy: 0.8746 - val_loss: 0.5508 - val_accuracy: 0.8209 - lr: 1.2500e-04\n",
            "Epoch 24/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.3561 - accuracy: 0.8752 - val_loss: 0.5553 - val_accuracy: 0.8204 - lr: 1.2500e-04\n",
            "Epoch 25/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 0.3508 - accuracy: 0.8773 - val_loss: 0.5547 - val_accuracy: 0.8232 - lr: 1.2500e-04\n",
            "Epoch 26/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.3498 - accuracy: 0.8773 - val_loss: 0.5444 - val_accuracy: 0.8225 - lr: 1.2500e-04\n",
            "Epoch 27/30\n",
            "1250/1250 [==============================] - 20s 16ms/step - loss: 0.3472 - accuracy: 0.8798 - val_loss: 0.5578 - val_accuracy: 0.8247 - lr: 1.2500e-04\n",
            "Epoch 28/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.3361 - accuracy: 0.8844 - val_loss: 0.5522 - val_accuracy: 0.8238 - lr: 1.2500e-04\n",
            "Epoch 29/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.3388 - accuracy: 0.8823 - val_loss: 0.5711 - val_accuracy: 0.8191 - lr: 1.2500e-04\n",
            "Epoch 30/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.3356 - accuracy: 0.8825 - val_loss: 0.5570 - val_accuracy: 0.8257 - lr: 1.2500e-04\n",
            "Score for fold 1: loss of 0.5569726228713989; accuracy of 82.56999850273132%\n",
            "Model: \"sequential_10\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_30 (Conv2D)          (None, 32, 32, 64)        1792      \n",
            "                                                                 \n",
            " max_pooling2d_9 (MaxPooling  multiple                 0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_31 (Conv2D)          (None, 16, 16, 128)       73856     \n",
            "                                                                 \n",
            " dropout_10 (Dropout)        (None, 8, 8, 128)         0         \n",
            "                                                                 \n",
            " conv2d_32 (Conv2D)          (None, 8, 8, 256)         295168    \n",
            "                                                                 \n",
            " flatten_10 (Flatten)        (None, 4096)              0         \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 10)                40970     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 411,786\n",
            "Trainable params: 411,786\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/30\n",
            " 182/1250 [===>..........................] - ETA: 17s - loss: 1.9648 - accuracy: 0.2759"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17312/1573698063.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'Training for fold {fold_no} ...'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m     history = model.train(\n\u001b[0m\u001b[0;32m     21\u001b[0m               \u001b[0mtrain_gen\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m                 \u001b[0mtrainX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17312/390019104.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, train_gen, train_x, train_y, test_x, test_y, lr_rate, lr_rate_type)\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_gen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtensorboard_callback\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcallback\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 165\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_gen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtensorboard_callback\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcallback\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m#  self.model.fit(train_x, train_y, validation_data=(test_x, test_y) , epochs=25, verbose=1)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    166\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_gen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtensorboard_callback\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1382\u001b[0m                 _r=1):\n\u001b[0;32m   1383\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1384\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1385\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1386\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    913\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    914\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 915\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    945\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    946\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 947\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    948\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2954\u001b[0m       (graph_function,\n\u001b[0;32m   2955\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2956\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   2957\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   2958\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1851\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1852\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1853\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1854\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1855\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    498\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 499\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    500\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    501\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Drop out rate 0.4"
      ],
      "metadata": {
        "id": "rSrVVG4XrMUo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "acc_per_fold = []\n",
        "loss_per_fold = []\n",
        "\n",
        "with tf.device('/device:GPU:0'):\n",
        "  fold_no = 1\n",
        "  for train, valid in kfold.split(trainX, trainy):\n",
        "\n",
        "    model = CNN_Model(batch_size=32, depth=3, hidden_neurons_1=64, hidden_neurons_2=128, hidden_neurons_3=256, kernel_size_1=3, kernel_size_2=3, kernel_size_3=3, \n",
        "               dropout_rate=0.4, batch_norm=False, pooling='max', layer_activation_1='relu', layer_activation_2='relu', layer_activation_3='relu',\n",
        "               kernel_initialiser='glorot_uniform', bias_initialiser='zeros', optimiser='Adam', weight_decay=None, learning_rate=1e-3, output_activation='softmax', SGD_momentum=0)\n",
        "  \n",
        "    train_gen = tf.keras.preprocessing.image.ImageDataGenerator(width_shift_range=0.1,\n",
        "      height_shift_range=0.1,\n",
        "      horizontal_flip=True)\n",
        "    train_gen.fit(trainX[train])\n",
        "  \n",
        "    print('------------------------------------------------------------------------')\n",
        "    print(f'Training for fold {fold_no} ...')\n",
        "\n",
        "    history = model.train(\n",
        "              train_gen,\n",
        "                trainX[train],\n",
        "                  trainy[train],\n",
        "                trainX[valid],\n",
        "                  trainy[valid],\n",
        "                  lr_rate_type='step')\n",
        "\n",
        "    scores = model.evaluate(trainX[valid], trainy[valid])\n",
        "\n",
        "    print(f'Score for fold {fold_no}: loss of {scores[0]}; accuracy of {scores[1]*100}%')\n",
        "    acc_per_fold.append(scores[1] * 100)\n",
        "    loss_per_fold.append(scores[0])\n",
        "\n",
        "    fold_no+=1\n",
        "\n",
        "# == Provide average scores ==\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Score per fold')\n",
        "for i in range(0, len(acc_per_fold)):\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Average scores for all folds:')\n",
        "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
        "print(f'> Loss: {np.mean(loss_per_fold)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "pDViRaodh6dP",
        "outputId": "895402b4-d1cf-4350-ee33-7b66eecc70e8"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_11\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_33 (Conv2D)          (None, 32, 32, 64)        1792      \n",
            "                                                                 \n",
            " max_pooling2d_10 (MaxPoolin  multiple                 0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_34 (Conv2D)          (None, 16, 16, 128)       73856     \n",
            "                                                                 \n",
            " dropout_11 (Dropout)        (None, 8, 8, 128)         0         \n",
            "                                                                 \n",
            " conv2d_35 (Conv2D)          (None, 8, 8, 256)         295168    \n",
            "                                                                 \n",
            " flatten_11 (Flatten)        (None, 4096)              0         \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 10)                40970     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 411,786\n",
            "Trainable params: 411,786\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 1.5448 - accuracy: 0.4396 - val_loss: 1.2872 - val_accuracy: 0.5392 - lr: 0.0010\n",
            "Epoch 2/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.2012 - accuracy: 0.5747 - val_loss: 0.9765 - val_accuracy: 0.6619 - lr: 0.0010\n",
            "Epoch 3/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.0480 - accuracy: 0.6328 - val_loss: 0.9045 - val_accuracy: 0.6888 - lr: 0.0010\n",
            "Epoch 4/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.9621 - accuracy: 0.6646 - val_loss: 0.8930 - val_accuracy: 0.6926 - lr: 0.0010\n",
            "Epoch 5/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 0.9047 - accuracy: 0.6858 - val_loss: 0.7830 - val_accuracy: 0.7300 - lr: 0.0010\n",
            "Epoch 6/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.8712 - accuracy: 0.6964 - val_loss: 0.7667 - val_accuracy: 0.7387 - lr: 0.0010\n",
            "Epoch 7/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.8356 - accuracy: 0.7107 - val_loss: 0.8049 - val_accuracy: 0.7232 - lr: 0.0010\n",
            "Epoch 8/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.8023 - accuracy: 0.7207 - val_loss: 0.8345 - val_accuracy: 0.7092 - lr: 0.0010\n",
            "Epoch 9/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.7858 - accuracy: 0.7269 - val_loss: 0.7580 - val_accuracy: 0.7425 - lr: 0.0010\n",
            "Epoch 10/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.7640 - accuracy: 0.7348 - val_loss: 0.6773 - val_accuracy: 0.7653 - lr: 0.0010\n",
            "Epoch 11/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.6858 - accuracy: 0.7617 - val_loss: 0.6193 - val_accuracy: 0.7874 - lr: 5.0000e-04\n",
            "Epoch 12/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.6675 - accuracy: 0.7671 - val_loss: 0.6004 - val_accuracy: 0.7969 - lr: 5.0000e-04\n",
            "Epoch 13/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.6563 - accuracy: 0.7721 - val_loss: 0.6828 - val_accuracy: 0.7677 - lr: 5.0000e-04\n",
            "Epoch 14/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.6478 - accuracy: 0.7742 - val_loss: 0.6234 - val_accuracy: 0.7899 - lr: 5.0000e-04\n",
            "Epoch 15/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.6298 - accuracy: 0.7814 - val_loss: 0.5980 - val_accuracy: 0.7938 - lr: 5.0000e-04\n",
            "Epoch 16/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.6206 - accuracy: 0.7834 - val_loss: 0.6291 - val_accuracy: 0.7841 - lr: 5.0000e-04\n",
            "Epoch 17/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.6132 - accuracy: 0.7854 - val_loss: 0.5935 - val_accuracy: 0.7994 - lr: 5.0000e-04\n",
            "Epoch 18/30\n",
            "1250/1250 [==============================] - 20s 16ms/step - loss: 0.6059 - accuracy: 0.7862 - val_loss: 0.5898 - val_accuracy: 0.7983 - lr: 5.0000e-04\n",
            "Epoch 19/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.5984 - accuracy: 0.7924 - val_loss: 0.5793 - val_accuracy: 0.8003 - lr: 5.0000e-04\n",
            "Epoch 20/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.5921 - accuracy: 0.7924 - val_loss: 0.5699 - val_accuracy: 0.8054 - lr: 5.0000e-04\n",
            "Epoch 21/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.5385 - accuracy: 0.8109 - val_loss: 0.5525 - val_accuracy: 0.8094 - lr: 1.2500e-04\n",
            "Epoch 22/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.5266 - accuracy: 0.8171 - val_loss: 0.5469 - val_accuracy: 0.8127 - lr: 1.2500e-04\n",
            "Epoch 23/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.5246 - accuracy: 0.8160 - val_loss: 0.5333 - val_accuracy: 0.8165 - lr: 1.2500e-04\n",
            "Epoch 24/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 0.5162 - accuracy: 0.8197 - val_loss: 0.5236 - val_accuracy: 0.8202 - lr: 1.2500e-04\n",
            "Epoch 25/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.5152 - accuracy: 0.8195 - val_loss: 0.5490 - val_accuracy: 0.8116 - lr: 1.2500e-04\n",
            "Epoch 26/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.5058 - accuracy: 0.8230 - val_loss: 0.5295 - val_accuracy: 0.8187 - lr: 1.2500e-04\n",
            "Epoch 27/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.5015 - accuracy: 0.8242 - val_loss: 0.5374 - val_accuracy: 0.8140 - lr: 1.2500e-04\n",
            "Epoch 28/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.4995 - accuracy: 0.8249 - val_loss: 0.5302 - val_accuracy: 0.8168 - lr: 1.2500e-04\n",
            "Epoch 29/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 0.4992 - accuracy: 0.8233 - val_loss: 0.5246 - val_accuracy: 0.8187 - lr: 1.2500e-04\n",
            "Epoch 30/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.4969 - accuracy: 0.8263 - val_loss: 0.5434 - val_accuracy: 0.8159 - lr: 1.2500e-04\n",
            "Score for fold 1: loss of 0.5433564782142639; accuracy of 81.59000277519226%\n",
            "Model: \"sequential_12\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_36 (Conv2D)          (None, 32, 32, 64)        1792      \n",
            "                                                                 \n",
            " max_pooling2d_11 (MaxPoolin  multiple                 0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_37 (Conv2D)          (None, 16, 16, 128)       73856     \n",
            "                                                                 \n",
            " dropout_12 (Dropout)        (None, 8, 8, 128)         0         \n",
            "                                                                 \n",
            " conv2d_38 (Conv2D)          (None, 8, 8, 256)         295168    \n",
            "                                                                 \n",
            " flatten_12 (Flatten)        (None, 4096)              0         \n",
            "                                                                 \n",
            " dense_12 (Dense)            (None, 10)                40970     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 411,786\n",
            "Trainable params: 411,786\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/30\n",
            " 198/1250 [===>..........................] - ETA: 15s - loss: 1.9744 - accuracy: 0.2633"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17312/3478072402.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'Training for fold {fold_no} ...'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m     history = model.train(\n\u001b[0m\u001b[0;32m     21\u001b[0m               \u001b[0mtrain_gen\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m                 \u001b[0mtrainX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17312/390019104.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, train_gen, train_x, train_y, test_x, test_y, lr_rate, lr_rate_type)\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_gen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtensorboard_callback\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcallback\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 165\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_gen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtensorboard_callback\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcallback\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m#  self.model.fit(train_x, train_y, validation_data=(test_x, test_y) , epochs=25, verbose=1)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    166\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_gen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtensorboard_callback\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1382\u001b[0m                 _r=1):\n\u001b[0;32m   1383\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1384\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1385\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1386\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    913\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    914\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 915\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    945\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    946\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 947\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    948\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2954\u001b[0m       (graph_function,\n\u001b[0;32m   2955\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2956\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   2957\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   2958\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1851\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1852\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1853\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1854\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1855\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    498\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 499\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    500\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    501\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Drop out rate 0.8"
      ],
      "metadata": {
        "id": "3CEfIV-GrPzu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "acc_per_fold = []\n",
        "loss_per_fold = []\n",
        "\n",
        "with tf.device('/device:GPU:0'):\n",
        "  fold_no = 1\n",
        "  for train, valid in kfold.split(trainX, trainy):\n",
        "\n",
        "    model = CNN_Model(batch_size=32, depth=3, hidden_neurons_1=64, hidden_neurons_2=128, hidden_neurons_3=256, kernel_size_1=3, kernel_size_2=3, kernel_size_3=3, \n",
        "               dropout_rate=0.8, batch_norm=False, pooling='max', layer_activation_1='relu', layer_activation_2='relu', layer_activation_3='relu',\n",
        "               kernel_initialiser='glorot_uniform', bias_initialiser='zeros', optimiser='Adam', weight_decay=None, learning_rate=1e-3, output_activation='softmax', SGD_momentum=0) \n",
        "  \n",
        "    train_gen = tf.keras.preprocessing.image.ImageDataGenerator(width_shift_range=0.1,\n",
        "      height_shift_range=0.1,\n",
        "      horizontal_flip=True)\n",
        "    train_gen.fit(trainX[train])\n",
        "  \n",
        "    print('------------------------------------------------------------------------')\n",
        "    print(f'Training for fold {fold_no} ...')\n",
        "\n",
        "    history = model.train(\n",
        "              train_gen,\n",
        "                trainX[train],\n",
        "                  trainy[train],\n",
        "                trainX[valid],\n",
        "                  trainy[valid],\n",
        "                  lr_rate_type='step')\n",
        "\n",
        "    scores = model.evaluate(trainX[valid], trainy[valid])\n",
        "\n",
        "    print(f'Score for fold {fold_no}: loss of {scores[0]}; accuracy of {scores[1]*100}%')\n",
        "    acc_per_fold.append(scores[1] * 100)\n",
        "    loss_per_fold.append(scores[0])\n",
        "\n",
        "    fold_no+=1\n",
        "\n",
        "# == Provide average scores ==\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Score per fold')\n",
        "for i in range(0, len(acc_per_fold)):\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Average scores for all folds:')\n",
        "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
        "print(f'> Loss: {np.mean(loss_per_fold)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "E2hnSqvmrSrv",
        "outputId": "81ffd2a2-2d81-4f8f-a6ff-97a1e2e72c90"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_13\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_39 (Conv2D)          (None, 32, 32, 64)        1792      \n",
            "                                                                 \n",
            " max_pooling2d_12 (MaxPoolin  multiple                 0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_40 (Conv2D)          (None, 16, 16, 128)       73856     \n",
            "                                                                 \n",
            " dropout_13 (Dropout)        (None, 8, 8, 128)         0         \n",
            "                                                                 \n",
            " conv2d_41 (Conv2D)          (None, 8, 8, 256)         295168    \n",
            "                                                                 \n",
            " flatten_13 (Flatten)        (None, 4096)              0         \n",
            "                                                                 \n",
            " dense_13 (Dense)            (None, 10)                40970     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 411,786\n",
            "Trainable params: 411,786\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.7054 - accuracy: 0.3760 - val_loss: 1.4928 - val_accuracy: 0.4786 - lr: 0.0010\n",
            "Epoch 2/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.4404 - accuracy: 0.4802 - val_loss: 1.4679 - val_accuracy: 0.5615 - lr: 0.0010\n",
            "Epoch 3/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 1.3426 - accuracy: 0.5175 - val_loss: 1.3746 - val_accuracy: 0.5592 - lr: 0.0010\n",
            "Epoch 4/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 1.2787 - accuracy: 0.5421 - val_loss: 1.3052 - val_accuracy: 0.5726 - lr: 0.0010\n",
            "Epoch 5/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.2316 - accuracy: 0.5596 - val_loss: 1.3606 - val_accuracy: 0.5647 - lr: 0.0010\n",
            "Epoch 6/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 1.1912 - accuracy: 0.5763 - val_loss: 1.2353 - val_accuracy: 0.6226 - lr: 0.0010\n",
            "Epoch 7/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.1615 - accuracy: 0.5899 - val_loss: 1.2424 - val_accuracy: 0.6327 - lr: 0.0010\n",
            "Epoch 8/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.1405 - accuracy: 0.5930 - val_loss: 1.2915 - val_accuracy: 0.6444 - lr: 0.0010\n",
            "Epoch 9/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.1124 - accuracy: 0.6055 - val_loss: 1.2374 - val_accuracy: 0.6305 - lr: 0.0010\n",
            "Epoch 10/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 1.1004 - accuracy: 0.6095 - val_loss: 1.1700 - val_accuracy: 0.6412 - lr: 0.0010\n",
            "Epoch 11/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.0309 - accuracy: 0.6351 - val_loss: 1.1376 - val_accuracy: 0.6594 - lr: 5.0000e-04\n",
            "Epoch 12/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.0043 - accuracy: 0.6453 - val_loss: 1.1017 - val_accuracy: 0.6594 - lr: 5.0000e-04\n",
            "Epoch 13/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.9974 - accuracy: 0.6497 - val_loss: 1.0587 - val_accuracy: 0.6917 - lr: 5.0000e-04\n",
            "Epoch 14/30\n",
            " 794/1250 [==================>...........] - ETA: 6s - loss: 0.9927 - accuracy: 0.6509"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17312/1986592322.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'Training for fold {fold_no} ...'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m     history = model.train(\n\u001b[0m\u001b[0;32m     21\u001b[0m               \u001b[0mtrain_gen\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m                 \u001b[0mtrainX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17312/390019104.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, train_gen, train_x, train_y, test_x, test_y, lr_rate, lr_rate_type)\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_gen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtensorboard_callback\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcallback\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 165\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_gen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtensorboard_callback\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcallback\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m#  self.model.fit(train_x, train_y, validation_data=(test_x, test_y) , epochs=25, verbose=1)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    166\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_gen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtensorboard_callback\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1382\u001b[0m                 _r=1):\n\u001b[0;32m   1383\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1384\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1385\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1386\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    913\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    914\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 915\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    945\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    946\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 947\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    948\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2954\u001b[0m       (graph_function,\n\u001b[0;32m   2955\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2956\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   2957\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   2958\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1851\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1852\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1853\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1854\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1855\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    498\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 499\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    500\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    501\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Batch norm with batch size 32"
      ],
      "metadata": {
        "id": "Jrma4IZ4rTKm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "acc_per_fold = []\n",
        "loss_per_fold = []\n",
        "\n",
        "with tf.device('/device:GPU:0'):\n",
        "  fold_no = 1\n",
        "  for train, valid in kfold.split(trainX, trainy):\n",
        "\n",
        "    model = CNN_Model(batch_size=32, depth=3, hidden_neurons_1=32, hidden_neurons_2=64, hidden_neurons_3=128, kernel_size_1=3, kernel_size_2=3, kernel_size_3=3, \n",
        "               dropout_rate=0.2, batch_norm=True, pooling='max', layer_activation_1='relu', layer_activation_2='relu', layer_activation_3='relu',\n",
        "               kernel_initialiser='glorot_uniform', bias_initialiser='zeros', optimiser='Adam', weight_decay=None, learning_rate=1e-3, output_activation='softmax', SGD_momentum=0) \n",
        "  \n",
        "    train_gen = tf.keras.preprocessing.image.ImageDataGenerator(width_shift_range=0.1,\n",
        "      height_shift_range=0.1,\n",
        "      horizontal_flip=True)\n",
        "    train_gen.fit(trainX[train])\n",
        "  \n",
        "    print('------------------------------------------------------------------------')\n",
        "    print(f'Training for fold {fold_no} ...')\n",
        "\n",
        "    history = model.train(\n",
        "              train_gen,\n",
        "                trainX[train],\n",
        "                  trainy[train],\n",
        "                trainX[valid],\n",
        "                  trainy[valid],\n",
        "                  lr_rate_type='step')\n",
        "\n",
        "    scores = model.evaluate(trainX[valid], trainy[valid])\n",
        "\n",
        "    print(f'Score for fold {fold_no}: loss of {scores[0]}; accuracy of {scores[1]*100}%')\n",
        "    acc_per_fold.append(scores[1] * 100)\n",
        "    loss_per_fold.append(scores[0])\n",
        "\n",
        "    fold_no+=1\n",
        "\n",
        "# == Provide average scores ==\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Score per fold')\n",
        "for i in range(0, len(acc_per_fold)):\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Average scores for all folds:')\n",
        "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
        "print(f'> Loss: {np.mean(loss_per_fold)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "WuDEqVU42Pxv",
        "outputId": "efed6610-3f0d-489c-eb19-c2b14ac84081"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_21 (Conv2D)          (None, 32, 32, 32)        896       \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 32, 32, 32)       128       \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " max_pooling2d_7 (MaxPooling  multiple                 0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_22 (Conv2D)          (None, 16, 16, 64)        18496     \n",
            "                                                                 \n",
            " batch_normalization_1 (Batc  (None, 16, 16, 64)       256       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " dropout_7 (Dropout)         (None, 8, 8, 64)          0         \n",
            "                                                                 \n",
            " conv2d_23 (Conv2D)          (None, 8, 8, 128)         73856     \n",
            "                                                                 \n",
            " batch_normalization_2 (Batc  (None, 8, 8, 128)        512       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " flatten_7 (Flatten)         (None, 2048)              0         \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 10)                20490     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 114,634\n",
            "Trainable params: 114,186\n",
            "Non-trainable params: 448\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/30\n",
            "1250/1250 [==============================] - 21s 16ms/step - loss: 1.6766 - accuracy: 0.4459 - val_loss: 1.6334 - val_accuracy: 0.4575 - lr: 0.0010\n",
            "Epoch 2/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.2635 - accuracy: 0.5667 - val_loss: 1.3224 - val_accuracy: 0.5606 - lr: 0.0010\n",
            "Epoch 3/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.0895 - accuracy: 0.6218 - val_loss: 1.0513 - val_accuracy: 0.6396 - lr: 0.0010\n",
            "Epoch 4/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.9903 - accuracy: 0.6565 - val_loss: 2.5066 - val_accuracy: 0.4447 - lr: 0.0010\n",
            "Epoch 5/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.9196 - accuracy: 0.6798 - val_loss: 0.9967 - val_accuracy: 0.6632 - lr: 0.0010\n",
            "Epoch 6/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.8747 - accuracy: 0.6973 - val_loss: 0.8870 - val_accuracy: 0.6911 - lr: 0.0010\n",
            "Epoch 7/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 0.8228 - accuracy: 0.7168 - val_loss: 0.8761 - val_accuracy: 0.7072 - lr: 0.0010\n",
            "Epoch 8/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.7919 - accuracy: 0.7250 - val_loss: 0.7660 - val_accuracy: 0.7366 - lr: 0.0010\n",
            "Epoch 9/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.7654 - accuracy: 0.7323 - val_loss: 0.7213 - val_accuracy: 0.7551 - lr: 0.0010\n",
            "Epoch 10/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.7441 - accuracy: 0.7408 - val_loss: 0.7621 - val_accuracy: 0.7357 - lr: 0.0010\n",
            "Epoch 11/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.6715 - accuracy: 0.7667 - val_loss: 0.6766 - val_accuracy: 0.7729 - lr: 5.0000e-04\n",
            "Epoch 12/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.6453 - accuracy: 0.7756 - val_loss: 0.6153 - val_accuracy: 0.7893 - lr: 5.0000e-04\n",
            "Epoch 13/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.6300 - accuracy: 0.7808 - val_loss: 0.6045 - val_accuracy: 0.7951 - lr: 5.0000e-04\n",
            "Epoch 14/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.6171 - accuracy: 0.7845 - val_loss: 0.7315 - val_accuracy: 0.7555 - lr: 5.0000e-04\n",
            "Epoch 15/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.6124 - accuracy: 0.7863 - val_loss: 0.5970 - val_accuracy: 0.7985 - lr: 5.0000e-04\n",
            "Epoch 16/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.5968 - accuracy: 0.7929 - val_loss: 0.6584 - val_accuracy: 0.7775 - lr: 5.0000e-04\n",
            "Epoch 17/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.5870 - accuracy: 0.7959 - val_loss: 0.6634 - val_accuracy: 0.7843 - lr: 5.0000e-04\n",
            "Epoch 18/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.5797 - accuracy: 0.7973 - val_loss: 0.6497 - val_accuracy: 0.7837 - lr: 5.0000e-04\n",
            "Epoch 19/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.5749 - accuracy: 0.8008 - val_loss: 0.6276 - val_accuracy: 0.7899 - lr: 5.0000e-04\n",
            "Epoch 20/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.5717 - accuracy: 0.8008 - val_loss: 0.7337 - val_accuracy: 0.7570 - lr: 5.0000e-04\n",
            "Epoch 21/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.5216 - accuracy: 0.8196 - val_loss: 0.5477 - val_accuracy: 0.8153 - lr: 1.2500e-04\n",
            "Epoch 22/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.5126 - accuracy: 0.8230 - val_loss: 0.5495 - val_accuracy: 0.8157 - lr: 1.2500e-04\n",
            "Epoch 23/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.5095 - accuracy: 0.8222 - val_loss: 0.5327 - val_accuracy: 0.8192 - lr: 1.2500e-04\n",
            "Epoch 24/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.5012 - accuracy: 0.8262 - val_loss: 0.5457 - val_accuracy: 0.8147 - lr: 1.2500e-04\n",
            "Epoch 25/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 0.5024 - accuracy: 0.8247 - val_loss: 0.5370 - val_accuracy: 0.8193 - lr: 1.2500e-04\n",
            "Epoch 26/30\n",
            " 102/1250 [=>............................] - ETA: 14s - loss: 0.5105 - accuracy: 0.8260"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_2392/637985234.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'Training for fold {fold_no} ...'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m     history = model.train(\n\u001b[0m\u001b[0;32m     21\u001b[0m               \u001b[0mtrain_gen\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m                 \u001b[0mtrainX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_2392/846801809.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, train_gen, train_x, train_y, test_x, test_y, lr_rate, lr_rate_type)\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_gen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtensorboard_callback\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcallback\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 165\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_gen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtensorboard_callback\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcallback\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m#  self.model.fit(train_x, train_y, validation_data=(test_x, test_y) , epochs=25, verbose=1)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    166\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_gen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtensorboard_callback\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1382\u001b[0m                 _r=1):\n\u001b[0;32m   1383\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1384\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1385\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1386\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    913\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    914\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 915\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    945\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    946\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 947\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    948\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2954\u001b[0m       (graph_function,\n\u001b[0;32m   2955\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2956\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   2957\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   2958\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1851\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1852\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1853\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1854\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1855\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    498\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 499\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    500\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    501\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Batch size 32"
      ],
      "metadata": {
        "id": "A_HlrPfq0y8l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "acc_per_fold = []\n",
        "loss_per_fold = []\n",
        "\n",
        "with tf.device('/device:GPU:0'):\n",
        "  fold_no = 1\n",
        "  for train, valid in kfold.split(trainX, trainy):\n",
        "\n",
        "    model = CNN_Model(batch_size=32, depth=3, hidden_neurons_1=32, hidden_neurons_2=64, hidden_neurons_3=128, kernel_size_1=3, kernel_size_2=3, kernel_size_3=3, \n",
        "               dropout_rate=0.2, batch_norm=False, pooling='max', layer_activation_1='relu', layer_activation_2='relu', layer_activation_3='relu',\n",
        "               kernel_initialiser='glorot_uniform', bias_initialiser='zeros', optimiser='Adam', weight_decay=None, learning_rate=1e-3, output_activation='softmax', SGD_momentum=0) \n",
        "  \n",
        "    train_gen = tf.keras.preprocessing.image.ImageDataGenerator(width_shift_range=0.1,\n",
        "      height_shift_range=0.1,\n",
        "      horizontal_flip=True)\n",
        "    train_gen.fit(trainX[train])\n",
        "  \n",
        "    print('------------------------------------------------------------------------')\n",
        "    print(f'Training for fold {fold_no} ...')\n",
        "\n",
        "    history = model.train(\n",
        "              train_gen,\n",
        "                trainX[train],\n",
        "                  trainy[train],\n",
        "                trainX[valid],\n",
        "                  trainy[valid],\n",
        "                  lr_rate_type='step')\n",
        "\n",
        "    scores = model.evaluate(trainX[valid], trainy[valid])\n",
        "\n",
        "    print(f'Score for fold {fold_no}: loss of {scores[0]}; accuracy of {scores[1]*100}%')\n",
        "    acc_per_fold.append(scores[1] * 100)\n",
        "    loss_per_fold.append(scores[0])\n",
        "\n",
        "    fold_no+=1\n",
        "\n",
        "# == Provide average scores ==\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Score per fold')\n",
        "for i in range(0, len(acc_per_fold)):\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Average scores for all folds:')\n",
        "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
        "print(f'> Loss: {np.mean(loss_per_fold)}')"
      ],
      "metadata": {
        "id": "uJuVNNzD029G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Batch norm with batch size 64"
      ],
      "metadata": {
        "id": "KsPu9xLc09oC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "acc_per_fold = []\n",
        "loss_per_fold = []\n",
        "\n",
        "with tf.device('/device:GPU:0'):\n",
        "  fold_no = 1\n",
        "  for train, valid in kfold.split(trainX, trainy):\n",
        "\n",
        "    model = CNN_Model(batch_size=64, depth=3, hidden_neurons_1=32, hidden_neurons_2=64, hidden_neurons_3=128, kernel_size_1=3, kernel_size_2=3, kernel_size_3=3, \n",
        "               dropout_rate=0.2, batch_norm=True, pooling='max', layer_activation_1='relu', layer_activation_2='relu', layer_activation_3='relu',\n",
        "               kernel_initialiser='glorot_uniform', bias_initialiser='zeros', optimiser='Adam', weight_decay=None, learning_rate=1e-3, output_activation='softmax', SGD_momentum=0) \n",
        "  \n",
        "    train_gen = tf.keras.preprocessing.image.ImageDataGenerator(width_shift_range=0.1,\n",
        "      height_shift_range=0.1,\n",
        "      horizontal_flip=True)\n",
        "    train_gen.fit(trainX[train])\n",
        "  \n",
        "    print('------------------------------------------------------------------------')\n",
        "    print(f'Training for fold {fold_no} ...')\n",
        "\n",
        "    history = model.train(\n",
        "              train_gen,\n",
        "                trainX[train],\n",
        "                  trainy[train],\n",
        "                trainX[valid],\n",
        "                  trainy[valid],\n",
        "                  lr_rate_type='step')\n",
        "\n",
        "    scores = model.evaluate(trainX[valid], trainy[valid])\n",
        "\n",
        "    print(f'Score for fold {fold_no}: loss of {scores[0]}; accuracy of {scores[1]*100}%')\n",
        "    acc_per_fold.append(scores[1] * 100)\n",
        "    loss_per_fold.append(scores[0])\n",
        "\n",
        "    fold_no+=1\n",
        "\n",
        "# == Provide average scores ==\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Score per fold')\n",
        "for i in range(0, len(acc_per_fold)):\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Average scores for all folds:')\n",
        "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
        "print(f'> Loss: {np.mean(loss_per_fold)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "mor8Y97L1Jor",
        "outputId": "88caf0e1-d418-47e0-f9a5-30c398c381bd"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_8\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_24 (Conv2D)          (None, 32, 32, 32)        896       \n",
            "                                                                 \n",
            " batch_normalization_3 (Batc  (None, 32, 32, 32)       128       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " max_pooling2d_8 (MaxPooling  multiple                 0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_25 (Conv2D)          (None, 16, 16, 64)        18496     \n",
            "                                                                 \n",
            " batch_normalization_4 (Batc  (None, 16, 16, 64)       256       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " dropout_8 (Dropout)         (None, 8, 8, 64)          0         \n",
            "                                                                 \n",
            " conv2d_26 (Conv2D)          (None, 8, 8, 128)         73856     \n",
            "                                                                 \n",
            " batch_normalization_5 (Batc  (None, 8, 8, 128)        512       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " flatten_8 (Flatten)         (None, 2048)              0         \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 10)                20490     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 114,634\n",
            "Trainable params: 114,186\n",
            "Non-trainable params: 448\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/30\n",
            "625/625 [==============================] - 17s 27ms/step - loss: 1.5800 - accuracy: 0.4721 - val_loss: 1.5285 - val_accuracy: 0.5208 - lr: 0.0010\n",
            "Epoch 2/30\n",
            "625/625 [==============================] - 16s 26ms/step - loss: 1.2047 - accuracy: 0.5864 - val_loss: 1.3649 - val_accuracy: 0.5717 - lr: 0.0010\n",
            "Epoch 3/30\n",
            "625/625 [==============================] - 16s 26ms/step - loss: 1.0634 - accuracy: 0.6366 - val_loss: 1.2544 - val_accuracy: 0.6209 - lr: 0.0010\n",
            "Epoch 4/30\n",
            "625/625 [==============================] - 16s 26ms/step - loss: 0.9810 - accuracy: 0.6625 - val_loss: 1.1709 - val_accuracy: 0.6243 - lr: 0.0010\n",
            "Epoch 5/30\n",
            "625/625 [==============================] - 16s 26ms/step - loss: 0.9211 - accuracy: 0.6844 - val_loss: 1.2244 - val_accuracy: 0.6058 - lr: 0.0010\n",
            "Epoch 6/30\n",
            "625/625 [==============================] - 16s 26ms/step - loss: 0.8711 - accuracy: 0.6981 - val_loss: 0.8647 - val_accuracy: 0.7080 - lr: 0.0010\n",
            "Epoch 7/30\n",
            "625/625 [==============================] - 15s 25ms/step - loss: 0.8367 - accuracy: 0.7105 - val_loss: 0.9352 - val_accuracy: 0.6887 - lr: 0.0010\n",
            "Epoch 8/30\n",
            "625/625 [==============================] - 15s 25ms/step - loss: 0.8076 - accuracy: 0.7214 - val_loss: 0.7601 - val_accuracy: 0.7462 - lr: 0.0010\n",
            "Epoch 9/30\n",
            "625/625 [==============================] - 16s 25ms/step - loss: 0.7762 - accuracy: 0.7312 - val_loss: 0.8688 - val_accuracy: 0.7156 - lr: 0.0010\n",
            "Epoch 10/30\n",
            "625/625 [==============================] - 16s 25ms/step - loss: 0.7459 - accuracy: 0.7436 - val_loss: 0.8787 - val_accuracy: 0.7057 - lr: 0.0010\n",
            "Epoch 11/30\n",
            "625/625 [==============================] - 16s 26ms/step - loss: 0.6748 - accuracy: 0.7653 - val_loss: 0.7335 - val_accuracy: 0.7552 - lr: 5.0000e-04\n",
            "Epoch 12/30\n",
            "625/625 [==============================] - 16s 26ms/step - loss: 0.6555 - accuracy: 0.7723 - val_loss: 0.6849 - val_accuracy: 0.7691 - lr: 5.0000e-04\n",
            "Epoch 13/30\n",
            "625/625 [==============================] - 16s 25ms/step - loss: 0.6398 - accuracy: 0.7782 - val_loss: 0.6853 - val_accuracy: 0.7672 - lr: 5.0000e-04\n",
            "Epoch 14/30\n",
            "625/625 [==============================] - 15s 25ms/step - loss: 0.6314 - accuracy: 0.7805 - val_loss: 0.6619 - val_accuracy: 0.7795 - lr: 5.0000e-04\n",
            "Epoch 15/30\n",
            "625/625 [==============================] - 16s 25ms/step - loss: 0.6157 - accuracy: 0.7854 - val_loss: 0.7001 - val_accuracy: 0.7681 - lr: 5.0000e-04\n",
            "Epoch 16/30\n",
            "625/625 [==============================] - 16s 26ms/step - loss: 0.6113 - accuracy: 0.7867 - val_loss: 0.7344 - val_accuracy: 0.7638 - lr: 5.0000e-04\n",
            "Epoch 17/30\n",
            "625/625 [==============================] - 17s 27ms/step - loss: 0.6062 - accuracy: 0.7879 - val_loss: 0.7022 - val_accuracy: 0.7688 - lr: 5.0000e-04\n",
            "Epoch 18/30\n",
            "625/625 [==============================] - 17s 27ms/step - loss: 0.5898 - accuracy: 0.7943 - val_loss: 0.6241 - val_accuracy: 0.7937 - lr: 5.0000e-04\n",
            "Epoch 19/30\n",
            "625/625 [==============================] - 15s 25ms/step - loss: 0.5910 - accuracy: 0.7944 - val_loss: 0.6723 - val_accuracy: 0.7790 - lr: 5.0000e-04\n",
            "Epoch 20/30\n",
            "625/625 [==============================] - 16s 26ms/step - loss: 0.5769 - accuracy: 0.7971 - val_loss: 0.6240 - val_accuracy: 0.7887 - lr: 5.0000e-04\n",
            "Epoch 21/30\n",
            "625/625 [==============================] - 16s 26ms/step - loss: 0.5308 - accuracy: 0.8165 - val_loss: 0.5856 - val_accuracy: 0.8065 - lr: 1.2500e-04\n",
            "Epoch 22/30\n",
            "625/625 [==============================] - 17s 27ms/step - loss: 0.5146 - accuracy: 0.8225 - val_loss: 0.6347 - val_accuracy: 0.7901 - lr: 1.2500e-04\n",
            "Epoch 23/30\n",
            "625/625 [==============================] - 16s 25ms/step - loss: 0.5186 - accuracy: 0.8196 - val_loss: 0.5610 - val_accuracy: 0.8145 - lr: 1.2500e-04\n",
            "Epoch 24/30\n",
            "625/625 [==============================] - 16s 25ms/step - loss: 0.5128 - accuracy: 0.8218 - val_loss: 0.6031 - val_accuracy: 0.7996 - lr: 1.2500e-04\n",
            "Epoch 25/30\n",
            "625/625 [==============================] - 15s 24ms/step - loss: 0.5104 - accuracy: 0.8231 - val_loss: 0.5741 - val_accuracy: 0.8085 - lr: 1.2500e-04\n",
            "Epoch 26/30\n",
            "625/625 [==============================] - 15s 24ms/step - loss: 0.5063 - accuracy: 0.8255 - val_loss: 0.5654 - val_accuracy: 0.8137 - lr: 1.2500e-04\n",
            "Epoch 27/30\n",
            "625/625 [==============================] - 15s 24ms/step - loss: 0.5030 - accuracy: 0.8249 - val_loss: 0.5634 - val_accuracy: 0.8133 - lr: 1.2500e-04\n",
            "Epoch 28/30\n",
            "625/625 [==============================] - 15s 24ms/step - loss: 0.4971 - accuracy: 0.8260 - val_loss: 0.5614 - val_accuracy: 0.8138 - lr: 1.2500e-04\n",
            "Epoch 29/30\n",
            "625/625 [==============================] - 15s 24ms/step - loss: 0.5023 - accuracy: 0.8246 - val_loss: 0.5835 - val_accuracy: 0.8103 - lr: 1.2500e-04\n",
            "Epoch 30/30\n",
            "625/625 [==============================] - 16s 25ms/step - loss: 0.4927 - accuracy: 0.8270 - val_loss: 0.5505 - val_accuracy: 0.8169 - lr: 1.2500e-04\n",
            "Score for fold 1: loss of 0.5505257248878479; accuracy of 81.69000148773193%\n",
            "Model: \"sequential_9\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_27 (Conv2D)          (None, 32, 32, 32)        896       \n",
            "                                                                 \n",
            " batch_normalization_6 (Batc  (None, 32, 32, 32)       128       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " max_pooling2d_9 (MaxPooling  multiple                 0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_28 (Conv2D)          (None, 16, 16, 64)        18496     \n",
            "                                                                 \n",
            " batch_normalization_7 (Batc  (None, 16, 16, 64)       256       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " dropout_9 (Dropout)         (None, 8, 8, 64)          0         \n",
            "                                                                 \n",
            " conv2d_29 (Conv2D)          (None, 8, 8, 128)         73856     \n",
            "                                                                 \n",
            " batch_normalization_8 (Batc  (None, 8, 8, 128)        512       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " flatten_9 (Flatten)         (None, 2048)              0         \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 10)                20490     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 114,634\n",
            "Trainable params: 114,186\n",
            "Non-trainable params: 448\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/30\n",
            "625/625 [==============================] - 16s 25ms/step - loss: 1.6272 - accuracy: 0.4559 - val_loss: 1.2948 - val_accuracy: 0.5563 - lr: 0.0010\n",
            "Epoch 2/30\n",
            "625/625 [==============================] - 15s 24ms/step - loss: 1.2320 - accuracy: 0.5755 - val_loss: 1.1634 - val_accuracy: 0.6055 - lr: 0.0010\n",
            "Epoch 3/30\n",
            "625/625 [==============================] - 15s 24ms/step - loss: 1.0918 - accuracy: 0.6238 - val_loss: 1.1212 - val_accuracy: 0.6204 - lr: 0.0010\n",
            "Epoch 4/30\n",
            "625/625 [==============================] - 16s 26ms/step - loss: 1.0070 - accuracy: 0.6524 - val_loss: 1.0359 - val_accuracy: 0.6442 - lr: 0.0010\n",
            "Epoch 5/30\n",
            "318/625 [==============>...............] - ETA: 7s - loss: 0.9464 - accuracy: 0.6747"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_2392/328375781.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'Training for fold {fold_no} ...'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m     history = model.train(\n\u001b[0m\u001b[0;32m     21\u001b[0m               \u001b[0mtrain_gen\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m                 \u001b[0mtrainX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_2392/846801809.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, train_gen, train_x, train_y, test_x, test_y, lr_rate, lr_rate_type)\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_gen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtensorboard_callback\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcallback\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 165\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_gen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtensorboard_callback\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcallback\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m#  self.model.fit(train_x, train_y, validation_data=(test_x, test_y) , epochs=25, verbose=1)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    166\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_gen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtensorboard_callback\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1382\u001b[0m                 _r=1):\n\u001b[0;32m   1383\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1384\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1385\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1386\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    913\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    914\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 915\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    945\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    946\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 947\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    948\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2954\u001b[0m       (graph_function,\n\u001b[0;32m   2955\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2956\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   2957\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   2958\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1851\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1852\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1853\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1854\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1855\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    498\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 499\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    500\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    501\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Batch size 64"
      ],
      "metadata": {
        "id": "vx-7cVTv0_lQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "acc_per_fold = []\n",
        "loss_per_fold = []\n",
        "\n",
        "with tf.device('/device:GPU:0'):\n",
        "  fold_no = 1\n",
        "  for train, valid in kfold.split(trainX, trainy):\n",
        "\n",
        "    model = CNN_Model(batch_size=64, depth=3, hidden_neurons_1=32, hidden_neurons_2=64, hidden_neurons_3=128, kernel_size_1=3, kernel_size_2=3, kernel_size_3=3, \n",
        "               dropout_rate=0.2, batch_norm=False, pooling='max', layer_activation_1='relu', layer_activation_2='relu', layer_activation_3='relu',\n",
        "               kernel_initialiser='glorot_uniform', bias_initialiser='zeros', optimiser='Adam', weight_decay=None, learning_rate=1e-3, output_activation='softmax', SGD_momentum=0) \n",
        "  \n",
        "    train_gen = tf.keras.preprocessing.image.ImageDataGenerator(width_shift_range=0.1,\n",
        "      height_shift_range=0.1,\n",
        "      horizontal_flip=True)\n",
        "    train_gen.fit(trainX[train])\n",
        "  \n",
        "    print('------------------------------------------------------------------------')\n",
        "    print(f'Training for fold {fold_no} ...')\n",
        "\n",
        "    history = model.train(\n",
        "              train_gen,\n",
        "                trainX[train],\n",
        "                  trainy[train],\n",
        "                trainX[valid],\n",
        "                  trainy[valid],\n",
        "                  lr_rate_type='step')\n",
        "\n",
        "    scores = model.evaluate(trainX[valid], trainy[valid])\n",
        "\n",
        "    print(f'Score for fold {fold_no}: loss of {scores[0]}; accuracy of {scores[1]*100}%')\n",
        "    acc_per_fold.append(scores[1] * 100)\n",
        "    loss_per_fold.append(scores[0])\n",
        "\n",
        "    fold_no+=1\n",
        "\n",
        "# == Provide average scores ==\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Score per fold')\n",
        "for i in range(0, len(acc_per_fold)):\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Average scores for all folds:')\n",
        "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
        "print(f'> Loss: {np.mean(loss_per_fold)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "aMIr8oQq597O",
        "outputId": "738a77fe-4c4a-490c-c792-56d620fac5d9"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_10\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_30 (Conv2D)          (None, 32, 32, 32)        896       \n",
            "                                                                 \n",
            " max_pooling2d_10 (MaxPoolin  multiple                 0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_31 (Conv2D)          (None, 16, 16, 64)        18496     \n",
            "                                                                 \n",
            " dropout_10 (Dropout)        (None, 8, 8, 64)          0         \n",
            "                                                                 \n",
            " conv2d_32 (Conv2D)          (None, 8, 8, 128)         73856     \n",
            "                                                                 \n",
            " flatten_10 (Flatten)        (None, 2048)              0         \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 10)                20490     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 113,738\n",
            "Trainable params: 113,738\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/30\n",
            "625/625 [==============================] - 18s 27ms/step - loss: 2.5374 - accuracy: 0.3379 - val_loss: 1.5302 - val_accuracy: 0.4470 - lr: 0.0010\n",
            "Epoch 2/30\n",
            "625/625 [==============================] - 17s 27ms/step - loss: 1.4995 - accuracy: 0.4633 - val_loss: 1.4261 - val_accuracy: 0.4980 - lr: 0.0010\n",
            "Epoch 3/30\n",
            "625/625 [==============================] - 16s 25ms/step - loss: 1.3810 - accuracy: 0.5097 - val_loss: 1.2679 - val_accuracy: 0.5478 - lr: 0.0010\n",
            "Epoch 4/30\n",
            "625/625 [==============================] - 16s 25ms/step - loss: 1.2988 - accuracy: 0.5378 - val_loss: 1.2309 - val_accuracy: 0.5655 - lr: 0.0010\n",
            "Epoch 5/30\n",
            "625/625 [==============================] - 16s 26ms/step - loss: 1.2402 - accuracy: 0.5667 - val_loss: 1.2290 - val_accuracy: 0.5745 - lr: 0.0010\n",
            "Epoch 6/30\n",
            "625/625 [==============================] - 15s 24ms/step - loss: 1.1913 - accuracy: 0.5798 - val_loss: 1.0596 - val_accuracy: 0.6313 - lr: 0.0010\n",
            "Epoch 7/30\n",
            "625/625 [==============================] - 15s 24ms/step - loss: 1.1703 - accuracy: 0.5900 - val_loss: 1.0497 - val_accuracy: 0.6297 - lr: 0.0010\n",
            "Epoch 8/30\n",
            "625/625 [==============================] - 15s 25ms/step - loss: 1.1335 - accuracy: 0.6056 - val_loss: 1.0068 - val_accuracy: 0.6544 - lr: 0.0010\n",
            "Epoch 9/30\n",
            "625/625 [==============================] - 16s 26ms/step - loss: 1.1095 - accuracy: 0.6160 - val_loss: 0.9732 - val_accuracy: 0.6592 - lr: 0.0010\n",
            "Epoch 10/30\n",
            "625/625 [==============================] - 17s 27ms/step - loss: 1.0939 - accuracy: 0.6207 - val_loss: 0.9647 - val_accuracy: 0.6663 - lr: 0.0010\n",
            "Epoch 11/30\n",
            "625/625 [==============================] - 17s 28ms/step - loss: 0.9961 - accuracy: 0.6537 - val_loss: 0.9633 - val_accuracy: 0.6636 - lr: 5.0000e-04\n",
            "Epoch 12/30\n",
            "625/625 [==============================] - 16s 26ms/step - loss: 0.9673 - accuracy: 0.6675 - val_loss: 0.9236 - val_accuracy: 0.6836 - lr: 5.0000e-04\n",
            "Epoch 13/30\n",
            "625/625 [==============================] - 16s 25ms/step - loss: 0.9405 - accuracy: 0.6731 - val_loss: 0.9298 - val_accuracy: 0.6812 - lr: 5.0000e-04\n",
            "Epoch 14/30\n",
            "625/625 [==============================] - 17s 27ms/step - loss: 0.9228 - accuracy: 0.6824 - val_loss: 0.9221 - val_accuracy: 0.6885 - lr: 5.0000e-04\n",
            "Epoch 15/30\n",
            "625/625 [==============================] - 17s 28ms/step - loss: 0.9124 - accuracy: 0.6862 - val_loss: 0.8668 - val_accuracy: 0.7039 - lr: 5.0000e-04\n",
            "Epoch 16/30\n",
            "625/625 [==============================] - 17s 27ms/step - loss: 0.9020 - accuracy: 0.6897 - val_loss: 0.8450 - val_accuracy: 0.7112 - lr: 5.0000e-04\n",
            "Epoch 17/30\n",
            "625/625 [==============================] - 17s 28ms/step - loss: 0.8927 - accuracy: 0.6927 - val_loss: 0.8719 - val_accuracy: 0.6965 - lr: 5.0000e-04\n",
            "Epoch 18/30\n",
            "625/625 [==============================] - 17s 28ms/step - loss: 0.8789 - accuracy: 0.6949 - val_loss: 0.8665 - val_accuracy: 0.7080 - lr: 5.0000e-04\n",
            "Epoch 19/30\n",
            "625/625 [==============================] - 16s 26ms/step - loss: 0.8720 - accuracy: 0.6980 - val_loss: 0.8310 - val_accuracy: 0.7158 - lr: 5.0000e-04\n",
            "Epoch 20/30\n",
            "625/625 [==============================] - 18s 28ms/step - loss: 0.8634 - accuracy: 0.7023 - val_loss: 0.8193 - val_accuracy: 0.7172 - lr: 5.0000e-04\n",
            "Epoch 21/30\n",
            "625/625 [==============================] - 17s 27ms/step - loss: 0.7880 - accuracy: 0.7268 - val_loss: 0.7926 - val_accuracy: 0.7297 - lr: 1.2500e-04\n",
            "Epoch 22/30\n",
            "625/625 [==============================] - 19s 30ms/step - loss: 0.7695 - accuracy: 0.7349 - val_loss: 0.7863 - val_accuracy: 0.7310 - lr: 1.2500e-04\n",
            "Epoch 23/30\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.7587 - accuracy: 0.7386 - val_loss: 0.7687 - val_accuracy: 0.7402 - lr: 1.2500e-04\n",
            "Epoch 24/30\n",
            "625/625 [==============================] - 17s 27ms/step - loss: 0.7503 - accuracy: 0.7398 - val_loss: 0.7446 - val_accuracy: 0.7468 - lr: 1.2500e-04\n",
            "Epoch 25/30\n",
            "625/625 [==============================] - 18s 28ms/step - loss: 0.7407 - accuracy: 0.7420 - val_loss: 0.7799 - val_accuracy: 0.7319 - lr: 1.2500e-04\n",
            "Epoch 26/30\n",
            "625/625 [==============================] - 18s 28ms/step - loss: 0.7417 - accuracy: 0.7459 - val_loss: 0.7608 - val_accuracy: 0.7405 - lr: 1.2500e-04\n",
            "Epoch 27/30\n",
            "625/625 [==============================] - 17s 27ms/step - loss: 0.7318 - accuracy: 0.7469 - val_loss: 0.7622 - val_accuracy: 0.7405 - lr: 1.2500e-04\n",
            "Epoch 28/30\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.7315 - accuracy: 0.7499 - val_loss: 0.7359 - val_accuracy: 0.7454 - lr: 1.2500e-04\n",
            "Epoch 29/30\n",
            "625/625 [==============================] - 17s 27ms/step - loss: 0.7242 - accuracy: 0.7512 - val_loss: 0.7308 - val_accuracy: 0.7519 - lr: 1.2500e-04\n",
            "Epoch 30/30\n",
            "625/625 [==============================] - 16s 26ms/step - loss: 0.7163 - accuracy: 0.7530 - val_loss: 0.7344 - val_accuracy: 0.7496 - lr: 1.2500e-04\n",
            "Score for fold 1: loss of 0.7343544960021973; accuracy of 74.95999932289124%\n",
            "Model: \"sequential_11\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_33 (Conv2D)          (None, 32, 32, 32)        896       \n",
            "                                                                 \n",
            " max_pooling2d_11 (MaxPoolin  multiple                 0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_34 (Conv2D)          (None, 16, 16, 64)        18496     \n",
            "                                                                 \n",
            " dropout_11 (Dropout)        (None, 8, 8, 64)          0         \n",
            "                                                                 \n",
            " conv2d_35 (Conv2D)          (None, 8, 8, 128)         73856     \n",
            "                                                                 \n",
            " flatten_11 (Flatten)        (None, 2048)              0         \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 10)                20490     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 113,738\n",
            "Trainable params: 113,738\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/30\n",
            "625/625 [==============================] - 18s 28ms/step - loss: 3.0061 - accuracy: 0.3392 - val_loss: 1.5787 - val_accuracy: 0.4504 - lr: 0.0010\n",
            "Epoch 2/30\n",
            "625/625 [==============================] - 16s 26ms/step - loss: 1.5262 - accuracy: 0.4499 - val_loss: 1.3456 - val_accuracy: 0.5200 - lr: 0.0010\n",
            "Epoch 3/30\n",
            "625/625 [==============================] - 17s 27ms/step - loss: 1.4023 - accuracy: 0.4982 - val_loss: 1.3850 - val_accuracy: 0.5140 - lr: 0.0010\n",
            "Epoch 4/30\n",
            " 20/625 [..............................] - ETA: 14s - loss: 1.4044 - accuracy: 0.5109"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_2392/1637200103.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'Training for fold {fold_no} ...'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m     history = model.train(\n\u001b[0m\u001b[0;32m     21\u001b[0m               \u001b[0mtrain_gen\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m                 \u001b[0mtrainX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_2392/846801809.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, train_gen, train_x, train_y, test_x, test_y, lr_rate, lr_rate_type)\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_gen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtensorboard_callback\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcallback\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 165\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_gen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtensorboard_callback\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcallback\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m#  self.model.fit(train_x, train_y, validation_data=(test_x, test_y) , epochs=25, verbose=1)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    166\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_gen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtensorboard_callback\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1382\u001b[0m                 _r=1):\n\u001b[0;32m   1383\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1384\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1385\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1386\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    913\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    914\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 915\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    945\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    946\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 947\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    948\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2954\u001b[0m       (graph_function,\n\u001b[0;32m   2955\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2956\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   2957\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   2958\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1851\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1852\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1853\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1854\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1855\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    498\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 499\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    500\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    501\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Batch norm with batch size 256"
      ],
      "metadata": {
        "id": "ssDSn2Aq1Qji"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "acc_per_fold = []\n",
        "loss_per_fold = []\n",
        "\n",
        "with tf.device('/device:GPU:0'):\n",
        "  fold_no = 1\n",
        "  for train, valid in kfold.split(trainX, trainy):\n",
        "\n",
        "    model = CNN_Model(batch_size=256, depth=3, hidden_neurons_1=32, hidden_neurons_2=64, hidden_neurons_3=128, kernel_size_1=3, kernel_size_2=3, kernel_size_3=3, \n",
        "               dropout_rate=0.2, batch_norm=True, pooling='max', layer_activation_1='relu', layer_activation_2='relu', layer_activation_3='relu',\n",
        "               kernel_initialiser='glorot_uniform', bias_initialiser='zeros', optimiser='Adam', weight_decay=None, learning_rate=1e-3, output_activation='softmax', SGD_momentum=0) \n",
        "  \n",
        "    train_gen = tf.keras.preprocessing.image.ImageDataGenerator(width_shift_range=0.1,\n",
        "      height_shift_range=0.1,\n",
        "      horizontal_flip=True)\n",
        "    train_gen.fit(trainX[train])\n",
        "  \n",
        "    print('------------------------------------------------------------------------')\n",
        "    print(f'Training for fold {fold_no} ...')\n",
        "\n",
        "    history = model.train(\n",
        "              train_gen,\n",
        "                trainX[train],\n",
        "                  trainy[train],\n",
        "                trainX[valid],\n",
        "                  trainy[valid],\n",
        "                  lr_rate_type='step')\n",
        "\n",
        "    scores = model.evaluate(trainX[valid], trainy[valid])\n",
        "\n",
        "    print(f'Score for fold {fold_no}: loss of {scores[0]}; accuracy of {scores[1]*100}%')\n",
        "    acc_per_fold.append(scores[1] * 100)\n",
        "    loss_per_fold.append(scores[0])\n",
        "\n",
        "    fold_no+=1\n",
        "\n",
        "# == Provide average scores ==\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Score per fold')\n",
        "for i in range(0, len(acc_per_fold)):\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Average scores for all folds:')\n",
        "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
        "print(f'> Loss: {np.mean(loss_per_fold)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "wc0T7eRt67_t",
        "outputId": "2184753d-bfa0-49ba-82e3-3e618a76f5f0"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_12\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_36 (Conv2D)          (None, 32, 32, 32)        896       \n",
            "                                                                 \n",
            " batch_normalization_9 (Batc  (None, 32, 32, 32)       128       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " max_pooling2d_12 (MaxPoolin  multiple                 0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_37 (Conv2D)          (None, 16, 16, 64)        18496     \n",
            "                                                                 \n",
            " batch_normalization_10 (Bat  (None, 16, 16, 64)       256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dropout_12 (Dropout)        (None, 8, 8, 64)          0         \n",
            "                                                                 \n",
            " conv2d_38 (Conv2D)          (None, 8, 8, 128)         73856     \n",
            "                                                                 \n",
            " batch_normalization_11 (Bat  (None, 8, 8, 128)        512       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " flatten_12 (Flatten)        (None, 2048)              0         \n",
            "                                                                 \n",
            " dense_12 (Dense)            (None, 10)                20490     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 114,634\n",
            "Trainable params: 114,186\n",
            "Non-trainable params: 448\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/30\n",
            "156/156 [==============================] - 15s 92ms/step - loss: 1.7308 - accuracy: 0.4267 - val_loss: 1.7652 - val_accuracy: 0.4251 - lr: 0.0010\n",
            "Epoch 2/30\n",
            "156/156 [==============================] - 15s 98ms/step - loss: 1.3104 - accuracy: 0.5427 - val_loss: 1.3532 - val_accuracy: 0.5386 - lr: 0.0010\n",
            "Epoch 3/30\n",
            "156/156 [==============================] - 16s 105ms/step - loss: 1.1604 - accuracy: 0.5962 - val_loss: 1.3299 - val_accuracy: 0.5839 - lr: 0.0010\n",
            "Epoch 4/30\n",
            "156/156 [==============================] - 15s 93ms/step - loss: 1.0749 - accuracy: 0.6262 - val_loss: 1.0726 - val_accuracy: 0.6332 - lr: 0.0010\n",
            "Epoch 5/30\n",
            "156/156 [==============================] - 15s 99ms/step - loss: 1.0110 - accuracy: 0.6493 - val_loss: 1.0391 - val_accuracy: 0.6480 - lr: 0.0010\n",
            "Epoch 6/30\n",
            "156/156 [==============================] - 15s 96ms/step - loss: 0.9576 - accuracy: 0.6660 - val_loss: 0.9571 - val_accuracy: 0.6803 - lr: 0.0010\n",
            "Epoch 7/30\n",
            "156/156 [==============================] - 15s 97ms/step - loss: 0.9198 - accuracy: 0.6811 - val_loss: 0.9916 - val_accuracy: 0.6748 - lr: 0.0010\n",
            "Epoch 8/30\n",
            "156/156 [==============================] - 16s 102ms/step - loss: 0.8899 - accuracy: 0.6903 - val_loss: 0.9833 - val_accuracy: 0.6797 - lr: 0.0010\n",
            "Epoch 9/30\n",
            "156/156 [==============================] - 15s 96ms/step - loss: 0.8585 - accuracy: 0.7010 - val_loss: 0.9192 - val_accuracy: 0.6986 - lr: 0.0010\n",
            "Epoch 10/30\n",
            "156/156 [==============================] - 15s 96ms/step - loss: 0.8329 - accuracy: 0.7125 - val_loss: 0.8648 - val_accuracy: 0.7118 - lr: 0.0010\n",
            "Epoch 11/30\n",
            "156/156 [==============================] - 15s 95ms/step - loss: 0.7641 - accuracy: 0.7341 - val_loss: 0.7742 - val_accuracy: 0.7364 - lr: 5.0000e-04\n",
            "Epoch 12/30\n",
            "156/156 [==============================] - 15s 93ms/step - loss: 0.7398 - accuracy: 0.7450 - val_loss: 0.7853 - val_accuracy: 0.7316 - lr: 5.0000e-04\n",
            "Epoch 13/30\n",
            "156/156 [==============================] - 15s 94ms/step - loss: 0.7351 - accuracy: 0.7466 - val_loss: 0.7449 - val_accuracy: 0.7458 - lr: 5.0000e-04\n",
            "Epoch 14/30\n",
            "156/156 [==============================] - 15s 95ms/step - loss: 0.7265 - accuracy: 0.7473 - val_loss: 0.7808 - val_accuracy: 0.7409 - lr: 5.0000e-04\n",
            "Epoch 15/30\n",
            "156/156 [==============================] - 15s 96ms/step - loss: 0.7156 - accuracy: 0.7500 - val_loss: 0.7545 - val_accuracy: 0.7429 - lr: 5.0000e-04\n",
            "Epoch 16/30\n",
            "156/156 [==============================] - 15s 94ms/step - loss: 0.7063 - accuracy: 0.7547 - val_loss: 0.8824 - val_accuracy: 0.7125 - lr: 5.0000e-04\n",
            "Epoch 17/30\n",
            "156/156 [==============================] - 16s 102ms/step - loss: 0.6962 - accuracy: 0.7586 - val_loss: 0.7576 - val_accuracy: 0.7488 - lr: 5.0000e-04\n",
            "Epoch 18/30\n",
            "156/156 [==============================] - 15s 96ms/step - loss: 0.6879 - accuracy: 0.7616 - val_loss: 0.7867 - val_accuracy: 0.7352 - lr: 5.0000e-04\n",
            "Epoch 19/30\n",
            "156/156 [==============================] - 15s 95ms/step - loss: 0.6753 - accuracy: 0.7645 - val_loss: 0.8016 - val_accuracy: 0.7381 - lr: 5.0000e-04\n",
            "Epoch 20/30\n",
            "156/156 [==============================] - 15s 95ms/step - loss: 0.6711 - accuracy: 0.7669 - val_loss: 0.6969 - val_accuracy: 0.7634 - lr: 5.0000e-04\n",
            "Epoch 21/30\n",
            "156/156 [==============================] - 15s 96ms/step - loss: 0.6288 - accuracy: 0.7807 - val_loss: 0.6732 - val_accuracy: 0.7736 - lr: 1.2500e-04\n",
            "Epoch 22/30\n",
            "156/156 [==============================] - 15s 94ms/step - loss: 0.6142 - accuracy: 0.7860 - val_loss: 0.6775 - val_accuracy: 0.7677 - lr: 1.2500e-04\n",
            "Epoch 23/30\n",
            "156/156 [==============================] - 15s 97ms/step - loss: 0.6178 - accuracy: 0.7857 - val_loss: 0.6743 - val_accuracy: 0.7749 - lr: 1.2500e-04\n",
            "Epoch 24/30\n",
            "156/156 [==============================] - 16s 99ms/step - loss: 0.6141 - accuracy: 0.7887 - val_loss: 0.6743 - val_accuracy: 0.7732 - lr: 1.2500e-04\n",
            "Epoch 25/30\n",
            "156/156 [==============================] - 15s 96ms/step - loss: 0.6080 - accuracy: 0.7887 - val_loss: 0.6678 - val_accuracy: 0.7751 - lr: 1.2500e-04\n",
            "Epoch 26/30\n",
            "156/156 [==============================] - 15s 98ms/step - loss: 0.6040 - accuracy: 0.7912 - val_loss: 0.6407 - val_accuracy: 0.7826 - lr: 1.2500e-04\n",
            "Epoch 27/30\n",
            "156/156 [==============================] - 15s 99ms/step - loss: 0.6047 - accuracy: 0.7912 - val_loss: 0.6555 - val_accuracy: 0.7749 - lr: 1.2500e-04\n",
            "Epoch 28/30\n",
            "156/156 [==============================] - 15s 97ms/step - loss: 0.5963 - accuracy: 0.7935 - val_loss: 0.6566 - val_accuracy: 0.7807 - lr: 1.2500e-04\n",
            "Epoch 29/30\n",
            "156/156 [==============================] - 15s 99ms/step - loss: 0.5985 - accuracy: 0.7911 - val_loss: 0.6256 - val_accuracy: 0.7921 - lr: 1.2500e-04\n",
            "Epoch 30/30\n",
            "156/156 [==============================] - 16s 99ms/step - loss: 0.5914 - accuracy: 0.7943 - val_loss: 0.6442 - val_accuracy: 0.7814 - lr: 1.2500e-04\n",
            "Score for fold 1: loss of 0.6442309021949768; accuracy of 78.14000248908997%\n",
            "Model: \"sequential_13\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_39 (Conv2D)          (None, 32, 32, 32)        896       \n",
            "                                                                 \n",
            " batch_normalization_12 (Bat  (None, 32, 32, 32)       128       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling2d_13 (MaxPoolin  multiple                 0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_40 (Conv2D)          (None, 16, 16, 64)        18496     \n",
            "                                                                 \n",
            " batch_normalization_13 (Bat  (None, 16, 16, 64)       256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dropout_13 (Dropout)        (None, 8, 8, 64)          0         \n",
            "                                                                 \n",
            " conv2d_41 (Conv2D)          (None, 8, 8, 128)         73856     \n",
            "                                                                 \n",
            " batch_normalization_14 (Bat  (None, 8, 8, 128)        512       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " flatten_13 (Flatten)        (None, 2048)              0         \n",
            "                                                                 \n",
            " dense_13 (Dense)            (None, 10)                20490     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 114,634\n",
            "Trainable params: 114,186\n",
            "Non-trainable params: 448\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/30\n",
            " 36/156 [=====>........................] - ETA: 10s - loss: 2.2008 - accuracy: 0.3264"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_2392/2379193426.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'Training for fold {fold_no} ...'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m     history = model.train(\n\u001b[0m\u001b[0;32m     21\u001b[0m               \u001b[0mtrain_gen\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m                 \u001b[0mtrainX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_2392/846801809.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, train_gen, train_x, train_y, test_x, test_y, lr_rate, lr_rate_type)\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_gen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtensorboard_callback\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcallback\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 165\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_gen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtensorboard_callback\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcallback\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m#  self.model.fit(train_x, train_y, validation_data=(test_x, test_y) , epochs=25, verbose=1)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    166\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_gen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtensorboard_callback\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1382\u001b[0m                 _r=1):\n\u001b[0;32m   1383\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1384\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1385\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1386\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    913\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    914\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 915\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    945\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    946\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 947\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    948\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2954\u001b[0m       (graph_function,\n\u001b[0;32m   2955\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2956\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   2957\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   2958\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1851\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1852\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1853\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1854\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1855\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    498\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 499\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    500\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    501\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Batch size 256"
      ],
      "metadata": {
        "id": "UDcbHWdA1Wy5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "acc_per_fold = []\n",
        "loss_per_fold = []\n",
        "\n",
        "with tf.device('/device:GPU:0'):\n",
        "  fold_no = 1\n",
        "  for train, valid in kfold.split(trainX, trainy):\n",
        "\n",
        "    model = CNN_Model(batch_size=256, depth=3, hidden_neurons_1=32, hidden_neurons_2=64, hidden_neurons_3=128, kernel_size_1=3, kernel_size_2=3, kernel_size_3=3, \n",
        "               dropout_rate=0.2, batch_norm=False, pooling='max', layer_activation_1='relu', layer_activation_2='relu', layer_activation_3='relu',\n",
        "               kernel_initialiser='glorot_uniform', bias_initialiser='zeros', optimiser='Adam', weight_decay=None, learning_rate=1e-3, output_activation='softmax', SGD_momentum=0)\n",
        "  \n",
        "    train_gen = tf.keras.preprocessing.image.ImageDataGenerator(width_shift_range=0.1,\n",
        "      height_shift_range=0.1,\n",
        "      horizontal_flip=True)\n",
        "    train_gen.fit(trainX[train])\n",
        "  \n",
        "    print('------------------------------------------------------------------------')\n",
        "    print(f'Training for fold {fold_no} ...')\n",
        "\n",
        "    history = model.train(\n",
        "              train_gen,\n",
        "                trainX[train],\n",
        "                  trainy[train],\n",
        "                trainX[valid],\n",
        "                  trainy[valid],\n",
        "                  lr_rate_type='step')\n",
        "\n",
        "    scores = model.evaluate(trainX[valid], trainy[valid])\n",
        "\n",
        "    print(f'Score for fold {fold_no}: loss of {scores[0]}; accuracy of {scores[1]*100}%')\n",
        "    acc_per_fold.append(scores[1] * 100)\n",
        "    loss_per_fold.append(scores[0])\n",
        "\n",
        "    fold_no+=1\n",
        "\n",
        "# == Provide average scores ==\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Score per fold')\n",
        "for i in range(0, len(acc_per_fold)):\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Average scores for all folds:')\n",
        "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
        "print(f'> Loss: {np.mean(loss_per_fold)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dm-1rWDf65Sg",
        "outputId": "4ec98fb9-4667-411d-b4df-3eaf6e6c1352"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_6 (Conv2D)           (None, 32, 32, 32)        896       \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPooling  multiple                 0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_7 (Conv2D)           (None, 16, 16, 64)        18496     \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 8, 8, 64)          0         \n",
            "                                                                 \n",
            " conv2d_8 (Conv2D)           (None, 8, 8, 128)         73856     \n",
            "                                                                 \n",
            " flatten_2 (Flatten)         (None, 2048)              0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 10)                20490     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 113,738\n",
            "Trainable params: 113,738\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/30\n",
            "156/156 [==============================] - 16s 96ms/step - loss: 4.2001 - accuracy: 0.2618 - val_loss: 1.6630 - val_accuracy: 0.4048 - lr: 0.0010\n",
            "Epoch 2/30\n",
            "156/156 [==============================] - 14s 91ms/step - loss: 1.6343 - accuracy: 0.4074 - val_loss: 1.5341 - val_accuracy: 0.4508 - lr: 0.0010\n",
            "Epoch 3/30\n",
            "156/156 [==============================] - 14s 88ms/step - loss: 1.5126 - accuracy: 0.4566 - val_loss: 1.4093 - val_accuracy: 0.4981 - lr: 0.0010\n",
            "Epoch 4/30\n",
            "156/156 [==============================] - 14s 89ms/step - loss: 1.4533 - accuracy: 0.4826 - val_loss: 1.4116 - val_accuracy: 0.4848 - lr: 0.0010\n",
            "Epoch 5/30\n",
            "156/156 [==============================] - 14s 90ms/step - loss: 1.3794 - accuracy: 0.5122 - val_loss: 1.3257 - val_accuracy: 0.5353 - lr: 0.0010\n",
            "Epoch 6/30\n",
            "156/156 [==============================] - 14s 90ms/step - loss: 1.3341 - accuracy: 0.5279 - val_loss: 1.2338 - val_accuracy: 0.5677 - lr: 0.0010\n",
            "Epoch 7/30\n",
            "156/156 [==============================] - 14s 88ms/step - loss: 1.2803 - accuracy: 0.5473 - val_loss: 1.1835 - val_accuracy: 0.5817 - lr: 0.0010\n",
            "Epoch 8/30\n",
            "156/156 [==============================] - 14s 89ms/step - loss: 1.2386 - accuracy: 0.5632 - val_loss: 1.1164 - val_accuracy: 0.6063 - lr: 0.0010\n",
            "Epoch 9/30\n",
            "156/156 [==============================] - 14s 89ms/step - loss: 1.1947 - accuracy: 0.5807 - val_loss: 1.0850 - val_accuracy: 0.6181 - lr: 0.0010\n",
            "Epoch 10/30\n",
            "156/156 [==============================] - 15s 94ms/step - loss: 1.1622 - accuracy: 0.5947 - val_loss: 1.1072 - val_accuracy: 0.6155 - lr: 0.0010\n",
            "Epoch 11/30\n",
            "156/156 [==============================] - 14s 92ms/step - loss: 1.0919 - accuracy: 0.6178 - val_loss: 1.0205 - val_accuracy: 0.6487 - lr: 5.0000e-04\n",
            "Epoch 12/30\n",
            "156/156 [==============================] - 14s 90ms/step - loss: 1.0793 - accuracy: 0.6216 - val_loss: 1.0480 - val_accuracy: 0.6428 - lr: 5.0000e-04\n",
            "Epoch 13/30\n",
            "156/156 [==============================] - 14s 91ms/step - loss: 1.0490 - accuracy: 0.6342 - val_loss: 0.9585 - val_accuracy: 0.6753 - lr: 5.0000e-04\n",
            "Epoch 14/30\n",
            "156/156 [==============================] - 14s 91ms/step - loss: 1.0299 - accuracy: 0.6421 - val_loss: 0.9635 - val_accuracy: 0.6705 - lr: 5.0000e-04\n",
            "Epoch 15/30\n",
            "156/156 [==============================] - 14s 91ms/step - loss: 1.0095 - accuracy: 0.6464 - val_loss: 1.0061 - val_accuracy: 0.6581 - lr: 5.0000e-04\n",
            "Epoch 16/30\n",
            "156/156 [==============================] - 15s 93ms/step - loss: 1.0027 - accuracy: 0.6517 - val_loss: 0.9572 - val_accuracy: 0.6765 - lr: 5.0000e-04\n",
            "Epoch 17/30\n",
            "156/156 [==============================] - 15s 93ms/step - loss: 0.9935 - accuracy: 0.6575 - val_loss: 0.9120 - val_accuracy: 0.6891 - lr: 5.0000e-04\n",
            "Epoch 18/30\n",
            "156/156 [==============================] - 14s 91ms/step - loss: 0.9713 - accuracy: 0.6622 - val_loss: 0.9121 - val_accuracy: 0.6856 - lr: 5.0000e-04\n",
            "Epoch 19/30\n",
            "156/156 [==============================] - 15s 93ms/step - loss: 0.9591 - accuracy: 0.6661 - val_loss: 0.9201 - val_accuracy: 0.6831 - lr: 5.0000e-04\n",
            "Epoch 20/30\n",
            "156/156 [==============================] - 14s 92ms/step - loss: 0.9438 - accuracy: 0.6719 - val_loss: 0.8674 - val_accuracy: 0.7061 - lr: 5.0000e-04\n",
            "Epoch 21/30\n",
            "156/156 [==============================] - 14s 89ms/step - loss: 0.8976 - accuracy: 0.6878 - val_loss: 0.8508 - val_accuracy: 0.7130 - lr: 1.2500e-04\n",
            "Epoch 22/30\n",
            "156/156 [==============================] - 14s 88ms/step - loss: 0.8872 - accuracy: 0.6933 - val_loss: 0.8480 - val_accuracy: 0.7085 - lr: 1.2500e-04\n",
            "Epoch 23/30\n",
            "156/156 [==============================] - 14s 88ms/step - loss: 0.8811 - accuracy: 0.6964 - val_loss: 0.8378 - val_accuracy: 0.7147 - lr: 1.2500e-04\n",
            "Epoch 24/30\n",
            "156/156 [==============================] - 14s 92ms/step - loss: 0.8681 - accuracy: 0.6999 - val_loss: 0.8217 - val_accuracy: 0.7218 - lr: 1.2500e-04\n",
            "Epoch 25/30\n",
            "156/156 [==============================] - 14s 92ms/step - loss: 0.8691 - accuracy: 0.7009 - val_loss: 0.8517 - val_accuracy: 0.7126 - lr: 1.2500e-04\n",
            "Epoch 26/30\n",
            "156/156 [==============================] - 14s 88ms/step - loss: 0.8720 - accuracy: 0.6987 - val_loss: 0.8240 - val_accuracy: 0.7207 - lr: 1.2500e-04\n",
            "Epoch 27/30\n",
            "156/156 [==============================] - 14s 88ms/step - loss: 0.8608 - accuracy: 0.7031 - val_loss: 0.8476 - val_accuracy: 0.7152 - lr: 1.2500e-04\n",
            "Epoch 28/30\n",
            "156/156 [==============================] - 14s 88ms/step - loss: 0.8615 - accuracy: 0.7041 - val_loss: 0.8119 - val_accuracy: 0.7279 - lr: 1.2500e-04\n",
            "Epoch 29/30\n",
            "156/156 [==============================] - 14s 89ms/step - loss: 0.8556 - accuracy: 0.7042 - val_loss: 0.8115 - val_accuracy: 0.7269 - lr: 1.2500e-04\n",
            "Epoch 30/30\n",
            "156/156 [==============================] - 15s 94ms/step - loss: 0.8446 - accuracy: 0.7083 - val_loss: 0.8069 - val_accuracy: 0.7272 - lr: 1.2500e-04\n",
            "Score for fold 1: loss of 0.8069173097610474; accuracy of 72.71999716758728%\n",
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_9 (Conv2D)           (None, 32, 32, 32)        896       \n",
            "                                                                 \n",
            " max_pooling2d_3 (MaxPooling  multiple                 0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_10 (Conv2D)          (None, 16, 16, 64)        18496     \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 8, 8, 64)          0         \n",
            "                                                                 \n",
            " conv2d_11 (Conv2D)          (None, 8, 8, 128)         73856     \n",
            "                                                                 \n",
            " flatten_3 (Flatten)         (None, 2048)              0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 10)                20490     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 113,738\n",
            "Trainable params: 113,738\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/30\n",
            "156/156 [==============================] - 15s 91ms/step - loss: 3.1125 - accuracy: 0.2882 - val_loss: 1.5563 - val_accuracy: 0.4412 - lr: 0.0010\n",
            "Epoch 2/30\n",
            "156/156 [==============================] - 14s 89ms/step - loss: 1.5619 - accuracy: 0.4371 - val_loss: 1.4454 - val_accuracy: 0.4879 - lr: 0.0010\n",
            "Epoch 3/30\n",
            "156/156 [==============================] - 14s 89ms/step - loss: 1.4319 - accuracy: 0.4888 - val_loss: 1.3024 - val_accuracy: 0.5433 - lr: 0.0010\n",
            "Epoch 4/30\n",
            "156/156 [==============================] - 14s 89ms/step - loss: 1.3580 - accuracy: 0.5163 - val_loss: 1.2506 - val_accuracy: 0.5529 - lr: 0.0010\n",
            "Epoch 5/30\n",
            "156/156 [==============================] - 14s 89ms/step - loss: 1.2793 - accuracy: 0.5481 - val_loss: 1.2647 - val_accuracy: 0.5452 - lr: 0.0010\n",
            "Epoch 6/30\n",
            "156/156 [==============================] - 14s 90ms/step - loss: 1.2387 - accuracy: 0.5643 - val_loss: 1.1212 - val_accuracy: 0.6097 - lr: 0.0010\n",
            "Epoch 7/30\n",
            "156/156 [==============================] - 14s 90ms/step - loss: 1.2026 - accuracy: 0.5772 - val_loss: 1.0836 - val_accuracy: 0.6203 - lr: 0.0010\n",
            "Epoch 8/30\n",
            "156/156 [==============================] - 14s 89ms/step - loss: 1.1534 - accuracy: 0.5940 - val_loss: 1.1013 - val_accuracy: 0.6168 - lr: 0.0010\n",
            "Epoch 9/30\n",
            "156/156 [==============================] - 14s 90ms/step - loss: 1.1149 - accuracy: 0.6109 - val_loss: 0.9741 - val_accuracy: 0.6597 - lr: 0.0010\n",
            "Epoch 10/30\n",
            "156/156 [==============================] - 14s 88ms/step - loss: 1.0897 - accuracy: 0.6181 - val_loss: 1.0025 - val_accuracy: 0.6517 - lr: 0.0010\n",
            "Epoch 11/30\n",
            "156/156 [==============================] - 14s 90ms/step - loss: 1.0211 - accuracy: 0.6425 - val_loss: 0.9428 - val_accuracy: 0.6741 - lr: 5.0000e-04\n",
            "Epoch 12/30\n",
            "156/156 [==============================] - 14s 89ms/step - loss: 1.0061 - accuracy: 0.6525 - val_loss: 0.9859 - val_accuracy: 0.6647 - lr: 5.0000e-04\n",
            "Epoch 13/30\n",
            "156/156 [==============================] - 14s 90ms/step - loss: 0.9840 - accuracy: 0.6559 - val_loss: 0.9307 - val_accuracy: 0.6778 - lr: 5.0000e-04\n",
            "Epoch 14/30\n",
            "156/156 [==============================] - 14s 88ms/step - loss: 0.9670 - accuracy: 0.6636 - val_loss: 0.8928 - val_accuracy: 0.6961 - lr: 5.0000e-04\n",
            "Epoch 15/30\n",
            "156/156 [==============================] - 14s 89ms/step - loss: 0.9549 - accuracy: 0.6679 - val_loss: 0.8899 - val_accuracy: 0.6908 - lr: 5.0000e-04\n",
            "Epoch 16/30\n",
            "156/156 [==============================] - 14s 92ms/step - loss: 0.9432 - accuracy: 0.6736 - val_loss: 0.8889 - val_accuracy: 0.6942 - lr: 5.0000e-04\n",
            "Epoch 17/30\n",
            "156/156 [==============================] - 14s 90ms/step - loss: 0.9229 - accuracy: 0.6803 - val_loss: 0.9114 - val_accuracy: 0.6849 - lr: 5.0000e-04\n",
            "Epoch 18/30\n",
            "156/156 [==============================] - 14s 90ms/step - loss: 0.9130 - accuracy: 0.6810 - val_loss: 0.8706 - val_accuracy: 0.7065 - lr: 5.0000e-04\n",
            "Epoch 19/30\n",
            "156/156 [==============================] - 14s 89ms/step - loss: 0.9021 - accuracy: 0.6889 - val_loss: 0.8929 - val_accuracy: 0.6948 - lr: 5.0000e-04\n",
            "Epoch 20/30\n",
            "156/156 [==============================] - 14s 88ms/step - loss: 0.8945 - accuracy: 0.6895 - val_loss: 0.8571 - val_accuracy: 0.7087 - lr: 5.0000e-04\n",
            "Epoch 21/30\n",
            "156/156 [==============================] - 14s 89ms/step - loss: 0.8370 - accuracy: 0.7108 - val_loss: 0.8197 - val_accuracy: 0.7202 - lr: 1.2500e-04\n",
            "Epoch 22/30\n",
            "156/156 [==============================] - 14s 92ms/step - loss: 0.8301 - accuracy: 0.7149 - val_loss: 0.8339 - val_accuracy: 0.7133 - lr: 1.2500e-04\n",
            "Epoch 23/30\n",
            "156/156 [==============================] - 14s 89ms/step - loss: 0.8264 - accuracy: 0.7161 - val_loss: 0.8014 - val_accuracy: 0.7254 - lr: 1.2500e-04\n",
            "Epoch 24/30\n",
            "156/156 [==============================] - 14s 90ms/step - loss: 0.8178 - accuracy: 0.7176 - val_loss: 0.8056 - val_accuracy: 0.7252 - lr: 1.2500e-04\n",
            "Epoch 25/30\n",
            "156/156 [==============================] - 14s 90ms/step - loss: 0.8066 - accuracy: 0.7220 - val_loss: 0.7920 - val_accuracy: 0.7300 - lr: 1.2500e-04\n",
            "Epoch 26/30\n",
            "156/156 [==============================] - 14s 90ms/step - loss: 0.8121 - accuracy: 0.7195 - val_loss: 0.8082 - val_accuracy: 0.7238 - lr: 1.2500e-04\n",
            "Epoch 27/30\n",
            "156/156 [==============================] - 14s 90ms/step - loss: 0.8012 - accuracy: 0.7233 - val_loss: 0.7878 - val_accuracy: 0.7325 - lr: 1.2500e-04\n",
            "Epoch 28/30\n",
            "156/156 [==============================] - 14s 92ms/step - loss: 0.8001 - accuracy: 0.7227 - val_loss: 0.7802 - val_accuracy: 0.7330 - lr: 1.2500e-04\n",
            "Epoch 29/30\n",
            "156/156 [==============================] - 14s 91ms/step - loss: 0.7955 - accuracy: 0.7253 - val_loss: 0.8057 - val_accuracy: 0.7248 - lr: 1.2500e-04\n",
            "Epoch 30/30\n",
            "156/156 [==============================] - 14s 92ms/step - loss: 0.7930 - accuracy: 0.7277 - val_loss: 0.8108 - val_accuracy: 0.7240 - lr: 1.2500e-04\n",
            "Score for fold 2: loss of 0.8107749819755554; accuracy of 72.39999771118164%\n",
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_12 (Conv2D)          (None, 32, 32, 32)        896       \n",
            "                                                                 \n",
            " max_pooling2d_4 (MaxPooling  multiple                 0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_13 (Conv2D)          (None, 16, 16, 64)        18496     \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 8, 8, 64)          0         \n",
            "                                                                 \n",
            " conv2d_14 (Conv2D)          (None, 8, 8, 128)         73856     \n",
            "                                                                 \n",
            " flatten_4 (Flatten)         (None, 2048)              0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 10)                20490     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 113,738\n",
            "Trainable params: 113,738\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/30\n",
            "156/156 [==============================] - 15s 94ms/step - loss: 4.3009 - accuracy: 0.2561 - val_loss: 1.6719 - val_accuracy: 0.3880 - lr: 0.0010\n",
            "Epoch 2/30\n",
            "156/156 [==============================] - 14s 90ms/step - loss: 1.6677 - accuracy: 0.3971 - val_loss: 1.5314 - val_accuracy: 0.4506 - lr: 0.0010\n",
            "Epoch 3/30\n",
            "156/156 [==============================] - 14s 93ms/step - loss: 1.5326 - accuracy: 0.4472 - val_loss: 1.4797 - val_accuracy: 0.4797 - lr: 0.0010\n",
            "Epoch 4/30\n",
            "156/156 [==============================] - 15s 94ms/step - loss: 1.4503 - accuracy: 0.4813 - val_loss: 1.3524 - val_accuracy: 0.5266 - lr: 0.0010\n",
            "Epoch 5/30\n",
            "156/156 [==============================] - 15s 93ms/step - loss: 1.3820 - accuracy: 0.5073 - val_loss: 1.2675 - val_accuracy: 0.5529 - lr: 0.0010\n",
            "Epoch 6/30\n",
            "156/156 [==============================] - 15s 95ms/step - loss: 1.3171 - accuracy: 0.5334 - val_loss: 1.2507 - val_accuracy: 0.5597 - lr: 0.0010\n",
            "Epoch 7/30\n",
            "156/156 [==============================] - 14s 90ms/step - loss: 1.2746 - accuracy: 0.5510 - val_loss: 1.1489 - val_accuracy: 0.5973 - lr: 0.0010\n",
            "Epoch 8/30\n",
            "156/156 [==============================] - 14s 89ms/step - loss: 1.2444 - accuracy: 0.5636 - val_loss: 1.1375 - val_accuracy: 0.6052 - lr: 0.0010\n",
            "Epoch 9/30\n",
            "156/156 [==============================] - 14s 89ms/step - loss: 1.2066 - accuracy: 0.5721 - val_loss: 1.1371 - val_accuracy: 0.6093 - lr: 0.0010\n",
            "Epoch 10/30\n",
            "156/156 [==============================] - 14s 90ms/step - loss: 1.1762 - accuracy: 0.5865 - val_loss: 1.0783 - val_accuracy: 0.6286 - lr: 0.0010\n",
            "Epoch 11/30\n",
            "156/156 [==============================] - 14s 90ms/step - loss: 1.1067 - accuracy: 0.6134 - val_loss: 1.0268 - val_accuracy: 0.6481 - lr: 5.0000e-04\n",
            "Epoch 12/30\n",
            "156/156 [==============================] - 14s 89ms/step - loss: 1.0697 - accuracy: 0.6268 - val_loss: 1.0291 - val_accuracy: 0.6451 - lr: 5.0000e-04\n",
            "Epoch 13/30\n",
            "156/156 [==============================] - 14s 89ms/step - loss: 1.0608 - accuracy: 0.6324 - val_loss: 0.9844 - val_accuracy: 0.6606 - lr: 5.0000e-04\n",
            "Epoch 14/30\n",
            "156/156 [==============================] - 14s 89ms/step - loss: 1.0404 - accuracy: 0.6396 - val_loss: 1.0219 - val_accuracy: 0.6493 - lr: 5.0000e-04\n",
            "Epoch 15/30\n",
            "156/156 [==============================] - 14s 89ms/step - loss: 1.0235 - accuracy: 0.6434 - val_loss: 0.9586 - val_accuracy: 0.6746 - lr: 5.0000e-04\n",
            "Epoch 16/30\n",
            "156/156 [==============================] - 14s 89ms/step - loss: 1.0099 - accuracy: 0.6504 - val_loss: 0.9520 - val_accuracy: 0.6696 - lr: 5.0000e-04\n",
            "Epoch 17/30\n",
            "156/156 [==============================] - 14s 90ms/step - loss: 0.9949 - accuracy: 0.6534 - val_loss: 0.9726 - val_accuracy: 0.6669 - lr: 5.0000e-04\n",
            "Epoch 18/30\n",
            "156/156 [==============================] - 14s 89ms/step - loss: 0.9761 - accuracy: 0.6605 - val_loss: 0.9690 - val_accuracy: 0.6642 - lr: 5.0000e-04\n",
            "Epoch 19/30\n",
            "156/156 [==============================] - 14s 88ms/step - loss: 0.9704 - accuracy: 0.6616 - val_loss: 0.9111 - val_accuracy: 0.6899 - lr: 5.0000e-04\n",
            "Epoch 20/30\n",
            "156/156 [==============================] - 14s 90ms/step - loss: 0.9547 - accuracy: 0.6663 - val_loss: 0.9237 - val_accuracy: 0.6788 - lr: 5.0000e-04\n",
            "Epoch 21/30\n",
            "156/156 [==============================] - 14s 91ms/step - loss: 0.9160 - accuracy: 0.6816 - val_loss: 0.8797 - val_accuracy: 0.6943 - lr: 1.2500e-04\n",
            "Epoch 22/30\n",
            "156/156 [==============================] - 14s 91ms/step - loss: 0.9044 - accuracy: 0.6878 - val_loss: 0.8584 - val_accuracy: 0.7064 - lr: 1.2500e-04\n",
            "Epoch 23/30\n",
            "156/156 [==============================] - 14s 91ms/step - loss: 0.8929 - accuracy: 0.6893 - val_loss: 0.8688 - val_accuracy: 0.6996 - lr: 1.2500e-04\n",
            "Epoch 24/30\n",
            "156/156 [==============================] - 14s 90ms/step - loss: 0.8873 - accuracy: 0.6915 - val_loss: 0.8570 - val_accuracy: 0.7049 - lr: 1.2500e-04\n",
            "Epoch 25/30\n",
            "156/156 [==============================] - 14s 93ms/step - loss: 0.8775 - accuracy: 0.6983 - val_loss: 0.8607 - val_accuracy: 0.7017 - lr: 1.2500e-04\n",
            "Epoch 26/30\n",
            "156/156 [==============================] - 14s 90ms/step - loss: 0.8786 - accuracy: 0.6965 - val_loss: 0.8500 - val_accuracy: 0.7072 - lr: 1.2500e-04\n",
            "Epoch 27/30\n",
            "156/156 [==============================] - 14s 91ms/step - loss: 0.8781 - accuracy: 0.6975 - val_loss: 0.8400 - val_accuracy: 0.7138 - lr: 1.2500e-04\n",
            "Epoch 28/30\n",
            "156/156 [==============================] - 14s 90ms/step - loss: 0.8674 - accuracy: 0.6972 - val_loss: 0.8424 - val_accuracy: 0.7114 - lr: 1.2500e-04\n",
            "Epoch 29/30\n",
            "156/156 [==============================] - 14s 91ms/step - loss: 0.8687 - accuracy: 0.6990 - val_loss: 0.8536 - val_accuracy: 0.7059 - lr: 1.2500e-04\n",
            "Epoch 30/30\n",
            "156/156 [==============================] - 14s 92ms/step - loss: 0.8600 - accuracy: 0.7041 - val_loss: 0.8261 - val_accuracy: 0.7140 - lr: 1.2500e-04\n",
            "Score for fold 3: loss of 0.8261310458183289; accuracy of 71.39999866485596%\n",
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_15 (Conv2D)          (None, 32, 32, 32)        896       \n",
            "                                                                 \n",
            " max_pooling2d_5 (MaxPooling  multiple                 0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_16 (Conv2D)          (None, 16, 16, 64)        18496     \n",
            "                                                                 \n",
            " dropout_5 (Dropout)         (None, 8, 8, 64)          0         \n",
            "                                                                 \n",
            " conv2d_17 (Conv2D)          (None, 8, 8, 128)         73856     \n",
            "                                                                 \n",
            " flatten_5 (Flatten)         (None, 2048)              0         \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 10)                20490     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 113,738\n",
            "Trainable params: 113,738\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/30\n",
            "156/156 [==============================] - 15s 93ms/step - loss: 4.5134 - accuracy: 0.2486 - val_loss: 1.8153 - val_accuracy: 0.3546 - lr: 0.0010\n",
            "Epoch 2/30\n",
            "156/156 [==============================] - 14s 91ms/step - loss: 1.7438 - accuracy: 0.3705 - val_loss: 1.6254 - val_accuracy: 0.4214 - lr: 0.0010\n",
            "Epoch 3/30\n",
            "156/156 [==============================] - 15s 93ms/step - loss: 1.6273 - accuracy: 0.4121 - val_loss: 1.6121 - val_accuracy: 0.4405 - lr: 0.0010\n",
            "Epoch 4/30\n",
            "156/156 [==============================] - 14s 91ms/step - loss: 1.5384 - accuracy: 0.4442 - val_loss: 1.4820 - val_accuracy: 0.4814 - lr: 0.0010\n",
            "Epoch 5/30\n",
            "156/156 [==============================] - 14s 90ms/step - loss: 1.4839 - accuracy: 0.4687 - val_loss: 1.3323 - val_accuracy: 0.5361 - lr: 0.0010\n",
            "Epoch 6/30\n",
            "156/156 [==============================] - 14s 91ms/step - loss: 1.4253 - accuracy: 0.4927 - val_loss: 1.3267 - val_accuracy: 0.5419 - lr: 0.0010\n",
            "Epoch 7/30\n",
            "156/156 [==============================] - 14s 91ms/step - loss: 1.3800 - accuracy: 0.5076 - val_loss: 1.3364 - val_accuracy: 0.5419 - lr: 0.0010\n",
            "Epoch 8/30\n",
            "156/156 [==============================] - 14s 91ms/step - loss: 1.3345 - accuracy: 0.5269 - val_loss: 1.2497 - val_accuracy: 0.5624 - lr: 0.0010\n",
            "Epoch 9/30\n",
            "156/156 [==============================] - 14s 91ms/step - loss: 1.2819 - accuracy: 0.5422 - val_loss: 1.1724 - val_accuracy: 0.5878 - lr: 0.0010\n",
            "Epoch 10/30\n",
            "156/156 [==============================] - 15s 94ms/step - loss: 1.2494 - accuracy: 0.5595 - val_loss: 1.1596 - val_accuracy: 0.5915 - lr: 0.0010\n",
            "Epoch 11/30\n",
            "156/156 [==============================] - 15s 95ms/step - loss: 1.1805 - accuracy: 0.5861 - val_loss: 1.1141 - val_accuracy: 0.6150 - lr: 5.0000e-04\n",
            "Epoch 12/30\n",
            "156/156 [==============================] - 14s 90ms/step - loss: 1.1519 - accuracy: 0.5965 - val_loss: 1.0798 - val_accuracy: 0.6257 - lr: 5.0000e-04\n",
            "Epoch 13/30\n",
            "156/156 [==============================] - 14s 89ms/step - loss: 1.1196 - accuracy: 0.6034 - val_loss: 1.0572 - val_accuracy: 0.6245 - lr: 5.0000e-04\n",
            "Epoch 14/30\n",
            "156/156 [==============================] - 14s 92ms/step - loss: 1.1084 - accuracy: 0.6123 - val_loss: 1.0232 - val_accuracy: 0.6434 - lr: 5.0000e-04\n",
            "Epoch 15/30\n",
            "156/156 [==============================] - 14s 91ms/step - loss: 1.0843 - accuracy: 0.6200 - val_loss: 1.0808 - val_accuracy: 0.6258 - lr: 5.0000e-04\n",
            "Epoch 16/30\n",
            "156/156 [==============================] - 14s 89ms/step - loss: 1.0685 - accuracy: 0.6281 - val_loss: 0.9620 - val_accuracy: 0.6668 - lr: 5.0000e-04\n",
            "Epoch 17/30\n",
            "156/156 [==============================] - 14s 89ms/step - loss: 1.0463 - accuracy: 0.6350 - val_loss: 0.9753 - val_accuracy: 0.6602 - lr: 5.0000e-04\n",
            "Epoch 18/30\n",
            "156/156 [==============================] - 14s 90ms/step - loss: 1.0359 - accuracy: 0.6373 - val_loss: 1.0194 - val_accuracy: 0.6468 - lr: 5.0000e-04\n",
            "Epoch 19/30\n",
            "156/156 [==============================] - 14s 89ms/step - loss: 1.0081 - accuracy: 0.6485 - val_loss: 0.9455 - val_accuracy: 0.6739 - lr: 5.0000e-04\n",
            "Epoch 20/30\n",
            "156/156 [==============================] - 14s 90ms/step - loss: 1.0018 - accuracy: 0.6493 - val_loss: 0.9789 - val_accuracy: 0.6591 - lr: 5.0000e-04\n",
            "Epoch 21/30\n",
            "156/156 [==============================] - 14s 90ms/step - loss: 0.9540 - accuracy: 0.6679 - val_loss: 0.9145 - val_accuracy: 0.6822 - lr: 1.2500e-04\n",
            "Epoch 22/30\n",
            "156/156 [==============================] - 14s 90ms/step - loss: 0.9375 - accuracy: 0.6747 - val_loss: 0.9007 - val_accuracy: 0.6893 - lr: 1.2500e-04\n",
            "Epoch 23/30\n",
            "156/156 [==============================] - 14s 90ms/step - loss: 0.9270 - accuracy: 0.6754 - val_loss: 0.8705 - val_accuracy: 0.7035 - lr: 1.2500e-04\n",
            "Epoch 24/30\n",
            "156/156 [==============================] - 15s 97ms/step - loss: 0.9314 - accuracy: 0.6774 - val_loss: 0.9111 - val_accuracy: 0.6859 - lr: 1.2500e-04\n",
            "Epoch 25/30\n",
            "156/156 [==============================] - 14s 88ms/step - loss: 0.9183 - accuracy: 0.6815 - val_loss: 0.8902 - val_accuracy: 0.6934 - lr: 1.2500e-04\n",
            "Epoch 26/30\n",
            "156/156 [==============================] - 14s 88ms/step - loss: 0.9117 - accuracy: 0.6848 - val_loss: 0.8672 - val_accuracy: 0.7016 - lr: 1.2500e-04\n",
            "Epoch 27/30\n",
            "156/156 [==============================] - 14s 90ms/step - loss: 0.9117 - accuracy: 0.6828 - val_loss: 0.8917 - val_accuracy: 0.6918 - lr: 1.2500e-04\n",
            "Epoch 28/30\n",
            "156/156 [==============================] - 14s 90ms/step - loss: 0.9043 - accuracy: 0.6844 - val_loss: 0.8610 - val_accuracy: 0.7062 - lr: 1.2500e-04\n",
            "Epoch 29/30\n",
            "156/156 [==============================] - 14s 89ms/step - loss: 0.9054 - accuracy: 0.6842 - val_loss: 0.8695 - val_accuracy: 0.7023 - lr: 1.2500e-04\n",
            "Epoch 30/30\n",
            "156/156 [==============================] - 14s 88ms/step - loss: 0.8960 - accuracy: 0.6903 - val_loss: 0.8802 - val_accuracy: 0.6950 - lr: 1.2500e-04\n",
            "Score for fold 4: loss of 0.8801662921905518; accuracy of 69.49999928474426%\n",
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_18 (Conv2D)          (None, 32, 32, 32)        896       \n",
            "                                                                 \n",
            " max_pooling2d_6 (MaxPooling  multiple                 0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_19 (Conv2D)          (None, 16, 16, 64)        18496     \n",
            "                                                                 \n",
            " dropout_6 (Dropout)         (None, 8, 8, 64)          0         \n",
            "                                                                 \n",
            " conv2d_20 (Conv2D)          (None, 8, 8, 128)         73856     \n",
            "                                                                 \n",
            " flatten_6 (Flatten)         (None, 2048)              0         \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 10)                20490     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 113,738\n",
            "Trainable params: 113,738\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/30\n",
            "156/156 [==============================] - 15s 92ms/step - loss: 4.5270 - accuracy: 0.2782 - val_loss: 1.6229 - val_accuracy: 0.4140 - lr: 0.0010\n",
            "Epoch 2/30\n",
            "156/156 [==============================] - 14s 90ms/step - loss: 1.6208 - accuracy: 0.4140 - val_loss: 1.5226 - val_accuracy: 0.4595 - lr: 0.0010\n",
            "Epoch 3/30\n",
            "156/156 [==============================] - 14s 90ms/step - loss: 1.5049 - accuracy: 0.4573 - val_loss: 1.4363 - val_accuracy: 0.4976 - lr: 0.0010\n",
            "Epoch 4/30\n",
            "156/156 [==============================] - 14s 91ms/step - loss: 1.4382 - accuracy: 0.4861 - val_loss: 1.3292 - val_accuracy: 0.5352 - lr: 0.0010\n",
            "Epoch 5/30\n",
            "156/156 [==============================] - 14s 91ms/step - loss: 1.3696 - accuracy: 0.5106 - val_loss: 1.3267 - val_accuracy: 0.5381 - lr: 0.0010\n",
            "Epoch 6/30\n",
            "156/156 [==============================] - 14s 91ms/step - loss: 1.3208 - accuracy: 0.5312 - val_loss: 1.2214 - val_accuracy: 0.5747 - lr: 0.0010\n",
            "Epoch 7/30\n",
            "156/156 [==============================] - 14s 91ms/step - loss: 1.2493 - accuracy: 0.5582 - val_loss: 1.1615 - val_accuracy: 0.5987 - lr: 0.0010\n",
            "Epoch 8/30\n",
            "156/156 [==============================] - 14s 92ms/step - loss: 1.2177 - accuracy: 0.5694 - val_loss: 1.1437 - val_accuracy: 0.5937 - lr: 0.0010\n",
            "Epoch 9/30\n",
            "156/156 [==============================] - 15s 97ms/step - loss: 1.1941 - accuracy: 0.5786 - val_loss: 1.0651 - val_accuracy: 0.6292 - lr: 0.0010\n",
            "Epoch 10/30\n",
            "156/156 [==============================] - 16s 100ms/step - loss: 1.1583 - accuracy: 0.5941 - val_loss: 1.0312 - val_accuracy: 0.6409 - lr: 0.0010\n",
            "Epoch 11/30\n",
            "156/156 [==============================] - 14s 88ms/step - loss: 1.0743 - accuracy: 0.6215 - val_loss: 1.0104 - val_accuracy: 0.6469 - lr: 5.0000e-04\n",
            "Epoch 12/30\n",
            "156/156 [==============================] - 14s 92ms/step - loss: 1.0596 - accuracy: 0.6333 - val_loss: 0.9762 - val_accuracy: 0.6607 - lr: 5.0000e-04\n",
            "Epoch 13/30\n",
            "156/156 [==============================] - 15s 98ms/step - loss: 1.0326 - accuracy: 0.6380 - val_loss: 0.9934 - val_accuracy: 0.6563 - lr: 5.0000e-04\n",
            "Epoch 14/30\n",
            "156/156 [==============================] - 14s 92ms/step - loss: 1.0288 - accuracy: 0.6421 - val_loss: 0.9749 - val_accuracy: 0.6674 - lr: 5.0000e-04\n",
            "Epoch 15/30\n",
            "156/156 [==============================] - 14s 91ms/step - loss: 1.0089 - accuracy: 0.6493 - val_loss: 0.9803 - val_accuracy: 0.6620 - lr: 5.0000e-04\n",
            "Epoch 16/30\n",
            "156/156 [==============================] - 14s 90ms/step - loss: 0.9965 - accuracy: 0.6530 - val_loss: 0.8822 - val_accuracy: 0.6988 - lr: 5.0000e-04\n",
            "Epoch 17/30\n",
            "156/156 [==============================] - 14s 89ms/step - loss: 0.9902 - accuracy: 0.6536 - val_loss: 0.8939 - val_accuracy: 0.6926 - lr: 5.0000e-04\n",
            "Epoch 18/30\n",
            "156/156 [==============================] - 14s 90ms/step - loss: 0.9610 - accuracy: 0.6664 - val_loss: 0.8850 - val_accuracy: 0.6954 - lr: 5.0000e-04\n",
            "Epoch 19/30\n",
            "156/156 [==============================] - 14s 90ms/step - loss: 0.9522 - accuracy: 0.6656 - val_loss: 0.8744 - val_accuracy: 0.7007 - lr: 5.0000e-04\n",
            "Epoch 20/30\n",
            "156/156 [==============================] - 14s 91ms/step - loss: 0.9544 - accuracy: 0.6675 - val_loss: 0.8757 - val_accuracy: 0.7002 - lr: 5.0000e-04\n",
            "Epoch 21/30\n",
            "156/156 [==============================] - 14s 92ms/step - loss: 0.9032 - accuracy: 0.6847 - val_loss: 0.8335 - val_accuracy: 0.7174 - lr: 1.2500e-04\n",
            "Epoch 22/30\n",
            "156/156 [==============================] - 14s 90ms/step - loss: 0.8939 - accuracy: 0.6904 - val_loss: 0.8164 - val_accuracy: 0.7215 - lr: 1.2500e-04\n",
            "Epoch 23/30\n",
            "156/156 [==============================] - 14s 89ms/step - loss: 0.8954 - accuracy: 0.6908 - val_loss: 0.8323 - val_accuracy: 0.7162 - lr: 1.2500e-04\n",
            "Epoch 24/30\n",
            "156/156 [==============================] - 14s 89ms/step - loss: 0.8817 - accuracy: 0.6935 - val_loss: 0.8236 - val_accuracy: 0.7206 - lr: 1.2500e-04\n",
            "Epoch 25/30\n",
            "156/156 [==============================] - 14s 91ms/step - loss: 0.8738 - accuracy: 0.6953 - val_loss: 0.8111 - val_accuracy: 0.7222 - lr: 1.2500e-04\n",
            "Epoch 26/30\n",
            "156/156 [==============================] - 14s 91ms/step - loss: 0.8711 - accuracy: 0.6978 - val_loss: 0.7937 - val_accuracy: 0.7260 - lr: 1.2500e-04\n",
            "Epoch 27/30\n",
            "156/156 [==============================] - 14s 91ms/step - loss: 0.8657 - accuracy: 0.7024 - val_loss: 0.8129 - val_accuracy: 0.7192 - lr: 1.2500e-04\n",
            "Epoch 28/30\n",
            "156/156 [==============================] - 14s 90ms/step - loss: 0.8669 - accuracy: 0.6995 - val_loss: 0.8019 - val_accuracy: 0.7234 - lr: 1.2500e-04\n",
            "Epoch 29/30\n",
            "156/156 [==============================] - 14s 89ms/step - loss: 0.8624 - accuracy: 0.7026 - val_loss: 0.7892 - val_accuracy: 0.7281 - lr: 1.2500e-04\n",
            "Epoch 30/30\n",
            "156/156 [==============================] - 14s 91ms/step - loss: 0.8492 - accuracy: 0.7065 - val_loss: 0.7960 - val_accuracy: 0.7287 - lr: 1.2500e-04\n",
            "Score for fold 5: loss of 0.7959635853767395; accuracy of 72.86999821662903%\n",
            "------------------------------------------------------------------------\n",
            "Score per fold\n",
            "------------------------------------------------------------------------\n",
            "> Fold 1 - Loss: 0.8069173097610474 - Accuracy: 72.71999716758728%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 2 - Loss: 0.8107749819755554 - Accuracy: 72.39999771118164%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 3 - Loss: 0.8261310458183289 - Accuracy: 71.39999866485596%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 4 - Loss: 0.8801662921905518 - Accuracy: 69.49999928474426%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 5 - Loss: 0.7959635853767395 - Accuracy: 72.86999821662903%\n",
            "------------------------------------------------------------------------\n",
            "Average scores for all folds:\n",
            "> Accuracy: 71.77799820899963 (+- 1.248909299319925)\n",
            "> Loss: 0.8239906430244446\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Weight initialisation, decay and activation functions"
      ],
      "metadata": {
        "id": "w7WgIlLOZF_9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Relu activation"
      ],
      "metadata": {
        "id": "C5VaZ2PDo1aN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "acc_per_fold = []\n",
        "loss_per_fold = []\n",
        "\n",
        "with tf.device('/device:GPU:0'):\n",
        "  fold_no = 1\n",
        "  for train, valid in kfold.split(trainX, trainy):\n",
        "\n",
        "    model = CNN_Model(batch_size=32, depth=3, hidden_neurons_1=64, hidden_neurons_2=128, hidden_neurons_3=256, kernel_size_1=3, kernel_size_2=3, kernel_size_3=3, \n",
        "               dropout_rate=0.2, batch_norm=False, pooling='max', layer_activation_1='relu', layer_activation_2='relu', layer_activation_3='relu',\n",
        "               kernel_initialiser='glorot_uniform', bias_initialiser='zeros', optimiser='Adam', weight_decay=None, learning_rate=1e-3, output_activation='softmax', SGD_momentum=0) \n",
        "  \n",
        "    train_gen = tf.keras.preprocessing.image.ImageDataGenerator(width_shift_range=0.1,\n",
        "      height_shift_range=0.1,\n",
        "      horizontal_flip=True)\n",
        "    train_gen.fit(trainX[train])\n",
        "  \n",
        "    print('------------------------------------------------------------------------')\n",
        "    print(f'Training for fold {fold_no} ...')\n",
        "\n",
        "    history = model.train(\n",
        "              train_gen,\n",
        "                trainX[train],\n",
        "                  trainy[train],\n",
        "                trainX[valid],\n",
        "                  trainy[valid],\n",
        "                  lr_rate_type='step')\n",
        "\n",
        "    scores = model.evaluate(trainX[valid], trainy[valid])\n",
        "\n",
        "    print(f'Score for fold {fold_no}: loss of {scores[0]}; accuracy of {scores[1]*100}%')\n",
        "    acc_per_fold.append(scores[1] * 100)\n",
        "    loss_per_fold.append(scores[0])\n",
        "\n",
        "    fold_no+=1\n",
        "\n",
        "# == Provide average scores ==\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Score per fold')\n",
        "for i in range(0, len(acc_per_fold)):\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Average scores for all folds:')\n",
        "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
        "print(f'> Loss: {np.mean(loss_per_fold)}')"
      ],
      "metadata": {
        "id": "Jgmj6JKao99E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Tanh activation"
      ],
      "metadata": {
        "id": "VPhowvFnpDLF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "acc_per_fold = []\n",
        "loss_per_fold = []\n",
        "\n",
        "with tf.device('/device:GPU:0'):\n",
        "  fold_no = 1\n",
        "  for train, valid in kfold.split(trainX, trainy):\n",
        "\n",
        "    model = CNN_Model(batch_size=32, depth=3, hidden_neurons_1=64, hidden_neurons_2=128, hidden_neurons_3=256, kernel_size_1=3, kernel_size_2=3, kernel_size_3=3, \n",
        "               dropout_rate=0.2, batch_norm=False, pooling='max', layer_activation_1='tanh', layer_activation_2='tanh', layer_activation_3='tanh',\n",
        "               kernel_initialiser='glorot_uniform', bias_initialiser='zeros', optimiser='Adam', weight_decay=None, learning_rate=1e-3, output_activation='softmax', SGD_momentum=0) \n",
        "  \n",
        "    train_gen = tf.keras.preprocessing.image.ImageDataGenerator(width_shift_range=0.1,\n",
        "      height_shift_range=0.1,\n",
        "      horizontal_flip=True)\n",
        "    train_gen.fit(trainX[train])\n",
        "  \n",
        "    print('------------------------------------------------------------------------')\n",
        "    print(f'Training for fold {fold_no} ...')\n",
        "\n",
        "    history = model.train(\n",
        "              train_gen,\n",
        "                trainX[train],\n",
        "                  trainy[train],\n",
        "                trainX[valid],\n",
        "                  trainy[valid],\n",
        "                  lr_rate_type='step')\n",
        "\n",
        "    scores = model.evaluate(trainX[valid], trainy[valid])\n",
        "\n",
        "    print(f'Score for fold {fold_no}: loss of {scores[0]}; accuracy of {scores[1]*100}%')\n",
        "    acc_per_fold.append(scores[1] * 100)\n",
        "    loss_per_fold.append(scores[0])\n",
        "\n",
        "    fold_no+=1\n",
        "\n",
        "# == Provide average scores ==\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Score per fold')\n",
        "for i in range(0, len(acc_per_fold)):\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Average scores for all folds:')\n",
        "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
        "print(f'> Loss: {np.mean(loss_per_fold)}')"
      ],
      "metadata": {
        "id": "KwD5F56zZSum",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "791196c8-6830-4226-f2c6-e47edd4e8dde"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_35\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_105 (Conv2D)         (None, 32, 32, 64)        1792      \n",
            "                                                                 \n",
            " max_pooling2d_35 (MaxPoolin  multiple                 0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_106 (Conv2D)         (None, 16, 16, 128)       73856     \n",
            "                                                                 \n",
            " dropout_35 (Dropout)        (None, 8, 8, 128)         0         \n",
            "                                                                 \n",
            " conv2d_107 (Conv2D)         (None, 8, 8, 256)         295168    \n",
            "                                                                 \n",
            " flatten_35 (Flatten)        (None, 4096)              0         \n",
            "                                                                 \n",
            " dense_35 (Dense)            (None, 10)                40970     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 411,786\n",
            "Trainable params: 411,786\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 1.4671 - accuracy: 0.4805 - val_loss: 1.1900 - val_accuracy: 0.5829 - lr: 0.0010\n",
            "Epoch 2/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.1991 - accuracy: 0.5805 - val_loss: 1.0324 - val_accuracy: 0.6481 - lr: 0.0010\n",
            "Epoch 3/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.1055 - accuracy: 0.6204 - val_loss: 1.0420 - val_accuracy: 0.6505 - lr: 0.0010\n",
            "Epoch 4/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.0508 - accuracy: 0.6371 - val_loss: 0.9771 - val_accuracy: 0.6708 - lr: 0.0010\n",
            "Epoch 5/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.0093 - accuracy: 0.6550 - val_loss: 1.0319 - val_accuracy: 0.6643 - lr: 0.0010\n",
            "Epoch 6/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.9790 - accuracy: 0.6638 - val_loss: 0.9566 - val_accuracy: 0.6866 - lr: 0.0010\n",
            "Epoch 7/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.9480 - accuracy: 0.6779 - val_loss: 0.9043 - val_accuracy: 0.7012 - lr: 0.0010\n",
            "Epoch 8/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.9299 - accuracy: 0.6854 - val_loss: 0.9722 - val_accuracy: 0.6815 - lr: 0.0010\n",
            "Epoch 9/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.9196 - accuracy: 0.6874 - val_loss: 0.9071 - val_accuracy: 0.7010 - lr: 0.0010\n",
            "Epoch 10/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.9025 - accuracy: 0.6966 - val_loss: 0.8347 - val_accuracy: 0.7238 - lr: 0.0010\n",
            "Epoch 11/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.8995 - accuracy: 0.6957 - val_loss: 0.8495 - val_accuracy: 0.7243 - lr: 0.0010\n",
            "Epoch 12/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.8800 - accuracy: 0.7007 - val_loss: 0.8577 - val_accuracy: 0.7276 - lr: 0.0010\n",
            "Epoch 13/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.8776 - accuracy: 0.7078 - val_loss: 0.8940 - val_accuracy: 0.7123 - lr: 0.0010\n",
            "Epoch 14/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.8694 - accuracy: 0.7071 - val_loss: 0.8705 - val_accuracy: 0.7236 - lr: 0.0010\n",
            "Epoch 15/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.8570 - accuracy: 0.7125 - val_loss: 0.8841 - val_accuracy: 0.7155 - lr: 0.0010\n",
            "Epoch 16/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.8253 - accuracy: 0.7194 - val_loss: 0.8254 - val_accuracy: 0.7314 - lr: 9.0484e-04\n",
            "Epoch 17/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.7938 - accuracy: 0.7323 - val_loss: 0.7639 - val_accuracy: 0.7513 - lr: 8.1873e-04\n",
            "Epoch 18/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.7583 - accuracy: 0.7439 - val_loss: 0.8018 - val_accuracy: 0.7444 - lr: 7.4082e-04\n",
            "Epoch 19/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.7370 - accuracy: 0.7483 - val_loss: 0.8275 - val_accuracy: 0.7369 - lr: 6.7032e-04\n",
            "Epoch 20/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.7158 - accuracy: 0.7584 - val_loss: 0.6902 - val_accuracy: 0.7736 - lr: 6.0653e-04\n",
            "Epoch 21/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.6849 - accuracy: 0.7660 - val_loss: 0.6839 - val_accuracy: 0.7758 - lr: 5.4881e-04\n",
            "Epoch 22/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.6649 - accuracy: 0.7696 - val_loss: 0.6984 - val_accuracy: 0.7714 - lr: 4.9659e-04\n",
            "Epoch 23/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.6467 - accuracy: 0.7776 - val_loss: 0.6542 - val_accuracy: 0.7842 - lr: 4.4933e-04\n",
            "Epoch 24/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.6309 - accuracy: 0.7844 - val_loss: 0.6945 - val_accuracy: 0.7706 - lr: 4.0657e-04\n",
            "Epoch 25/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.6133 - accuracy: 0.7907 - val_loss: 0.6587 - val_accuracy: 0.7892 - lr: 3.6788e-04\n",
            "Epoch 26/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.5957 - accuracy: 0.7941 - val_loss: 0.6790 - val_accuracy: 0.7808 - lr: 3.3287e-04\n",
            "Epoch 27/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.5800 - accuracy: 0.7998 - val_loss: 0.6699 - val_accuracy: 0.7870 - lr: 3.0119e-04\n",
            "Epoch 28/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.5698 - accuracy: 0.8034 - val_loss: 0.6517 - val_accuracy: 0.7920 - lr: 2.7253e-04\n",
            "Epoch 29/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.5663 - accuracy: 0.8034 - val_loss: 0.6338 - val_accuracy: 0.7940 - lr: 2.4660e-04\n",
            "Epoch 30/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.5437 - accuracy: 0.8132 - val_loss: 0.6363 - val_accuracy: 0.7973 - lr: 2.2313e-04\n",
            "Score for fold 1: loss of 0.6362960338592529; accuracy of 79.72999811172485%\n",
            "Model: \"sequential_36\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_108 (Conv2D)         (None, 32, 32, 64)        1792      \n",
            "                                                                 \n",
            " max_pooling2d_36 (MaxPoolin  multiple                 0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_109 (Conv2D)         (None, 16, 16, 128)       73856     \n",
            "                                                                 \n",
            " dropout_36 (Dropout)        (None, 8, 8, 128)         0         \n",
            "                                                                 \n",
            " conv2d_110 (Conv2D)         (None, 8, 8, 256)         295168    \n",
            "                                                                 \n",
            " flatten_36 (Flatten)        (None, 4096)              0         \n",
            "                                                                 \n",
            " dense_36 (Dense)            (None, 10)                40970     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 411,786\n",
            "Trainable params: 411,786\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.4667 - accuracy: 0.4823 - val_loss: 1.1493 - val_accuracy: 0.6013 - lr: 0.0010\n",
            "Epoch 2/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.1910 - accuracy: 0.5845 - val_loss: 1.0935 - val_accuracy: 0.6333 - lr: 0.0010\n",
            "Epoch 3/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.0986 - accuracy: 0.6166 - val_loss: 1.0215 - val_accuracy: 0.6594 - lr: 0.0010\n",
            "Epoch 4/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.0435 - accuracy: 0.6393 - val_loss: 0.9317 - val_accuracy: 0.6859 - lr: 0.0010\n",
            "Epoch 5/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.9984 - accuracy: 0.6566 - val_loss: 0.9019 - val_accuracy: 0.6956 - lr: 0.0010\n",
            "Epoch 6/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.9739 - accuracy: 0.6662 - val_loss: 0.9940 - val_accuracy: 0.6727 - lr: 0.0010\n",
            "Epoch 7/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.9483 - accuracy: 0.6754 - val_loss: 0.8860 - val_accuracy: 0.7082 - lr: 0.0010\n",
            "Epoch 8/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.9309 - accuracy: 0.6830 - val_loss: 0.8741 - val_accuracy: 0.7145 - lr: 0.0010\n",
            "Epoch 9/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.9096 - accuracy: 0.6892 - val_loss: 0.8546 - val_accuracy: 0.7196 - lr: 0.0010\n",
            "Epoch 10/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.9020 - accuracy: 0.6948 - val_loss: 0.8472 - val_accuracy: 0.7239 - lr: 0.0010\n",
            "Epoch 11/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.8810 - accuracy: 0.7015 - val_loss: 0.9165 - val_accuracy: 0.7094 - lr: 0.0010\n",
            "Epoch 12/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.8685 - accuracy: 0.7072 - val_loss: 0.8555 - val_accuracy: 0.7211 - lr: 0.0010\n",
            "Epoch 13/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.8746 - accuracy: 0.7069 - val_loss: 0.8243 - val_accuracy: 0.7360 - lr: 0.0010\n",
            "Epoch 14/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.8727 - accuracy: 0.7075 - val_loss: 0.8456 - val_accuracy: 0.7362 - lr: 0.0010\n",
            "Epoch 15/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.8475 - accuracy: 0.7150 - val_loss: 0.8598 - val_accuracy: 0.7247 - lr: 0.0010\n",
            "Epoch 16/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.8176 - accuracy: 0.7231 - val_loss: 0.7879 - val_accuracy: 0.7440 - lr: 9.0484e-04\n",
            "Epoch 17/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.7891 - accuracy: 0.7330 - val_loss: 0.7530 - val_accuracy: 0.7583 - lr: 8.1873e-04\n",
            "Epoch 18/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.7632 - accuracy: 0.7408 - val_loss: 0.7667 - val_accuracy: 0.7578 - lr: 7.4082e-04\n",
            "Epoch 19/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.7345 - accuracy: 0.7487 - val_loss: 0.7132 - val_accuracy: 0.7732 - lr: 6.7032e-04\n",
            "Epoch 20/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.7076 - accuracy: 0.7585 - val_loss: 0.7319 - val_accuracy: 0.7702 - lr: 6.0653e-04\n",
            "Epoch 21/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 0.6840 - accuracy: 0.7664 - val_loss: 0.6762 - val_accuracy: 0.7845 - lr: 5.4881e-04\n",
            "Epoch 22/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.6621 - accuracy: 0.7728 - val_loss: 0.6599 - val_accuracy: 0.7872 - lr: 4.9659e-04\n",
            "Epoch 23/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.6482 - accuracy: 0.7787 - val_loss: 0.6599 - val_accuracy: 0.7860 - lr: 4.4933e-04\n",
            "Epoch 24/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.6271 - accuracy: 0.7818 - val_loss: 0.6745 - val_accuracy: 0.7866 - lr: 4.0657e-04\n",
            "Epoch 25/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.6106 - accuracy: 0.7923 - val_loss: 0.6889 - val_accuracy: 0.7847 - lr: 3.6788e-04\n",
            "Epoch 26/30\n",
            "1213/1250 [============================>.] - ETA: 0s - loss: 0.5961 - accuracy: 0.7921"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_22076/3650374549.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'Training for fold {fold_no} ...'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m     history = model.train(\n\u001b[0m\u001b[0;32m     21\u001b[0m               \u001b[0mtrain_gen\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m                 \u001b[0mtrainX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_22076/4196491080.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, train_gen, train_x, train_y, test_x, test_y, lr_rate, lr_rate_type)\u001b[0m\n\u001b[0;32m    160\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_gen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtensorboard_callback\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcallback\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    161\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 162\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_gen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtensorboard_callback\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcallback\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m#  self.model.fit(train_x, train_y, validation_data=(test_x, test_y) , epochs=25, verbose=1)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    163\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_gen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtensorboard_callback\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1382\u001b[0m                 _r=1):\n\u001b[0;32m   1383\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1384\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1385\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1386\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    913\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    914\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 915\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    945\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    946\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 947\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    948\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2954\u001b[0m       (graph_function,\n\u001b[0;32m   2955\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2956\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   2957\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   2958\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1851\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1852\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1853\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1854\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1855\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    498\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 499\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    500\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    501\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### No layer activation"
      ],
      "metadata": {
        "id": "U3N9a3Chpjb3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "acc_per_fold = []\n",
        "loss_per_fold = []\n",
        "\n",
        "with tf.device('/device:GPU:0'):\n",
        "  fold_no = 1\n",
        "  for train, valid in kfold.split(trainX, trainy):\n",
        "\n",
        "    model = CNN_Model(batch_size=32, depth=3, hidden_neurons_1=64, hidden_neurons_2=128, hidden_neurons_3=256, kernel_size_1=3, kernel_size_2=3, kernel_size_3=3, \n",
        "               dropout_rate=0.2, batch_norm=False, pooling='max', layer_activation_1=None, layer_activation_2=None, layer_activation_3=None,\n",
        "               kernel_initialiser='glorot_uniform', bias_initialiser='zeros', optimiser='Adam', weight_decay=None, learning_rate=1e-3, output_activation='softmax', SGD_momentum=0) \n",
        "  \n",
        "    train_gen = tf.keras.preprocessing.image.ImageDataGenerator(width_shift_range=0.1,\n",
        "      height_shift_range=0.1,\n",
        "      horizontal_flip=True)\n",
        "    train_gen.fit(trainX[train])\n",
        "  \n",
        "    print('------------------------------------------------------------------------')\n",
        "    print(f'Training for fold {fold_no} ...')\n",
        "\n",
        "    history = model.train(\n",
        "              train_gen,\n",
        "                trainX[train],\n",
        "                  trainy[train],\n",
        "                trainX[valid],\n",
        "                  trainy[valid],\n",
        "                  lr_rate_type='step')\n",
        "\n",
        "    scores = model.evaluate(trainX[valid], trainy[valid])\n",
        "\n",
        "    print(f'Score for fold {fold_no}: loss of {scores[0]}; accuracy of {scores[1]*100}%')\n",
        "    acc_per_fold.append(scores[1] * 100)\n",
        "    loss_per_fold.append(scores[0])\n",
        "\n",
        "    fold_no+=1\n",
        "\n",
        "# == Provide average scores ==\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Score per fold')\n",
        "for i in range(0, len(acc_per_fold)):\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Average scores for all folds:')\n",
        "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
        "print(f'> Loss: {np.mean(loss_per_fold)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "tedLqXizWFVU",
        "outputId": "6e3372e3-528d-4cee-c24e-bd006f82c573"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_37\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_111 (Conv2D)         (None, 32, 32, 64)        1792      \n",
            "                                                                 \n",
            " max_pooling2d_37 (MaxPoolin  multiple                 0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_112 (Conv2D)         (None, 16, 16, 128)       73856     \n",
            "                                                                 \n",
            " dropout_37 (Dropout)        (None, 8, 8, 128)         0         \n",
            "                                                                 \n",
            " conv2d_113 (Conv2D)         (None, 8, 8, 256)         295168    \n",
            "                                                                 \n",
            " flatten_37 (Flatten)        (None, 4096)              0         \n",
            "                                                                 \n",
            " dense_37 (Dense)            (None, 10)                40970     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 411,786\n",
            "Trainable params: 411,786\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/30\n",
            "1250/1250 [==============================] - 19s 14ms/step - loss: 1.4933 - accuracy: 0.4737 - val_loss: 1.2456 - val_accuracy: 0.5808 - lr: 0.0010\n",
            "Epoch 2/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.2070 - accuracy: 0.5817 - val_loss: 1.0903 - val_accuracy: 0.6391 - lr: 0.0010\n",
            "Epoch 3/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.1170 - accuracy: 0.6157 - val_loss: 0.9775 - val_accuracy: 0.6737 - lr: 0.0010\n",
            "Epoch 4/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.0575 - accuracy: 0.6388 - val_loss: 1.0169 - val_accuracy: 0.6742 - lr: 0.0010\n",
            "Epoch 5/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.0246 - accuracy: 0.6493 - val_loss: 0.9922 - val_accuracy: 0.6707 - lr: 0.0010\n",
            "Epoch 6/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.9996 - accuracy: 0.6584 - val_loss: 0.9002 - val_accuracy: 0.7079 - lr: 0.0010\n",
            "Epoch 7/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.9778 - accuracy: 0.6690 - val_loss: 0.8914 - val_accuracy: 0.7098 - lr: 0.0010\n",
            "Epoch 8/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.9629 - accuracy: 0.6726 - val_loss: 0.8712 - val_accuracy: 0.7188 - lr: 0.0010\n",
            "Epoch 9/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.9484 - accuracy: 0.6803 - val_loss: 0.9133 - val_accuracy: 0.7110 - lr: 0.0010\n",
            "Epoch 10/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.9285 - accuracy: 0.6836 - val_loss: 0.9063 - val_accuracy: 0.7134 - lr: 0.0010\n",
            "Epoch 11/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.9219 - accuracy: 0.6861 - val_loss: 0.8793 - val_accuracy: 0.7111 - lr: 0.0010\n",
            "Epoch 12/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.9139 - accuracy: 0.6916 - val_loss: 0.9511 - val_accuracy: 0.7010 - lr: 0.0010\n",
            "Epoch 13/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.9013 - accuracy: 0.6949 - val_loss: 0.8517 - val_accuracy: 0.7289 - lr: 0.0010\n",
            "Epoch 14/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.8941 - accuracy: 0.6997 - val_loss: 0.9560 - val_accuracy: 0.6988 - lr: 0.0010\n",
            "Epoch 15/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.8959 - accuracy: 0.6982 - val_loss: 0.8795 - val_accuracy: 0.7245 - lr: 0.0010\n",
            "Epoch 16/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.8609 - accuracy: 0.7088 - val_loss: 0.8516 - val_accuracy: 0.7311 - lr: 9.0484e-04\n",
            "Epoch 17/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.8320 - accuracy: 0.7215 - val_loss: 0.8765 - val_accuracy: 0.7324 - lr: 8.1873e-04\n",
            "Epoch 18/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.8066 - accuracy: 0.7286 - val_loss: 0.8319 - val_accuracy: 0.7372 - lr: 7.4082e-04\n",
            "Epoch 19/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 0.7821 - accuracy: 0.7359 - val_loss: 0.7786 - val_accuracy: 0.7527 - lr: 6.7032e-04\n",
            "Epoch 20/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.7561 - accuracy: 0.7429 - val_loss: 0.7423 - val_accuracy: 0.7614 - lr: 6.0653e-04\n",
            "Epoch 21/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.7354 - accuracy: 0.7495 - val_loss: 0.7402 - val_accuracy: 0.7692 - lr: 5.4881e-04\n",
            "Epoch 22/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.7239 - accuracy: 0.7559 - val_loss: 0.7757 - val_accuracy: 0.7593 - lr: 4.9659e-04\n",
            "Epoch 23/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.6964 - accuracy: 0.7627 - val_loss: 0.7308 - val_accuracy: 0.7730 - lr: 4.4933e-04\n",
            "Epoch 24/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.6802 - accuracy: 0.7700 - val_loss: 0.6929 - val_accuracy: 0.7794 - lr: 4.0657e-04\n",
            "Epoch 25/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.6647 - accuracy: 0.7749 - val_loss: 0.6920 - val_accuracy: 0.7814 - lr: 3.6788e-04\n",
            "Epoch 26/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.6485 - accuracy: 0.7800 - val_loss: 0.6809 - val_accuracy: 0.7857 - lr: 3.3287e-04\n",
            "Epoch 27/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.6389 - accuracy: 0.7830 - val_loss: 0.6592 - val_accuracy: 0.7888 - lr: 3.0119e-04\n",
            "Epoch 28/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.6263 - accuracy: 0.7859 - val_loss: 0.6734 - val_accuracy: 0.7869 - lr: 2.7253e-04\n",
            "Epoch 29/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.6131 - accuracy: 0.7910 - val_loss: 0.6561 - val_accuracy: 0.7885 - lr: 2.4660e-04\n",
            "Epoch 30/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.5956 - accuracy: 0.7972 - val_loss: 0.6586 - val_accuracy: 0.7911 - lr: 2.2313e-04\n",
            "Score for fold 1: loss of 0.658635139465332; accuracy of 79.1100025177002%\n",
            "Model: \"sequential_38\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_114 (Conv2D)         (None, 32, 32, 64)        1792      \n",
            "                                                                 \n",
            " max_pooling2d_38 (MaxPoolin  multiple                 0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_115 (Conv2D)         (None, 16, 16, 128)       73856     \n",
            "                                                                 \n",
            " dropout_38 (Dropout)        (None, 8, 8, 128)         0         \n",
            "                                                                 \n",
            " conv2d_116 (Conv2D)         (None, 8, 8, 256)         295168    \n",
            "                                                                 \n",
            " flatten_38 (Flatten)        (None, 4096)              0         \n",
            "                                                                 \n",
            " dense_38 (Dense)            (None, 10)                40970     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 411,786\n",
            "Trainable params: 411,786\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/30\n",
            " 616/1250 [=============>................] - ETA: 8s - loss: 1.6409 - accuracy: 0.4148"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_22076/274287208.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'Training for fold {fold_no} ...'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m     history = model.train(\n\u001b[0m\u001b[0;32m     21\u001b[0m               \u001b[0mtrain_gen\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m                 \u001b[0mtrainX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_22076/4196491080.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, train_gen, train_x, train_y, test_x, test_y, lr_rate, lr_rate_type)\u001b[0m\n\u001b[0;32m    160\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_gen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtensorboard_callback\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcallback\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    161\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 162\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_gen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtensorboard_callback\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcallback\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m#  self.model.fit(train_x, train_y, validation_data=(test_x, test_y) , epochs=25, verbose=1)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    163\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_gen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtensorboard_callback\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1387\u001b[0m               \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtmp_logs\u001b[0m  \u001b[1;31m# No error, now safe to assign to logs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1388\u001b[0m               \u001b[0mend_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep_increment\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1389\u001b[1;33m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mend_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1390\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1391\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m    436\u001b[0m     \"\"\"\n\u001b[0;32m    437\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_should_call_train_batch_hooks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 438\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'end'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    439\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    440\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mon_test_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[1;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[0;32m    295\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_batch_begin_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    296\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'end'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 297\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_batch_end_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    298\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    299\u001b[0m       raise ValueError(\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_call_batch_end_hook\u001b[1;34m(self, mode, batch, logs)\u001b[0m\n\u001b[0;32m    316\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_time\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 318\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_batch_hook_helper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    319\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    320\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_batches_for_timing_check\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook_helper\u001b[1;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[0;32m    354\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    355\u001b[0m       \u001b[0mhook\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 356\u001b[1;33m       \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    357\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    358\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_timing\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m   1032\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1033\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mon_train_batch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1034\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_batch_update_progbar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1035\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1036\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mon_test_batch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_batch_update_progbar\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m   1104\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1105\u001b[0m       \u001b[1;31m# Only block async when verbose = 1.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1106\u001b[1;33m       \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msync_to_numpy_or_python_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1107\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinalize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\tf_utils.py\u001b[0m in \u001b[0;36msync_to_numpy_or_python_type\u001b[1;34m(tensors)\u001b[0m\n\u001b[0;32m    561\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    562\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 563\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    564\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    565\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[1;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[0;32m    912\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    913\u001b[0m   return pack_sequence_as(\n\u001b[1;32m--> 914\u001b[1;33m       \u001b[0mstructure\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    915\u001b[0m       expand_composites=expand_composites)\n\u001b[0;32m    916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    912\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    913\u001b[0m   return pack_sequence_as(\n\u001b[1;32m--> 914\u001b[1;33m       \u001b[0mstructure\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    915\u001b[0m       expand_composites=expand_composites)\n\u001b[0;32m    916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\tf_utils.py\u001b[0m in \u001b[0;36m_to_single_numpy_or_python_type\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    555\u001b[0m     \u001b[1;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    556\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 557\u001b[1;33m       \u001b[0mt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    558\u001b[0m     \u001b[1;31m# Strings, ragged and sparse tensors don't have .item(). Return them as-is.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    559\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgeneric\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1221\u001b[0m     \"\"\"\n\u001b[0;32m   1222\u001b[0m     \u001b[1;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1223\u001b[1;33m     \u001b[0mmaybe_arr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1224\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1225\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1187\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1188\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1189\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1190\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1191\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Xavier initialiser and no weight decay"
      ],
      "metadata": {
        "id": "oia341F6taf2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "acc_per_fold = []\n",
        "loss_per_fold = []\n",
        "\n",
        "with tf.device('/device:GPU:0'):\n",
        "  fold_no = 1\n",
        "  for train, valid in kfold.split(trainX, trainy):\n",
        "\n",
        "    model = CNN_Model(batch_size=32, depth=3, hidden_neurons_1=64, hidden_neurons_2=128, hidden_neurons_3=256, kernel_size_1=3, kernel_size_2=3, kernel_size_3=3, \n",
        "               dropout_rate=0.2, batch_norm=False, pooling='max', layer_activation_1='relu', layer_activation_2='relu', layer_activation_3='relu',\n",
        "               kernel_initialiser='glorot_uniform', bias_initialiser='zeros', optimiser='Adam', weight_decay=None, learning_rate=1e-3, output_activation='softmax') #tf.keras.regularizers.l2(0.001)\n",
        "  \n",
        "    train_gen = tf.keras.preprocessing.image.ImageDataGenerator(width_shift_range=0.1,\n",
        "      height_shift_range=0.1,\n",
        "      horizontal_flip=True)\n",
        "    train_gen.fit(trainX[train])\n",
        "  \n",
        "    print('------------------------------------------------------------------------')\n",
        "    print(f'Training for fold {fold_no} ...')\n",
        "\n",
        "    history = model.train(\n",
        "              train_gen,\n",
        "                trainX[train],\n",
        "                  trainy[train],\n",
        "                trainX[valid],\n",
        "                  trainy[valid])\n",
        "\n",
        "    scores = model.evaluate(trainX[valid], trainy[valid])\n",
        "\n",
        "    print(f'Score for fold {fold_no}: loss of {scores[0]}; accuracy of {scores[1]*100}%')\n",
        "    acc_per_fold.append(scores[1] * 100)\n",
        "    loss_per_fold.append(scores[0])\n",
        "\n",
        "    fold_no+=1\n",
        "\n",
        "# == Provide average scores ==\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Score per fold')\n",
        "for i in range(0, len(acc_per_fold)):\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Average scores for all folds:')\n",
        "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
        "print(f'> Loss: {np.mean(loss_per_fold)}')"
      ],
      "metadata": {
        "id": "XEYCfEEwtgGF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Xavier initialiser and no decay"
      ],
      "metadata": {
        "id": "_6_Jc0TzuuYZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "acc_per_fold = []\n",
        "loss_per_fold = []\n",
        "\n",
        "with tf.device('/device:GPU:0'):\n",
        "  fold_no = 1\n",
        "  for train, valid in kfold.split(trainX, trainy):\n",
        "\n",
        "    model = CNN_Model(batch_size=32, depth=3, hidden_neurons_1=64, hidden_neurons_2=128, hidden_neurons_3=256, kernel_size_1=3, kernel_size_2=3, kernel_size_3=3, \n",
        "               dropout_rate=0.2, batch_norm=False, pooling='max', layer_activation_1='relu', layer_activation_2='relu', layer_activation_3='relu',\n",
        "               kernel_initialiser='glorot_uniform', bias_initialiser='zeros', optimiser='Adam', weight_decay=None, learning_rate=1e-3, output_activation='softmax', SGD_momentum=0) \n",
        "  \n",
        "    train_gen = tf.keras.preprocessing.image.ImageDataGenerator(width_shift_range=0.1,\n",
        "      height_shift_range=0.1,\n",
        "      horizontal_flip=True)\n",
        "    train_gen.fit(trainX[train])\n",
        "  \n",
        "    print('------------------------------------------------------------------------')\n",
        "    print(f'Training for fold {fold_no} ...')\n",
        "\n",
        "    history = model.train(\n",
        "              train_gen,\n",
        "                trainX[train],\n",
        "                  trainy[train],\n",
        "                trainX[valid],\n",
        "                  trainy[valid],\n",
        "                  lr_rate_type='step')\n",
        "\n",
        "    scores = model.evaluate(trainX[valid], trainy[valid])\n",
        "\n",
        "    print(f'Score for fold {fold_no}: loss of {scores[0]}; accuracy of {scores[1]*100}%')\n",
        "    acc_per_fold.append(scores[1] * 100)\n",
        "    loss_per_fold.append(scores[0])\n",
        "\n",
        "    fold_no+=1\n",
        "\n",
        "# == Provide average scores ==\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Score per fold')\n",
        "for i in range(0, len(acc_per_fold)):\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Average scores for all folds:')\n",
        "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
        "print(f'> Loss: {np.mean(loss_per_fold)}')"
      ],
      "metadata": {
        "id": "cDdNG7X4uyft"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Xavier initialiser and 1e-3 decay"
      ],
      "metadata": {
        "id": "rexTXJlJp_DY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "acc_per_fold = []\n",
        "loss_per_fold = []\n",
        "\n",
        "with tf.device('/device:GPU:0'):\n",
        "  fold_no = 1\n",
        "  for train, valid in kfold.split(trainX, trainy):\n",
        "\n",
        "    model = CNN_Model(batch_size=32, depth=3, hidden_neurons_1=64, hidden_neurons_2=128, hidden_neurons_3=256, kernel_size_1=3, kernel_size_2=3, kernel_size_3=3, \n",
        "               dropout_rate=0.2, batch_norm=False, pooling='max', layer_activation_1='relu', layer_activation_2='relu', layer_activation_3='relu',\n",
        "               kernel_initialiser='glorot_uniform', bias_initialiser='zeros', optimiser='Adam', weight_decay=tf.keras.regularizers.l2(1e-3), learning_rate=1e-3, output_activation='softmax', SGD_momentum=0) \n",
        "  \n",
        "    train_gen = tf.keras.preprocessing.image.ImageDataGenerator(width_shift_range=0.1,\n",
        "      height_shift_range=0.1,\n",
        "      horizontal_flip=True)\n",
        "    train_gen.fit(trainX[train])\n",
        "  \n",
        "    print('------------------------------------------------------------------------')\n",
        "    print(f'Training for fold {fold_no} ...')\n",
        "\n",
        "    history = model.train(\n",
        "              train_gen,\n",
        "                trainX[train],\n",
        "                  trainy[train],\n",
        "                trainX[valid],\n",
        "                  trainy[valid],\n",
        "                  lr_rate_type='step')\n",
        "\n",
        "    scores = model.evaluate(trainX[valid], trainy[valid])\n",
        "\n",
        "    print(f'Score for fold {fold_no}: loss of {scores[0]}; accuracy of {scores[1]*100}%')\n",
        "    acc_per_fold.append(scores[1] * 100)\n",
        "    loss_per_fold.append(scores[0])\n",
        "\n",
        "    fold_no+=1\n",
        "\n",
        "# == Provide average scores ==\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Score per fold')\n",
        "for i in range(0, len(acc_per_fold)):\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Average scores for all folds:')\n",
        "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
        "print(f'> Loss: {np.mean(loss_per_fold)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ud4wrm3ce2QS",
        "outputId": "ae25d40e-387d-427a-943a-196637066ea5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_39\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_117 (Conv2D)         (None, 32, 32, 64)        1792      \n",
            "                                                                 \n",
            " max_pooling2d_39 (MaxPoolin  multiple                 0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_118 (Conv2D)         (None, 16, 16, 128)       73856     \n",
            "                                                                 \n",
            " dropout_39 (Dropout)        (None, 8, 8, 128)         0         \n",
            "                                                                 \n",
            " conv2d_119 (Conv2D)         (None, 8, 8, 256)         295168    \n",
            "                                                                 \n",
            " flatten_39 (Flatten)        (None, 4096)              0         \n",
            "                                                                 \n",
            " dense_39 (Dense)            (None, 10)                40970     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 411,786\n",
            "Trainable params: 411,786\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/30\n",
            "1250/1250 [==============================] - 20s 15ms/step - loss: 1.6969 - accuracy: 0.4066 - val_loss: 1.4072 - val_accuracy: 0.5270 - lr: 0.0010\n",
            "Epoch 2/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.3980 - accuracy: 0.5349 - val_loss: 1.2096 - val_accuracy: 0.6190 - lr: 0.0010\n",
            "Epoch 3/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.2797 - accuracy: 0.5870 - val_loss: 1.1823 - val_accuracy: 0.6371 - lr: 0.0010\n",
            "Epoch 4/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.2117 - accuracy: 0.6180 - val_loss: 1.0660 - val_accuracy: 0.6760 - lr: 0.0010\n",
            "Epoch 5/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.1632 - accuracy: 0.6402 - val_loss: 1.0290 - val_accuracy: 0.6921 - lr: 0.0010\n",
            "Epoch 6/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.1273 - accuracy: 0.6569 - val_loss: 1.0110 - val_accuracy: 0.7061 - lr: 0.0010\n",
            "Epoch 7/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.1093 - accuracy: 0.6658 - val_loss: 0.9553 - val_accuracy: 0.7252 - lr: 0.0010\n",
            "Epoch 8/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.0849 - accuracy: 0.6738 - val_loss: 1.0280 - val_accuracy: 0.6965 - lr: 0.0010\n",
            "Epoch 9/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.0635 - accuracy: 0.6831 - val_loss: 0.9419 - val_accuracy: 0.7305 - lr: 0.0010\n",
            "Epoch 10/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.0566 - accuracy: 0.6873 - val_loss: 0.9446 - val_accuracy: 0.7356 - lr: 0.0010\n",
            "Epoch 11/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 1.0396 - accuracy: 0.6945 - val_loss: 0.9225 - val_accuracy: 0.7383 - lr: 0.0010\n",
            "Epoch 12/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.0274 - accuracy: 0.7019 - val_loss: 0.9638 - val_accuracy: 0.7272 - lr: 0.0010\n",
            "Epoch 13/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.0189 - accuracy: 0.7062 - val_loss: 0.9294 - val_accuracy: 0.7405 - lr: 0.0010\n",
            "Epoch 14/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.0131 - accuracy: 0.7092 - val_loss: 0.9892 - val_accuracy: 0.7259 - lr: 0.0010\n",
            "Epoch 15/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.9987 - accuracy: 0.7138 - val_loss: 0.9805 - val_accuracy: 0.7255 - lr: 0.0010\n",
            "Epoch 16/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.9761 - accuracy: 0.7198 - val_loss: 0.9437 - val_accuracy: 0.7381 - lr: 9.0484e-04\n",
            "Epoch 17/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.9585 - accuracy: 0.7267 - val_loss: 0.8681 - val_accuracy: 0.7540 - lr: 8.1873e-04\n",
            "Epoch 18/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.9387 - accuracy: 0.7322 - val_loss: 0.8565 - val_accuracy: 0.7630 - lr: 7.4082e-04\n",
            "Epoch 19/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.9135 - accuracy: 0.7409 - val_loss: 0.8380 - val_accuracy: 0.7713 - lr: 6.7032e-04\n",
            "Epoch 20/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.9015 - accuracy: 0.7436 - val_loss: 0.8514 - val_accuracy: 0.7640 - lr: 6.0653e-04\n",
            "Epoch 21/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.8810 - accuracy: 0.7492 - val_loss: 0.8435 - val_accuracy: 0.7602 - lr: 5.4881e-04\n",
            "Epoch 22/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.8628 - accuracy: 0.7556 - val_loss: 0.7850 - val_accuracy: 0.7838 - lr: 4.9659e-04\n",
            "Epoch 23/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.8485 - accuracy: 0.7607 - val_loss: 0.8039 - val_accuracy: 0.7764 - lr: 4.4933e-04\n",
            "Epoch 24/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.8358 - accuracy: 0.7637 - val_loss: 0.8446 - val_accuracy: 0.7595 - lr: 4.0657e-04\n",
            "Epoch 25/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.8214 - accuracy: 0.7684 - val_loss: 0.7816 - val_accuracy: 0.7857 - lr: 3.6788e-04\n",
            "Epoch 26/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.8188 - accuracy: 0.7687 - val_loss: 0.7685 - val_accuracy: 0.7870 - lr: 3.3287e-04\n",
            "Epoch 27/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.7946 - accuracy: 0.7764 - val_loss: 0.7839 - val_accuracy: 0.7803 - lr: 3.0119e-04\n",
            "Epoch 28/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.7884 - accuracy: 0.7801 - val_loss: 0.7798 - val_accuracy: 0.7840 - lr: 2.7253e-04\n",
            "Epoch 29/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.7807 - accuracy: 0.7806 - val_loss: 0.7662 - val_accuracy: 0.7878 - lr: 2.4660e-04\n",
            "Epoch 30/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.7704 - accuracy: 0.7818 - val_loss: 0.7596 - val_accuracy: 0.7883 - lr: 2.2313e-04\n",
            "Score for fold 1: loss of 0.7595964670181274; accuracy of 78.82999777793884%\n",
            "Model: \"sequential_40\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_120 (Conv2D)         (None, 32, 32, 64)        1792      \n",
            "                                                                 \n",
            " max_pooling2d_40 (MaxPoolin  multiple                 0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_121 (Conv2D)         (None, 16, 16, 128)       73856     \n",
            "                                                                 \n",
            " dropout_40 (Dropout)        (None, 8, 8, 128)         0         \n",
            "                                                                 \n",
            " conv2d_122 (Conv2D)         (None, 8, 8, 256)         295168    \n",
            "                                                                 \n",
            " flatten_40 (Flatten)        (None, 4096)              0         \n",
            "                                                                 \n",
            " dense_40 (Dense)            (None, 10)                40970     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 411,786\n",
            "Trainable params: 411,786\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/30\n",
            " 924/1250 [=====================>........] - ETA: 4s - loss: 1.7400 - accuracy: 0.3943"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_22076/659367116.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'Training for fold {fold_no} ...'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m     history = model.train(\n\u001b[0m\u001b[0;32m     21\u001b[0m               \u001b[0mtrain_gen\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m                 \u001b[0mtrainX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_22076/4196491080.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, train_gen, train_x, train_y, test_x, test_y, lr_rate, lr_rate_type)\u001b[0m\n\u001b[0;32m    160\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_gen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtensorboard_callback\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcallback\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    161\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 162\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_gen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtensorboard_callback\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcallback\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m#  self.model.fit(train_x, train_y, validation_data=(test_x, test_y) , epochs=25, verbose=1)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    163\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_gen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtensorboard_callback\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1382\u001b[0m                 _r=1):\n\u001b[0;32m   1383\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1384\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1385\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1386\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    913\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    914\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 915\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    945\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    946\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 947\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    948\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2954\u001b[0m       (graph_function,\n\u001b[0;32m   2955\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2956\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   2957\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   2958\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1851\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1852\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1853\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1854\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1855\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    498\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 499\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    500\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    501\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Xavier initialiser and 1e-5 decay"
      ],
      "metadata": {
        "id": "XOKO0j_hqETp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "acc_per_fold = []\n",
        "loss_per_fold = []\n",
        "\n",
        "with tf.device('/device:GPU:0'):\n",
        "  fold_no = 1\n",
        "  for train, valid in kfold.split(trainX, trainy):\n",
        "\n",
        "    model = CNN_Model(batch_size=32, depth=3, hidden_neurons_1=64, hidden_neurons_2=128, hidden_neurons_3=256, kernel_size_1=3, kernel_size_2=3, kernel_size_3=3, \n",
        "               dropout_rate=0.2, batch_norm=False, pooling='max', layer_activation_1='relu', layer_activation_2='relu', layer_activation_3='relu',\n",
        "               kernel_initialiser='glorot_uniform', bias_initialiser='zeros', optimiser='Adam', weight_decay=tf.keras.regularizers.l2(1e-5), learning_rate=1e-3, output_activation='softmax', SGD_momentum=0) \n",
        "  \n",
        "    train_gen = tf.keras.preprocessing.image.ImageDataGenerator(width_shift_range=0.1,\n",
        "      height_shift_range=0.1,\n",
        "      horizontal_flip=True)\n",
        "    train_gen.fit(trainX[train])\n",
        "  \n",
        "    print('------------------------------------------------------------------------')\n",
        "    print(f'Training for fold {fold_no} ...')\n",
        "\n",
        "    history = model.train(\n",
        "              train_gen,\n",
        "                trainX[train],\n",
        "                  trainy[train],\n",
        "                trainX[valid],\n",
        "                  trainy[valid],\n",
        "                  lr_rate_type='step')\n",
        "\n",
        "    scores = model.evaluate(trainX[valid], trainy[valid])\n",
        "\n",
        "    print(f'Score for fold {fold_no}: loss of {scores[0]}; accuracy of {scores[1]*100}%')\n",
        "    acc_per_fold.append(scores[1] * 100)\n",
        "    loss_per_fold.append(scores[0])\n",
        "\n",
        "    fold_no+=1\n",
        "\n",
        "# == Provide average scores ==\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Score per fold')\n",
        "for i in range(0, len(acc_per_fold)):\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Average scores for all folds:')\n",
        "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
        "print(f'> Loss: {np.mean(loss_per_fold)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "fY9hoPKzfC4z",
        "outputId": "c72145e3-ec0f-4901-99c5-ac19261d3583"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_41\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_123 (Conv2D)         (None, 32, 32, 64)        1792      \n",
            "                                                                 \n",
            " max_pooling2d_41 (MaxPoolin  multiple                 0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_124 (Conv2D)         (None, 16, 16, 128)       73856     \n",
            "                                                                 \n",
            " dropout_41 (Dropout)        (None, 8, 8, 128)         0         \n",
            "                                                                 \n",
            " conv2d_125 (Conv2D)         (None, 8, 8, 256)         295168    \n",
            "                                                                 \n",
            " flatten_41 (Flatten)        (None, 4096)              0         \n",
            "                                                                 \n",
            " dense_41 (Dense)            (None, 10)                40970     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 411,786\n",
            "Trainable params: 411,786\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.5624 - accuracy: 0.4318 - val_loss: 1.2499 - val_accuracy: 0.5553 - lr: 0.0010\n",
            "Epoch 2/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.1762 - accuracy: 0.5853 - val_loss: 0.9702 - val_accuracy: 0.6682 - lr: 0.0010\n",
            "Epoch 3/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.0262 - accuracy: 0.6455 - val_loss: 0.9242 - val_accuracy: 0.6808 - lr: 0.0010\n",
            "Epoch 4/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.9407 - accuracy: 0.6772 - val_loss: 0.8518 - val_accuracy: 0.7150 - lr: 0.0010\n",
            "Epoch 5/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.8781 - accuracy: 0.6983 - val_loss: 0.7896 - val_accuracy: 0.7296 - lr: 0.0010\n",
            "Epoch 6/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.8388 - accuracy: 0.7166 - val_loss: 0.8054 - val_accuracy: 0.7278 - lr: 0.0010\n",
            "Epoch 7/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.8045 - accuracy: 0.7271 - val_loss: 0.8059 - val_accuracy: 0.7329 - lr: 0.0010\n",
            "Epoch 8/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.7791 - accuracy: 0.7369 - val_loss: 0.7543 - val_accuracy: 0.7429 - lr: 0.0010\n",
            "Epoch 9/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.7531 - accuracy: 0.7478 - val_loss: 0.7299 - val_accuracy: 0.7581 - lr: 0.0010\n",
            "Epoch 10/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.7344 - accuracy: 0.7546 - val_loss: 0.7075 - val_accuracy: 0.7671 - lr: 0.0010\n",
            "Epoch 11/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.7133 - accuracy: 0.7605 - val_loss: 0.6956 - val_accuracy: 0.7764 - lr: 0.0010\n",
            "Epoch 12/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.6931 - accuracy: 0.7712 - val_loss: 0.6860 - val_accuracy: 0.7746 - lr: 0.0010\n",
            "Epoch 13/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.6840 - accuracy: 0.7739 - val_loss: 0.7398 - val_accuracy: 0.7621 - lr: 0.0010\n",
            "Epoch 14/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.6760 - accuracy: 0.7774 - val_loss: 0.6998 - val_accuracy: 0.7756 - lr: 0.0010\n",
            "Epoch 15/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.6654 - accuracy: 0.7811 - val_loss: 0.6623 - val_accuracy: 0.7879 - lr: 0.0010\n",
            "Epoch 16/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.6415 - accuracy: 0.7870 - val_loss: 0.6731 - val_accuracy: 0.7869 - lr: 9.0484e-04\n",
            "Epoch 17/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.6166 - accuracy: 0.7982 - val_loss: 0.6474 - val_accuracy: 0.8002 - lr: 8.1873e-04\n",
            "Epoch 18/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.5922 - accuracy: 0.8072 - val_loss: 0.6492 - val_accuracy: 0.7956 - lr: 7.4082e-04\n",
            "Epoch 19/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.5749 - accuracy: 0.8126 - val_loss: 0.6301 - val_accuracy: 0.8017 - lr: 6.7032e-04\n",
            "Epoch 20/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.5550 - accuracy: 0.8201 - val_loss: 0.6513 - val_accuracy: 0.7964 - lr: 6.0653e-04\n",
            "Epoch 21/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.5369 - accuracy: 0.8277 - val_loss: 0.6100 - val_accuracy: 0.8087 - lr: 5.4881e-04\n",
            "Epoch 22/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.5298 - accuracy: 0.8269 - val_loss: 0.6071 - val_accuracy: 0.8112 - lr: 4.9659e-04\n",
            "Epoch 23/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.5091 - accuracy: 0.8344 - val_loss: 0.6186 - val_accuracy: 0.8070 - lr: 4.4933e-04\n",
            "Epoch 24/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.4991 - accuracy: 0.8393 - val_loss: 0.6115 - val_accuracy: 0.8091 - lr: 4.0657e-04\n",
            "Epoch 25/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.4868 - accuracy: 0.8419 - val_loss: 0.6112 - val_accuracy: 0.8148 - lr: 3.6788e-04\n",
            "Epoch 26/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.4741 - accuracy: 0.8468 - val_loss: 0.5879 - val_accuracy: 0.8202 - lr: 3.3287e-04\n",
            "Epoch 27/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.4659 - accuracy: 0.8482 - val_loss: 0.6037 - val_accuracy: 0.8187 - lr: 3.0119e-04\n",
            "Epoch 28/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.4599 - accuracy: 0.8527 - val_loss: 0.6005 - val_accuracy: 0.8143 - lr: 2.7253e-04\n",
            "Epoch 29/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.4451 - accuracy: 0.8561 - val_loss: 0.5741 - val_accuracy: 0.8258 - lr: 2.4660e-04\n",
            "Epoch 30/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.4400 - accuracy: 0.8602 - val_loss: 0.5819 - val_accuracy: 0.8214 - lr: 2.2313e-04\n",
            "Score for fold 1: loss of 0.5819459557533264; accuracy of 82.1399986743927%\n",
            "Model: \"sequential_42\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_126 (Conv2D)         (None, 32, 32, 64)        1792      \n",
            "                                                                 \n",
            " max_pooling2d_42 (MaxPoolin  multiple                 0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_127 (Conv2D)         (None, 16, 16, 128)       73856     \n",
            "                                                                 \n",
            " dropout_42 (Dropout)        (None, 8, 8, 128)         0         \n",
            "                                                                 \n",
            " conv2d_128 (Conv2D)         (None, 8, 8, 256)         295168    \n",
            "                                                                 \n",
            " flatten_42 (Flatten)        (None, 4096)              0         \n",
            "                                                                 \n",
            " dense_42 (Dense)            (None, 10)                40970     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 411,786\n",
            "Trainable params: 411,786\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.5592 - accuracy: 0.4359 - val_loss: 1.2155 - val_accuracy: 0.5713 - lr: 0.0010\n",
            "Epoch 2/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.1776 - accuracy: 0.5871 - val_loss: 1.0068 - val_accuracy: 0.6534 - lr: 0.0010\n",
            "Epoch 3/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.0238 - accuracy: 0.6457 - val_loss: 0.8847 - val_accuracy: 0.7018 - lr: 0.0010\n",
            "Epoch 4/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.9377 - accuracy: 0.6785 - val_loss: 0.8392 - val_accuracy: 0.7143 - lr: 0.0010\n",
            "Epoch 5/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.8782 - accuracy: 0.6988 - val_loss: 0.8121 - val_accuracy: 0.7222 - lr: 0.0010\n",
            "Epoch 6/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.8319 - accuracy: 0.7156 - val_loss: 0.7915 - val_accuracy: 0.7308 - lr: 0.0010\n",
            "Epoch 7/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.8067 - accuracy: 0.7268 - val_loss: 0.7375 - val_accuracy: 0.7534 - lr: 0.0010\n",
            "Epoch 8/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.7697 - accuracy: 0.7395 - val_loss: 0.7679 - val_accuracy: 0.7437 - lr: 0.0010\n",
            "Epoch 9/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 0.7577 - accuracy: 0.7431 - val_loss: 0.7348 - val_accuracy: 0.7600 - lr: 0.0010\n",
            "Epoch 10/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.7338 - accuracy: 0.7532 - val_loss: 0.7528 - val_accuracy: 0.7519 - lr: 0.0010\n",
            "Epoch 11/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.7202 - accuracy: 0.7589 - val_loss: 0.7286 - val_accuracy: 0.7587 - lr: 0.0010\n",
            "Epoch 12/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.7024 - accuracy: 0.7676 - val_loss: 0.7009 - val_accuracy: 0.7735 - lr: 0.0010\n",
            "Epoch 13/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.6885 - accuracy: 0.7710 - val_loss: 0.7204 - val_accuracy: 0.7630 - lr: 0.0010\n",
            "Epoch 14/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.6819 - accuracy: 0.7717 - val_loss: 0.6844 - val_accuracy: 0.7763 - lr: 0.0010\n",
            "Epoch 15/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 0.6652 - accuracy: 0.7812 - val_loss: 0.7263 - val_accuracy: 0.7620 - lr: 0.0010\n",
            "Epoch 16/30\n",
            "1124/1250 [=========================>....] - ETA: 1s - loss: 0.6415 - accuracy: 0.7866"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_22076/1906139115.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'Training for fold {fold_no} ...'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m     history = model.train(\n\u001b[0m\u001b[0;32m     21\u001b[0m               \u001b[0mtrain_gen\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m                 \u001b[0mtrainX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_22076/4196491080.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, train_gen, train_x, train_y, test_x, test_y, lr_rate, lr_rate_type)\u001b[0m\n\u001b[0;32m    160\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_gen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtensorboard_callback\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcallback\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    161\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 162\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_gen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtensorboard_callback\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcallback\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m#  self.model.fit(train_x, train_y, validation_data=(test_x, test_y) , epochs=25, verbose=1)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    163\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_gen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtensorboard_callback\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1382\u001b[0m                 _r=1):\n\u001b[0;32m   1383\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1384\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1385\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1386\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    913\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    914\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 915\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    945\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    946\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 947\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    948\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2954\u001b[0m       (graph_function,\n\u001b[0;32m   2955\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2956\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   2957\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   2958\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1851\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1852\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1853\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1854\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1855\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    498\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 499\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    500\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    501\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### He initaliser No weight decay"
      ],
      "metadata": {
        "id": "9n3QVvmXpxkv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "acc_per_fold = []\n",
        "loss_per_fold = []\n",
        "\n",
        "with tf.device('/device:GPU:0'):\n",
        "  fold_no = 1\n",
        "  for train, valid in kfold.split(trainX, trainy):\n",
        "\n",
        "    model = CNN_Model(batch_size=32, depth=3, hidden_neurons_1=64, hidden_neurons_2=128, hidden_neurons_3=256, kernel_size_1=3, kernel_size_2=3, kernel_size_3=3, \n",
        "               dropout_rate=0.2, batch_norm=False, pooling='max', layer_activation_1='relu', layer_activation_2='relu', layer_activation_3='relu',\n",
        "               kernel_initialiser=tf.keras.initializers.HeNormal(), bias_initialiser='zeros', optimiser='Adam', weight_decay=None, learning_rate=1e-3, output_activation='softmax', SGD_momentum=0) \n",
        "  \n",
        "    train_gen = tf.keras.preprocessing.image.ImageDataGenerator(width_shift_range=0.1,\n",
        "      height_shift_range=0.1,\n",
        "      horizontal_flip=True)\n",
        "    train_gen.fit(trainX[train])\n",
        "  \n",
        "    print('------------------------------------------------------------------------')\n",
        "    print(f'Training for fold {fold_no} ...')\n",
        "\n",
        "    history = model.train(\n",
        "              train_gen,\n",
        "                trainX[train],\n",
        "                  trainy[train],\n",
        "                trainX[valid],\n",
        "                  trainy[valid],\n",
        "                  lr_rate_type='step')\n",
        "\n",
        "    scores = model.evaluate(trainX[valid], trainy[valid])\n",
        "\n",
        "    print(f'Score for fold {fold_no}: loss of {scores[0]}; accuracy of {scores[1]*100}%')\n",
        "    acc_per_fold.append(scores[1] * 100)\n",
        "    loss_per_fold.append(scores[0])\n",
        "\n",
        "    fold_no+=1\n",
        "\n",
        "# == Provide average scores ==\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Score per fold')\n",
        "for i in range(0, len(acc_per_fold)):\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Average scores for all folds:')\n",
        "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
        "print(f'> Loss: {np.mean(loss_per_fold)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "BVAOEqVAaaak",
        "outputId": "22390635-8075-4f20-dbc1-b77bdaf56542"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_43\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_129 (Conv2D)         (None, 32, 32, 64)        1792      \n",
            "                                                                 \n",
            " max_pooling2d_43 (MaxPoolin  multiple                 0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_130 (Conv2D)         (None, 16, 16, 128)       73856     \n",
            "                                                                 \n",
            " dropout_43 (Dropout)        (None, 8, 8, 128)         0         \n",
            "                                                                 \n",
            " conv2d_131 (Conv2D)         (None, 8, 8, 256)         295168    \n",
            "                                                                 \n",
            " flatten_43 (Flatten)        (None, 4096)              0         \n",
            "                                                                 \n",
            " dense_43 (Dense)            (None, 10)                40970     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 411,786\n",
            "Trainable params: 411,786\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 1.5382 - accuracy: 0.4469 - val_loss: 1.2424 - val_accuracy: 0.5499 - lr: 0.0010\n",
            "Epoch 2/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.1486 - accuracy: 0.5966 - val_loss: 1.0013 - val_accuracy: 0.6556 - lr: 0.0010\n",
            "Epoch 3/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 1.0038 - accuracy: 0.6494 - val_loss: 0.9740 - val_accuracy: 0.6602 - lr: 0.0010\n",
            "Epoch 4/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 0.9180 - accuracy: 0.6819 - val_loss: 0.8387 - val_accuracy: 0.7126 - lr: 0.0010\n",
            "Epoch 5/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.8694 - accuracy: 0.6946 - val_loss: 0.7907 - val_accuracy: 0.7310 - lr: 0.0010\n",
            "Epoch 6/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.8210 - accuracy: 0.7146 - val_loss: 0.7199 - val_accuracy: 0.7468 - lr: 0.0010\n",
            "Epoch 7/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.7883 - accuracy: 0.7264 - val_loss: 0.7648 - val_accuracy: 0.7379 - lr: 0.0010\n",
            "Epoch 8/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.7596 - accuracy: 0.7358 - val_loss: 0.7467 - val_accuracy: 0.7455 - lr: 0.0010\n",
            "Epoch 9/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.7340 - accuracy: 0.7472 - val_loss: 0.6840 - val_accuracy: 0.7646 - lr: 0.0010\n",
            "Epoch 10/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.7085 - accuracy: 0.7501 - val_loss: 0.7147 - val_accuracy: 0.7583 - lr: 0.0010\n",
            "Epoch 11/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.6968 - accuracy: 0.7573 - val_loss: 0.7550 - val_accuracy: 0.7456 - lr: 0.0010\n",
            "Epoch 12/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.6826 - accuracy: 0.7617 - val_loss: 0.7186 - val_accuracy: 0.7576 - lr: 0.0010\n",
            "Epoch 13/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.6681 - accuracy: 0.7683 - val_loss: 0.7561 - val_accuracy: 0.7504 - lr: 0.0010\n",
            "Epoch 14/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.6535 - accuracy: 0.7728 - val_loss: 0.6605 - val_accuracy: 0.7801 - lr: 0.0010\n",
            "Epoch 15/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.6395 - accuracy: 0.7766 - val_loss: 0.6720 - val_accuracy: 0.7756 - lr: 0.0010\n",
            "Epoch 16/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.6213 - accuracy: 0.7845 - val_loss: 0.6941 - val_accuracy: 0.7614 - lr: 9.0484e-04\n",
            "Epoch 17/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.5965 - accuracy: 0.7944 - val_loss: 0.6440 - val_accuracy: 0.7825 - lr: 8.1873e-04\n",
            "Epoch 18/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.5734 - accuracy: 0.8007 - val_loss: 0.6530 - val_accuracy: 0.7827 - lr: 7.4082e-04\n",
            "Epoch 19/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.5563 - accuracy: 0.8070 - val_loss: 0.6183 - val_accuracy: 0.7982 - lr: 6.7032e-04\n",
            "Epoch 20/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.5326 - accuracy: 0.8142 - val_loss: 0.6250 - val_accuracy: 0.7921 - lr: 6.0653e-04\n",
            "Epoch 21/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.5108 - accuracy: 0.8224 - val_loss: 0.6017 - val_accuracy: 0.7987 - lr: 5.4881e-04\n",
            "Epoch 22/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.4937 - accuracy: 0.8281 - val_loss: 0.5984 - val_accuracy: 0.8038 - lr: 4.9659e-04\n",
            "Epoch 23/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.4818 - accuracy: 0.8324 - val_loss: 0.6058 - val_accuracy: 0.8007 - lr: 4.4933e-04\n",
            "Epoch 24/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.4665 - accuracy: 0.8374 - val_loss: 0.5823 - val_accuracy: 0.8098 - lr: 4.0657e-04\n",
            "Epoch 25/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.4590 - accuracy: 0.8393 - val_loss: 0.5570 - val_accuracy: 0.8150 - lr: 3.6788e-04\n",
            "Epoch 26/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.4514 - accuracy: 0.8400 - val_loss: 0.5762 - val_accuracy: 0.8110 - lr: 3.3287e-04\n",
            "Epoch 27/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.4362 - accuracy: 0.8469 - val_loss: 0.5566 - val_accuracy: 0.8177 - lr: 3.0119e-04\n",
            "Epoch 28/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.4264 - accuracy: 0.8517 - val_loss: 0.5636 - val_accuracy: 0.8138 - lr: 2.7253e-04\n",
            "Epoch 29/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 0.4231 - accuracy: 0.8536 - val_loss: 0.5547 - val_accuracy: 0.8198 - lr: 2.4660e-04\n",
            "Epoch 30/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.4095 - accuracy: 0.8565 - val_loss: 0.5540 - val_accuracy: 0.8206 - lr: 2.2313e-04\n",
            "Score for fold 1: loss of 0.5539748668670654; accuracy of 82.05999732017517%\n",
            "Model: \"sequential_44\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_132 (Conv2D)         (None, 32, 32, 64)        1792      \n",
            "                                                                 \n",
            " max_pooling2d_44 (MaxPoolin  multiple                 0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_133 (Conv2D)         (None, 16, 16, 128)       73856     \n",
            "                                                                 \n",
            " dropout_44 (Dropout)        (None, 8, 8, 128)         0         \n",
            "                                                                 \n",
            " conv2d_134 (Conv2D)         (None, 8, 8, 256)         295168    \n",
            "                                                                 \n",
            " flatten_44 (Flatten)        (None, 4096)              0         \n",
            "                                                                 \n",
            " dense_44 (Dense)            (None, 10)                40970     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 411,786\n",
            "Trainable params: 411,786\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/30\n",
            "1250/1250 [==============================] - 19s 14ms/step - loss: 1.5212 - accuracy: 0.4552 - val_loss: 1.1790 - val_accuracy: 0.5787 - lr: 0.0010\n",
            "Epoch 2/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.1523 - accuracy: 0.5928 - val_loss: 0.9864 - val_accuracy: 0.6603 - lr: 0.0010\n",
            "Epoch 3/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.0166 - accuracy: 0.6451 - val_loss: 0.8947 - val_accuracy: 0.6969 - lr: 0.0010\n",
            "Epoch 4/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.9203 - accuracy: 0.6827 - val_loss: 0.8776 - val_accuracy: 0.6970 - lr: 0.0010\n",
            "Epoch 5/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.8610 - accuracy: 0.7031 - val_loss: 0.7841 - val_accuracy: 0.7307 - lr: 0.0010\n",
            "Epoch 6/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.8251 - accuracy: 0.7114 - val_loss: 0.7694 - val_accuracy: 0.7379 - lr: 0.0010\n",
            "Epoch 7/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.7828 - accuracy: 0.7266 - val_loss: 0.7581 - val_accuracy: 0.7425 - lr: 0.0010\n",
            "Epoch 8/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.7528 - accuracy: 0.7397 - val_loss: 0.7405 - val_accuracy: 0.7472 - lr: 0.0010\n",
            "Epoch 9/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.7280 - accuracy: 0.7491 - val_loss: 0.7231 - val_accuracy: 0.7532 - lr: 0.0010\n",
            "Epoch 10/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.7077 - accuracy: 0.7547 - val_loss: 0.7055 - val_accuracy: 0.7590 - lr: 0.0010\n",
            "Epoch 11/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.6885 - accuracy: 0.7598 - val_loss: 0.6908 - val_accuracy: 0.7675 - lr: 0.0010\n",
            "Epoch 12/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.6785 - accuracy: 0.7650 - val_loss: 0.6705 - val_accuracy: 0.7752 - lr: 0.0010\n",
            "Epoch 13/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.6563 - accuracy: 0.7718 - val_loss: 0.6863 - val_accuracy: 0.7688 - lr: 0.0010\n",
            "Epoch 14/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.6470 - accuracy: 0.7762 - val_loss: 0.6252 - val_accuracy: 0.7936 - lr: 0.0010\n",
            "Epoch 15/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.6340 - accuracy: 0.7778 - val_loss: 0.6487 - val_accuracy: 0.7842 - lr: 0.0010\n",
            "Epoch 16/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.6017 - accuracy: 0.7905 - val_loss: 0.6708 - val_accuracy: 0.7884 - lr: 9.0484e-04\n",
            "Epoch 17/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.5771 - accuracy: 0.8004 - val_loss: 0.6788 - val_accuracy: 0.7772 - lr: 8.1873e-04\n",
            "Epoch 18/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.5611 - accuracy: 0.8048 - val_loss: 0.6876 - val_accuracy: 0.7749 - lr: 7.4082e-04\n",
            "Epoch 19/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.5437 - accuracy: 0.8116 - val_loss: 0.6041 - val_accuracy: 0.8066 - lr: 6.7032e-04\n",
            "Epoch 20/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.5167 - accuracy: 0.8188 - val_loss: 0.6091 - val_accuracy: 0.8032 - lr: 6.0653e-04\n",
            "Epoch 21/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.5060 - accuracy: 0.8227 - val_loss: 0.6221 - val_accuracy: 0.7987 - lr: 5.4881e-04\n",
            "Epoch 22/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.4815 - accuracy: 0.8310 - val_loss: 0.6077 - val_accuracy: 0.8038 - lr: 4.9659e-04\n",
            "Epoch 23/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.4684 - accuracy: 0.8349 - val_loss: 0.5838 - val_accuracy: 0.8084 - lr: 4.4933e-04\n",
            "Epoch 24/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.4529 - accuracy: 0.8422 - val_loss: 0.6111 - val_accuracy: 0.8035 - lr: 4.0657e-04\n",
            "Epoch 25/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.4428 - accuracy: 0.8445 - val_loss: 0.5599 - val_accuracy: 0.8162 - lr: 3.6788e-04\n",
            "Epoch 26/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.4334 - accuracy: 0.8465 - val_loss: 0.5779 - val_accuracy: 0.8165 - lr: 3.3287e-04\n",
            "Epoch 27/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 0.4188 - accuracy: 0.8543 - val_loss: 0.5720 - val_accuracy: 0.8157 - lr: 3.0119e-04\n",
            "Epoch 28/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.4091 - accuracy: 0.8585 - val_loss: 0.5543 - val_accuracy: 0.8187 - lr: 2.7253e-04\n",
            "Epoch 29/30\n",
            " 655/1250 [==============>...............] - ETA: 7s - loss: 0.4044 - accuracy: 0.8595"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_22076/1153685671.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'Training for fold {fold_no} ...'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m     history = model.train(\n\u001b[0m\u001b[0;32m     21\u001b[0m               \u001b[0mtrain_gen\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m                 \u001b[0mtrainX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_22076/4196491080.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, train_gen, train_x, train_y, test_x, test_y, lr_rate, lr_rate_type)\u001b[0m\n\u001b[0;32m    160\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_gen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtensorboard_callback\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcallback\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    161\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 162\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_gen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtensorboard_callback\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcallback\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m#  self.model.fit(train_x, train_y, validation_data=(test_x, test_y) , epochs=25, verbose=1)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    163\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_gen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtensorboard_callback\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1382\u001b[0m                 _r=1):\n\u001b[0;32m   1383\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1384\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1385\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1386\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    913\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    914\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 915\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    945\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    946\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 947\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    948\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2954\u001b[0m       (graph_function,\n\u001b[0;32m   2955\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2956\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   2957\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   2958\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1851\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1852\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1853\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1854\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1855\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    498\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 499\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    500\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    501\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### He initialiser and 1e-3 weight decay"
      ],
      "metadata": {
        "id": "kHYG2yhctsCP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "acc_per_fold = []\n",
        "loss_per_fold = []\n",
        "\n",
        "with tf.device('/device:GPU:0'):\n",
        "  fold_no = 1\n",
        "  for train, valid in kfold.split(trainX, trainy):\n",
        "\n",
        "    model = CNN_Model(batch_size=32, depth=3, hidden_neurons_1=64, hidden_neurons_2=128, hidden_neurons_3=256, kernel_size_1=3, kernel_size_2=3, kernel_size_3=3, \n",
        "               dropout_rate=0.2, batch_norm=False, pooling='max', layer_activation_1='relu', layer_activation_2='relu', layer_activation_3='relu',\n",
        "               kernel_initialiser=tf.keras.initializers.HeNormal(), bias_initialiser='zeros', optimiser='Adam', weight_decay=tf.keras.regularizers.l2(1e-3), learning_rate=1e-3, output_activation='softmax', SGD_momentum=0) \n",
        "  \n",
        "    train_gen = tf.keras.preprocessing.image.ImageDataGenerator(width_shift_range=0.1,\n",
        "      height_shift_range=0.1,\n",
        "      horizontal_flip=True)\n",
        "    train_gen.fit(trainX[train])\n",
        "  \n",
        "    print('------------------------------------------------------------------------')\n",
        "    print(f'Training for fold {fold_no} ...')\n",
        "\n",
        "    history = model.train(\n",
        "              train_gen,\n",
        "                trainX[train],\n",
        "                  trainy[train],\n",
        "                trainX[valid],\n",
        "                  trainy[valid],\n",
        "                  lr_rate_type='step')\n",
        "\n",
        "    scores = model.evaluate(trainX[valid], trainy[valid])\n",
        "\n",
        "    print(f'Score for fold {fold_no}: loss of {scores[0]}; accuracy of {scores[1]*100}%')\n",
        "    acc_per_fold.append(scores[1] * 100)\n",
        "    loss_per_fold.append(scores[0])\n",
        "\n",
        "    fold_no+=1\n",
        "\n",
        "# == Provide average scores ==\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Score per fold')\n",
        "for i in range(0, len(acc_per_fold)):\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Average scores for all folds:')\n",
        "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
        "print(f'> Loss: {np.mean(loss_per_fold)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Y1mHXpZjt96p",
        "outputId": "96167400-f578-4b07-c9c0-8dc7bc04fd26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_45\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_135 (Conv2D)         (None, 32, 32, 64)        1792      \n",
            "                                                                 \n",
            " max_pooling2d_45 (MaxPoolin  multiple                 0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_136 (Conv2D)         (None, 16, 16, 128)       73856     \n",
            "                                                                 \n",
            " dropout_45 (Dropout)        (None, 8, 8, 128)         0         \n",
            "                                                                 \n",
            " conv2d_137 (Conv2D)         (None, 8, 8, 256)         295168    \n",
            "                                                                 \n",
            " flatten_45 (Flatten)        (None, 4096)              0         \n",
            "                                                                 \n",
            " dense_45 (Dense)            (None, 10)                40970     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 411,786\n",
            "Trainable params: 411,786\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/30\n",
            "1250/1250 [==============================] - 20s 15ms/step - loss: 1.8795 - accuracy: 0.4586 - val_loss: 1.4720 - val_accuracy: 0.5693 - lr: 0.0010\n",
            "Epoch 2/30\n",
            " 502/1250 [===========>..................] - ETA: 10s - loss: 1.4879 - accuracy: 0.5630"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_22076/2459144388.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'Training for fold {fold_no} ...'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m     history = model.train(\n\u001b[0m\u001b[0;32m     21\u001b[0m               \u001b[0mtrain_gen\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m                 \u001b[0mtrainX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_22076/4196491080.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, train_gen, train_x, train_y, test_x, test_y, lr_rate, lr_rate_type)\u001b[0m\n\u001b[0;32m    160\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_gen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtensorboard_callback\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcallback\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    161\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 162\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_gen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtensorboard_callback\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcallback\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m#  self.model.fit(train_x, train_y, validation_data=(test_x, test_y) , epochs=25, verbose=1)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    163\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_gen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtensorboard_callback\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1382\u001b[0m                 _r=1):\n\u001b[0;32m   1383\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1384\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1385\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1386\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    913\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    914\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 915\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    945\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    946\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 947\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    948\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2954\u001b[0m       (graph_function,\n\u001b[0;32m   2955\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2956\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   2957\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   2958\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1851\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1852\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1853\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1854\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1855\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    498\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 499\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    500\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    501\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### He initialiser and 1e-5 weight decay"
      ],
      "metadata": {
        "id": "NUU27JJ3tvX9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "acc_per_fold = []\n",
        "loss_per_fold = []\n",
        "\n",
        "with tf.device('/device:GPU:0'):\n",
        "  fold_no = 1\n",
        "  for train, valid in kfold.split(trainX, trainy):\n",
        "\n",
        "    model = CNN_Model(batch_size=32, depth=3, hidden_neurons_1=64, hidden_neurons_2=128, hidden_neurons_3=256, kernel_size_1=3, kernel_size_2=3, kernel_size_3=3, \n",
        "               dropout_rate=0.2, batch_norm=False, pooling='max', layer_activation_1='relu', layer_activation_2='relu', layer_activation_3='relu',\n",
        "               kernel_initialiser=tf.keras.initializers.HeNormal(), bias_initialiser='zeros', optimiser='Adam', weight_decay=tf.keras.regularizers.l2(1e-5), learning_rate=1e-3, output_activation='softmax', SGD_momentum=0)\n",
        "  \n",
        "    train_gen = tf.keras.preprocessing.image.ImageDataGenerator(width_shift_range=0.1,\n",
        "      height_shift_range=0.1,\n",
        "      horizontal_flip=True)\n",
        "    train_gen.fit(trainX[train])\n",
        "  \n",
        "    print('------------------------------------------------------------------------')\n",
        "    print(f'Training for fold {fold_no} ...')\n",
        "\n",
        "    history = model.train(\n",
        "              train_gen,\n",
        "                trainX[train],\n",
        "                  trainy[train],\n",
        "                trainX[valid],\n",
        "                  trainy[valid],\n",
        "                  lr_rate_type='step')\n",
        "\n",
        "    scores = model.evaluate(trainX[valid], trainy[valid])\n",
        "\n",
        "    print(f'Score for fold {fold_no}: loss of {scores[0]}; accuracy of {scores[1]*100}%')\n",
        "    acc_per_fold.append(scores[1] * 100)\n",
        "    loss_per_fold.append(scores[0])\n",
        "\n",
        "    fold_no+=1\n",
        "\n",
        "# == Provide average scores ==\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Score per fold')\n",
        "for i in range(0, len(acc_per_fold)):\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Average scores for all folds:')\n",
        "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
        "print(f'> Loss: {np.mean(loss_per_fold)}')"
      ],
      "metadata": {
        "id": "ay1hcRXFt-Q1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Random initialiser and no decay"
      ],
      "metadata": {
        "id": "3LRr6ljop4Zv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "acc_per_fold = []\n",
        "loss_per_fold = []\n",
        "\n",
        "with tf.device('/device:GPU:0'):\n",
        "  fold_no = 1\n",
        "  for train, valid in kfold.split(trainX, trainy):\n",
        "\n",
        "    model = CNN_Model(batch_size=32, depth=3, hidden_neurons_1=64, hidden_neurons_2=128, hidden_neurons_3=256, kernel_size_1=3, kernel_size_2=3, kernel_size_3=3, \n",
        "               dropout_rate=0.2, batch_norm=False, pooling='max', layer_activation_1='relu', layer_activation_2='relu', layer_activation_3='relu',\n",
        "               kernel_initialiser=tf.keras.initializers.RandomUniform(), bias_initialiser='zeros', optimiser='Adam', weight_decay=None, learning_rate=1e-3, output_activation='softmax', SGD_momentum=0)\n",
        "  \n",
        "    train_gen = tf.keras.preprocessing.image.ImageDataGenerator(width_shift_range=0.1,\n",
        "      height_shift_range=0.1,\n",
        "      horizontal_flip=True)\n",
        "    train_gen.fit(trainX[train])\n",
        "  \n",
        "    print('------------------------------------------------------------------------')\n",
        "    print(f'Training for fold {fold_no} ...')\n",
        "\n",
        "    history = model.train(\n",
        "              train_gen,\n",
        "                trainX[train],\n",
        "                  trainy[train],\n",
        "                trainX[valid],\n",
        "                  trainy[valid],\n",
        "                  lr_rate_type='step')\n",
        "\n",
        "    scores = model.evaluate(trainX[valid], trainy[valid])\n",
        "\n",
        "    print(f'Score for fold {fold_no}: loss of {scores[0]}; accuracy of {scores[1]*100}%')\n",
        "    acc_per_fold.append(scores[1] * 100)\n",
        "    loss_per_fold.append(scores[0])\n",
        "\n",
        "    fold_no+=1\n",
        "\n",
        "# == Provide average scores ==\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Score per fold')\n",
        "for i in range(0, len(acc_per_fold)):\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Average scores for all folds:')\n",
        "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
        "print(f'> Loss: {np.mean(loss_per_fold)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Iea5vkeKbD90",
        "outputId": "0391728c-d9e2-4bf3-ea9f-ddcfb5d257a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_46\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_138 (Conv2D)         (None, 32, 32, 64)        1792      \n",
            "                                                                 \n",
            " max_pooling2d_46 (MaxPoolin  multiple                 0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_139 (Conv2D)         (None, 16, 16, 128)       73856     \n",
            "                                                                 \n",
            " dropout_46 (Dropout)        (None, 8, 8, 128)         0         \n",
            "                                                                 \n",
            " conv2d_140 (Conv2D)         (None, 8, 8, 256)         295168    \n",
            "                                                                 \n",
            " flatten_46 (Flatten)        (None, 4096)              0         \n",
            "                                                                 \n",
            " dense_46 (Dense)            (None, 10)                40970     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 411,786\n",
            "Trainable params: 411,786\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.6043 - accuracy: 0.4153 - val_loss: 1.2533 - val_accuracy: 0.5548 - lr: 0.0010\n",
            "Epoch 2/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.2270 - accuracy: 0.5651 - val_loss: 1.0568 - val_accuracy: 0.6302 - lr: 0.0010\n",
            "Epoch 3/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.0661 - accuracy: 0.6272 - val_loss: 0.9230 - val_accuracy: 0.6754 - lr: 0.0010\n",
            "Epoch 4/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.9758 - accuracy: 0.6594 - val_loss: 0.9004 - val_accuracy: 0.6863 - lr: 0.0010\n",
            "Epoch 5/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.9173 - accuracy: 0.6819 - val_loss: 0.8890 - val_accuracy: 0.6888 - lr: 0.0010\n",
            "Epoch 6/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.8625 - accuracy: 0.6989 - val_loss: 0.7598 - val_accuracy: 0.7328 - lr: 0.0010\n",
            "Epoch 7/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.8236 - accuracy: 0.7153 - val_loss: 0.7352 - val_accuracy: 0.7480 - lr: 0.0010\n",
            "Epoch 8/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.7825 - accuracy: 0.7290 - val_loss: 0.7597 - val_accuracy: 0.7389 - lr: 0.0010\n",
            "Epoch 9/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.7629 - accuracy: 0.7344 - val_loss: 0.7095 - val_accuracy: 0.7566 - lr: 0.0010\n",
            "Epoch 10/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.7407 - accuracy: 0.7425 - val_loss: 0.6910 - val_accuracy: 0.7631 - lr: 0.0010\n",
            "Epoch 11/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.7257 - accuracy: 0.7478 - val_loss: 0.7312 - val_accuracy: 0.7466 - lr: 0.0010\n",
            "Epoch 12/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.7072 - accuracy: 0.7541 - val_loss: 0.6779 - val_accuracy: 0.7654 - lr: 0.0010\n",
            "Epoch 13/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.6973 - accuracy: 0.7580 - val_loss: 0.7002 - val_accuracy: 0.7573 - lr: 0.0010\n",
            "Epoch 14/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.6739 - accuracy: 0.7655 - val_loss: 0.6740 - val_accuracy: 0.7670 - lr: 0.0010\n",
            "Epoch 15/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.6657 - accuracy: 0.7688 - val_loss: 0.6803 - val_accuracy: 0.7673 - lr: 0.0010\n",
            "Epoch 16/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.6409 - accuracy: 0.7761 - val_loss: 0.6481 - val_accuracy: 0.7760 - lr: 9.0484e-04\n",
            "Epoch 17/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.6235 - accuracy: 0.7822 - val_loss: 0.6397 - val_accuracy: 0.7763 - lr: 8.1873e-04\n",
            "Epoch 18/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.6001 - accuracy: 0.7908 - val_loss: 0.6235 - val_accuracy: 0.7869 - lr: 7.4082e-04\n",
            "Epoch 19/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.5816 - accuracy: 0.7989 - val_loss: 0.6319 - val_accuracy: 0.7884 - lr: 6.7032e-04\n",
            "Epoch 20/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.5637 - accuracy: 0.8048 - val_loss: 0.6076 - val_accuracy: 0.7929 - lr: 6.0653e-04\n",
            "Epoch 21/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.5459 - accuracy: 0.8107 - val_loss: 0.5921 - val_accuracy: 0.7970 - lr: 5.4881e-04\n",
            "Epoch 22/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.5292 - accuracy: 0.8150 - val_loss: 0.6161 - val_accuracy: 0.7915 - lr: 4.9659e-04\n",
            "Epoch 23/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.5152 - accuracy: 0.8196 - val_loss: 0.5893 - val_accuracy: 0.7973 - lr: 4.4933e-04\n",
            "Epoch 24/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 0.5064 - accuracy: 0.8234 - val_loss: 0.5931 - val_accuracy: 0.7939 - lr: 4.0657e-04\n",
            "Epoch 25/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 0.4971 - accuracy: 0.8280 - val_loss: 0.5800 - val_accuracy: 0.8004 - lr: 3.6788e-04\n",
            "Epoch 26/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.4806 - accuracy: 0.8304 - val_loss: 0.5808 - val_accuracy: 0.8067 - lr: 3.3287e-04\n",
            "Epoch 27/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.4800 - accuracy: 0.8333 - val_loss: 0.5648 - val_accuracy: 0.8090 - lr: 3.0119e-04\n",
            "Epoch 28/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.4642 - accuracy: 0.8363 - val_loss: 0.6087 - val_accuracy: 0.8002 - lr: 2.7253e-04\n",
            "Epoch 29/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.4622 - accuracy: 0.8376 - val_loss: 0.5682 - val_accuracy: 0.8098 - lr: 2.4660e-04\n",
            "Epoch 30/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 0.4576 - accuracy: 0.8407 - val_loss: 0.5584 - val_accuracy: 0.8087 - lr: 2.2313e-04\n",
            "Score for fold 1: loss of 0.5583553314208984; accuracy of 80.87000250816345%\n",
            "Model: \"sequential_47\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_141 (Conv2D)         (None, 32, 32, 64)        1792      \n",
            "                                                                 \n",
            " max_pooling2d_47 (MaxPoolin  multiple                 0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_142 (Conv2D)         (None, 16, 16, 128)       73856     \n",
            "                                                                 \n",
            " dropout_47 (Dropout)        (None, 8, 8, 128)         0         \n",
            "                                                                 \n",
            " conv2d_143 (Conv2D)         (None, 8, 8, 256)         295168    \n",
            "                                                                 \n",
            " flatten_47 (Flatten)        (None, 4096)              0         \n",
            "                                                                 \n",
            " dense_47 (Dense)            (None, 10)                40970     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 411,786\n",
            "Trainable params: 411,786\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.5822 - accuracy: 0.4235 - val_loss: 1.2273 - val_accuracy: 0.5667 - lr: 0.0010\n",
            "Epoch 2/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.1898 - accuracy: 0.5823 - val_loss: 0.9706 - val_accuracy: 0.6629 - lr: 0.0010\n",
            "Epoch 3/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.0495 - accuracy: 0.6327 - val_loss: 0.9076 - val_accuracy: 0.6838 - lr: 0.0010\n",
            "Epoch 4/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.9568 - accuracy: 0.6660 - val_loss: 0.8167 - val_accuracy: 0.7178 - lr: 0.0010\n",
            "Epoch 5/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.8937 - accuracy: 0.6887 - val_loss: 0.8287 - val_accuracy: 0.7129 - lr: 0.0010\n",
            "Epoch 6/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.8492 - accuracy: 0.7042 - val_loss: 0.8015 - val_accuracy: 0.7205 - lr: 0.0010\n",
            "Epoch 7/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.8158 - accuracy: 0.7169 - val_loss: 0.7569 - val_accuracy: 0.7392 - lr: 0.0010\n",
            "Epoch 8/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.7796 - accuracy: 0.7285 - val_loss: 0.7999 - val_accuracy: 0.7288 - lr: 0.0010\n",
            "Epoch 9/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.7538 - accuracy: 0.7391 - val_loss: 0.7900 - val_accuracy: 0.7316 - lr: 0.0010\n",
            "Epoch 10/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.7326 - accuracy: 0.7462 - val_loss: 0.6962 - val_accuracy: 0.7585 - lr: 0.0010\n",
            "Epoch 11/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.7059 - accuracy: 0.7547 - val_loss: 0.6895 - val_accuracy: 0.7654 - lr: 0.0010\n",
            "Epoch 12/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.6981 - accuracy: 0.7601 - val_loss: 0.7287 - val_accuracy: 0.7537 - lr: 0.0010\n",
            "Epoch 13/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.6815 - accuracy: 0.7622 - val_loss: 0.6722 - val_accuracy: 0.7755 - lr: 0.0010\n",
            "Epoch 14/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.6631 - accuracy: 0.7704 - val_loss: 0.6622 - val_accuracy: 0.7780 - lr: 0.0010\n",
            "Epoch 15/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.6478 - accuracy: 0.7733 - val_loss: 0.6523 - val_accuracy: 0.7789 - lr: 0.0010\n",
            "Epoch 16/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.6236 - accuracy: 0.7827 - val_loss: 0.6190 - val_accuracy: 0.7890 - lr: 9.0484e-04\n",
            "Epoch 17/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.6028 - accuracy: 0.7897 - val_loss: 0.6391 - val_accuracy: 0.7799 - lr: 8.1873e-04\n",
            "Epoch 18/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.5754 - accuracy: 0.8005 - val_loss: 0.6241 - val_accuracy: 0.7860 - lr: 7.4082e-04\n",
            "Epoch 19/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.5604 - accuracy: 0.8072 - val_loss: 0.6217 - val_accuracy: 0.7949 - lr: 6.7032e-04\n",
            "Epoch 20/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.5334 - accuracy: 0.8148 - val_loss: 0.6088 - val_accuracy: 0.7951 - lr: 6.0653e-04\n",
            "Epoch 21/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.5173 - accuracy: 0.8189 - val_loss: 0.5883 - val_accuracy: 0.8045 - lr: 5.4881e-04\n",
            "Epoch 22/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.5052 - accuracy: 0.8252 - val_loss: 0.5735 - val_accuracy: 0.8082 - lr: 4.9659e-04\n",
            "Epoch 23/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.4865 - accuracy: 0.8297 - val_loss: 0.5945 - val_accuracy: 0.8044 - lr: 4.4933e-04\n",
            "Epoch 24/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.4732 - accuracy: 0.8367 - val_loss: 0.5666 - val_accuracy: 0.8098 - lr: 4.0657e-04\n",
            "Epoch 25/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.4625 - accuracy: 0.8382 - val_loss: 0.5760 - val_accuracy: 0.8083 - lr: 3.6788e-04\n",
            "Epoch 26/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.4541 - accuracy: 0.8421 - val_loss: 0.5873 - val_accuracy: 0.8069 - lr: 3.3287e-04\n",
            "Epoch 27/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.4454 - accuracy: 0.8431 - val_loss: 0.5733 - val_accuracy: 0.8101 - lr: 3.0119e-04\n",
            "Epoch 28/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.4385 - accuracy: 0.8447 - val_loss: 0.5477 - val_accuracy: 0.8179 - lr: 2.7253e-04\n",
            "Epoch 29/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.4264 - accuracy: 0.8505 - val_loss: 0.5562 - val_accuracy: 0.8186 - lr: 2.4660e-04\n",
            "Epoch 30/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.4193 - accuracy: 0.8543 - val_loss: 0.5330 - val_accuracy: 0.8229 - lr: 2.2313e-04\n",
            "Score for fold 2: loss of 0.5329740047454834; accuracy of 82.28999972343445%\n",
            "Model: \"sequential_48\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_144 (Conv2D)         (None, 32, 32, 64)        1792      \n",
            "                                                                 \n",
            " max_pooling2d_48 (MaxPoolin  multiple                 0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_145 (Conv2D)         (None, 16, 16, 128)       73856     \n",
            "                                                                 \n",
            " dropout_48 (Dropout)        (None, 8, 8, 128)         0         \n",
            "                                                                 \n",
            " conv2d_146 (Conv2D)         (None, 8, 8, 256)         295168    \n",
            "                                                                 \n",
            " flatten_48 (Flatten)        (None, 4096)              0         \n",
            "                                                                 \n",
            " dense_48 (Dense)            (None, 10)                40970     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 411,786\n",
            "Trainable params: 411,786\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 1.5803 - accuracy: 0.4300 - val_loss: 1.2201 - val_accuracy: 0.5686 - lr: 0.0010\n",
            "Epoch 2/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.1935 - accuracy: 0.5782 - val_loss: 1.0095 - val_accuracy: 0.6463 - lr: 0.0010\n",
            "Epoch 3/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.0439 - accuracy: 0.6372 - val_loss: 0.9535 - val_accuracy: 0.6753 - lr: 0.0010\n",
            "Epoch 4/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.9472 - accuracy: 0.6683 - val_loss: 0.8443 - val_accuracy: 0.7102 - lr: 0.0010\n",
            "Epoch 5/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.8830 - accuracy: 0.6898 - val_loss: 0.8300 - val_accuracy: 0.7202 - lr: 0.0010\n",
            "Epoch 6/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.8428 - accuracy: 0.7063 - val_loss: 0.8317 - val_accuracy: 0.7131 - lr: 0.0010\n",
            "Epoch 7/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.8069 - accuracy: 0.7186 - val_loss: 0.7792 - val_accuracy: 0.7332 - lr: 0.0010\n",
            "Epoch 8/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.7740 - accuracy: 0.7317 - val_loss: 0.7428 - val_accuracy: 0.7466 - lr: 0.0010\n",
            "Epoch 9/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.7462 - accuracy: 0.7424 - val_loss: 0.7223 - val_accuracy: 0.7536 - lr: 0.0010\n",
            "Epoch 10/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.7283 - accuracy: 0.7485 - val_loss: 0.7598 - val_accuracy: 0.7392 - lr: 0.0010\n",
            "Epoch 11/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.7081 - accuracy: 0.7553 - val_loss: 0.6881 - val_accuracy: 0.7646 - lr: 0.0010\n",
            "Epoch 12/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.6867 - accuracy: 0.7617 - val_loss: 0.7003 - val_accuracy: 0.7612 - lr: 0.0010\n",
            "Epoch 13/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.6689 - accuracy: 0.7692 - val_loss: 0.6878 - val_accuracy: 0.7640 - lr: 0.0010\n",
            "Epoch 14/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.6552 - accuracy: 0.7723 - val_loss: 0.6802 - val_accuracy: 0.7695 - lr: 0.0010\n",
            "Epoch 15/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.6482 - accuracy: 0.7722 - val_loss: 0.6425 - val_accuracy: 0.7812 - lr: 0.0010\n",
            "Epoch 16/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.6195 - accuracy: 0.7861 - val_loss: 0.6746 - val_accuracy: 0.7725 - lr: 9.0484e-04\n",
            "Epoch 17/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.5959 - accuracy: 0.7914 - val_loss: 0.6114 - val_accuracy: 0.7928 - lr: 8.1873e-04\n",
            "Epoch 18/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.5821 - accuracy: 0.7992 - val_loss: 0.5844 - val_accuracy: 0.8014 - lr: 7.4082e-04\n",
            "Epoch 19/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.5532 - accuracy: 0.8084 - val_loss: 0.6002 - val_accuracy: 0.7982 - lr: 6.7032e-04\n",
            "Epoch 20/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.5347 - accuracy: 0.8146 - val_loss: 0.5997 - val_accuracy: 0.7999 - lr: 6.0653e-04\n",
            "Epoch 21/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.5193 - accuracy: 0.8199 - val_loss: 0.5811 - val_accuracy: 0.7996 - lr: 5.4881e-04\n",
            "Epoch 22/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.5079 - accuracy: 0.8232 - val_loss: 0.5805 - val_accuracy: 0.8030 - lr: 4.9659e-04\n",
            "Epoch 23/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.4901 - accuracy: 0.8297 - val_loss: 0.6052 - val_accuracy: 0.8020 - lr: 4.4933e-04\n",
            "Epoch 24/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.4798 - accuracy: 0.8306 - val_loss: 0.5863 - val_accuracy: 0.8007 - lr: 4.0657e-04\n",
            "Epoch 25/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.4715 - accuracy: 0.8359 - val_loss: 0.5645 - val_accuracy: 0.8081 - lr: 3.6788e-04\n",
            "Epoch 26/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.4646 - accuracy: 0.8371 - val_loss: 0.5623 - val_accuracy: 0.8132 - lr: 3.3287e-04\n",
            "Epoch 27/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.4484 - accuracy: 0.8433 - val_loss: 0.5618 - val_accuracy: 0.8126 - lr: 3.0119e-04\n",
            "Epoch 28/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.4410 - accuracy: 0.8462 - val_loss: 0.5465 - val_accuracy: 0.8145 - lr: 2.7253e-04\n",
            "Epoch 29/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.4294 - accuracy: 0.8499 - val_loss: 0.5476 - val_accuracy: 0.8163 - lr: 2.4660e-04\n",
            "Epoch 30/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.4251 - accuracy: 0.8536 - val_loss: 0.5389 - val_accuracy: 0.8216 - lr: 2.2313e-04\n",
            "Score for fold 3: loss of 0.5389429926872253; accuracy of 82.16000199317932%\n",
            "Model: \"sequential_49\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_147 (Conv2D)         (None, 32, 32, 64)        1792      \n",
            "                                                                 \n",
            " max_pooling2d_49 (MaxPoolin  multiple                 0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_148 (Conv2D)         (None, 16, 16, 128)       73856     \n",
            "                                                                 \n",
            " dropout_49 (Dropout)        (None, 8, 8, 128)         0         \n",
            "                                                                 \n",
            " conv2d_149 (Conv2D)         (None, 8, 8, 256)         295168    \n",
            "                                                                 \n",
            " flatten_49 (Flatten)        (None, 4096)              0         \n",
            "                                                                 \n",
            " dense_49 (Dense)            (None, 10)                40970     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 411,786\n",
            "Trainable params: 411,786\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.6444 - accuracy: 0.4006 - val_loss: 1.3382 - val_accuracy: 0.5268 - lr: 0.0010\n",
            "Epoch 2/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.2777 - accuracy: 0.5471 - val_loss: 1.1712 - val_accuracy: 0.5907 - lr: 0.0010\n",
            "Epoch 3/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.1118 - accuracy: 0.6086 - val_loss: 0.9764 - val_accuracy: 0.6629 - lr: 0.0010\n",
            "Epoch 4/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.0001 - accuracy: 0.6465 - val_loss: 0.8864 - val_accuracy: 0.6986 - lr: 0.0010\n",
            "Epoch 5/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.9318 - accuracy: 0.6745 - val_loss: 0.8339 - val_accuracy: 0.7134 - lr: 0.0010\n",
            "Epoch 6/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.8802 - accuracy: 0.6930 - val_loss: 0.7917 - val_accuracy: 0.7304 - lr: 0.0010\n",
            "Epoch 7/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.8442 - accuracy: 0.7062 - val_loss: 0.7668 - val_accuracy: 0.7389 - lr: 0.0010\n",
            "Epoch 8/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 0.8037 - accuracy: 0.7185 - val_loss: 0.7110 - val_accuracy: 0.7587 - lr: 0.0010\n",
            "Epoch 9/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.7806 - accuracy: 0.7262 - val_loss: 0.7137 - val_accuracy: 0.7555 - lr: 0.0010\n",
            "Epoch 10/30\n",
            "  53/1250 [>.............................] - ETA: 20s - loss: 0.7610 - accuracy: 0.7252"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_22076/3855632344.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'Training for fold {fold_no} ...'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m     history = model.train(\n\u001b[0m\u001b[0;32m     21\u001b[0m               \u001b[0mtrain_gen\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m                 \u001b[0mtrainX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_22076/4196491080.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, train_gen, train_x, train_y, test_x, test_y, lr_rate, lr_rate_type)\u001b[0m\n\u001b[0;32m    160\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_gen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtensorboard_callback\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcallback\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    161\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 162\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_gen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtensorboard_callback\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcallback\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m#  self.model.fit(train_x, train_y, validation_data=(test_x, test_y) , epochs=25, verbose=1)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    163\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_gen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtensorboard_callback\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1382\u001b[0m                 _r=1):\n\u001b[0;32m   1383\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1384\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1385\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1386\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    913\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    914\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 915\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    945\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    946\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 947\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    948\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2954\u001b[0m       (graph_function,\n\u001b[0;32m   2955\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2956\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   2957\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   2958\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1851\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1852\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1853\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1854\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1855\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    498\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 499\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    500\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    501\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Random initialiser and 1e-3 weight decay"
      ],
      "metadata": {
        "id": "EOaXR3G0tyU3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "acc_per_fold = []\n",
        "loss_per_fold = []\n",
        "\n",
        "with tf.device('/device:GPU:0'):\n",
        "  fold_no = 1\n",
        "  for train, valid in kfold.split(trainX, trainy):\n",
        "\n",
        "    model = CNN_Model(batch_size=32, depth=3, hidden_neurons_1=64, hidden_neurons_2=128, hidden_neurons_3=256, kernel_size_1=3, kernel_size_2=3, kernel_size_3=3, \n",
        "               dropout_rate=0.2, batch_norm=False, pooling='max', layer_activation_1='relu', layer_activation_2='relu', layer_activation_3='relu',\n",
        "               kernel_initialiser=tf.keras.initializers.RandomUniform(), bias_initialiser='zeros', optimiser='Adam', weight_decay=tf.keras.regularizers.l2(1e-3), learning_rate=1e-3, output_activation='softmax', SGD_momentum=0) \n",
        "  \n",
        "    train_gen = tf.keras.preprocessing.image.ImageDataGenerator(width_shift_range=0.1,\n",
        "      height_shift_range=0.1,\n",
        "      horizontal_flip=True)\n",
        "    train_gen.fit(trainX[train])\n",
        "  \n",
        "    print('------------------------------------------------------------------------')\n",
        "    print(f'Training for fold {fold_no} ...')\n",
        "\n",
        "    history = model.train(\n",
        "              train_gen,\n",
        "                trainX[train],\n",
        "                  trainy[train],\n",
        "                trainX[valid],\n",
        "                  trainy[valid],\n",
        "                  lr_rate_type='step')\n",
        "\n",
        "    scores = model.evaluate(trainX[valid], trainy[valid])\n",
        "\n",
        "    print(f'Score for fold {fold_no}: loss of {scores[0]}; accuracy of {scores[1]*100}%')\n",
        "    acc_per_fold.append(scores[1] * 100)\n",
        "    loss_per_fold.append(scores[0])\n",
        "\n",
        "    fold_no+=1\n",
        "\n",
        "# == Provide average scores ==\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Score per fold')\n",
        "for i in range(0, len(acc_per_fold)):\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Average scores for all folds:')\n",
        "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
        "print(f'> Loss: {np.mean(loss_per_fold)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "rV7NtuDSt-qe",
        "outputId": "a1a2f057-c14f-4ce8-8f2c-6b7df8224eb5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_50\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_150 (Conv2D)         (None, 32, 32, 64)        1792      \n",
            "                                                                 \n",
            " max_pooling2d_50 (MaxPoolin  multiple                 0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_151 (Conv2D)         (None, 16, 16, 128)       73856     \n",
            "                                                                 \n",
            " dropout_50 (Dropout)        (None, 8, 8, 128)         0         \n",
            "                                                                 \n",
            " conv2d_152 (Conv2D)         (None, 8, 8, 256)         295168    \n",
            "                                                                 \n",
            " flatten_50 (Flatten)        (None, 4096)              0         \n",
            "                                                                 \n",
            " dense_50 (Dense)            (None, 10)                40970     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 411,786\n",
            "Trainable params: 411,786\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 1.7426 - accuracy: 0.3847 - val_loss: 1.4689 - val_accuracy: 0.4986 - lr: 0.0010\n",
            "Epoch 2/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.4550 - accuracy: 0.5073 - val_loss: 1.3284 - val_accuracy: 0.5663 - lr: 0.0010\n",
            "Epoch 3/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.3485 - accuracy: 0.5552 - val_loss: 1.1957 - val_accuracy: 0.6164 - lr: 0.0010\n",
            "Epoch 4/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.2664 - accuracy: 0.5896 - val_loss: 1.1790 - val_accuracy: 0.6280 - lr: 0.0010\n",
            "Epoch 5/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.2157 - accuracy: 0.6102 - val_loss: 1.2101 - val_accuracy: 0.6205 - lr: 0.0010\n",
            "Epoch 6/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.1815 - accuracy: 0.6301 - val_loss: 1.0871 - val_accuracy: 0.6710 - lr: 0.0010\n",
            "Epoch 7/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.1397 - accuracy: 0.6471 - val_loss: 1.0405 - val_accuracy: 0.6883 - lr: 0.0010\n",
            "Epoch 8/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.1202 - accuracy: 0.6557 - val_loss: 1.0484 - val_accuracy: 0.6833 - lr: 0.0010\n",
            "Epoch 9/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.1001 - accuracy: 0.6643 - val_loss: 1.0346 - val_accuracy: 0.6933 - lr: 0.0010\n",
            "Epoch 10/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.0769 - accuracy: 0.6727 - val_loss: 1.0223 - val_accuracy: 0.6988 - lr: 0.0010\n",
            "Epoch 11/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.0708 - accuracy: 0.6767 - val_loss: 0.9680 - val_accuracy: 0.7208 - lr: 0.0010\n",
            "Epoch 12/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.0606 - accuracy: 0.6797 - val_loss: 0.9913 - val_accuracy: 0.7159 - lr: 0.0010\n",
            "Epoch 13/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.0447 - accuracy: 0.6878 - val_loss: 1.0066 - val_accuracy: 0.7063 - lr: 0.0010\n",
            "Epoch 14/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.0333 - accuracy: 0.6913 - val_loss: 0.9593 - val_accuracy: 0.7250 - lr: 0.0010\n",
            "Epoch 15/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.0224 - accuracy: 0.6987 - val_loss: 0.9889 - val_accuracy: 0.7182 - lr: 0.0010\n",
            "Epoch 16/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.9991 - accuracy: 0.7052 - val_loss: 0.9800 - val_accuracy: 0.7120 - lr: 9.0484e-04\n",
            "Epoch 17/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.9774 - accuracy: 0.7146 - val_loss: 0.9284 - val_accuracy: 0.7372 - lr: 8.1873e-04\n",
            "Epoch 18/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.9601 - accuracy: 0.7189 - val_loss: 0.9247 - val_accuracy: 0.7345 - lr: 7.4082e-04\n",
            "Epoch 19/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.9371 - accuracy: 0.7260 - val_loss: 0.9065 - val_accuracy: 0.7435 - lr: 6.7032e-04\n",
            "Epoch 20/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.9171 - accuracy: 0.7348 - val_loss: 0.8560 - val_accuracy: 0.7592 - lr: 6.0653e-04\n",
            "Epoch 21/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.8966 - accuracy: 0.7379 - val_loss: 0.8299 - val_accuracy: 0.7686 - lr: 5.4881e-04\n",
            "Epoch 22/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.8865 - accuracy: 0.7431 - val_loss: 0.8487 - val_accuracy: 0.7568 - lr: 4.9659e-04\n",
            "Epoch 23/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.8686 - accuracy: 0.7456 - val_loss: 0.8183 - val_accuracy: 0.7699 - lr: 4.4933e-04\n",
            "Epoch 24/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.8584 - accuracy: 0.7503 - val_loss: 0.8113 - val_accuracy: 0.7707 - lr: 4.0657e-04\n",
            "Epoch 25/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.8466 - accuracy: 0.7541 - val_loss: 0.8201 - val_accuracy: 0.7679 - lr: 3.6788e-04\n",
            "Epoch 26/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.8326 - accuracy: 0.7583 - val_loss: 0.8337 - val_accuracy: 0.7589 - lr: 3.3287e-04\n",
            "Epoch 27/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.8217 - accuracy: 0.7603 - val_loss: 0.8138 - val_accuracy: 0.7671 - lr: 3.0119e-04\n",
            "Epoch 28/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.8158 - accuracy: 0.7624 - val_loss: 0.7697 - val_accuracy: 0.7809 - lr: 2.7253e-04\n",
            "Epoch 29/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.8007 - accuracy: 0.7685 - val_loss: 0.7883 - val_accuracy: 0.7729 - lr: 2.4660e-04\n",
            "Epoch 30/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.7993 - accuracy: 0.7670 - val_loss: 0.7940 - val_accuracy: 0.7744 - lr: 2.2313e-04\n",
            "Score for fold 1: loss of 0.7940229177474976; accuracy of 77.4399995803833%\n",
            "Model: \"sequential_51\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_153 (Conv2D)         (None, 32, 32, 64)        1792      \n",
            "                                                                 \n",
            " max_pooling2d_51 (MaxPoolin  multiple                 0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_154 (Conv2D)         (None, 16, 16, 128)       73856     \n",
            "                                                                 \n",
            " dropout_51 (Dropout)        (None, 8, 8, 128)         0         \n",
            "                                                                 \n",
            " conv2d_155 (Conv2D)         (None, 8, 8, 256)         295168    \n",
            "                                                                 \n",
            " flatten_51 (Flatten)        (None, 4096)              0         \n",
            "                                                                 \n",
            " dense_51 (Dense)            (None, 10)                40970     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 411,786\n",
            "Trainable params: 411,786\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 1.7388 - accuracy: 0.3878 - val_loss: 1.4824 - val_accuracy: 0.4851 - lr: 0.0010\n",
            "Epoch 2/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.4464 - accuracy: 0.5138 - val_loss: 1.3297 - val_accuracy: 0.5602 - lr: 0.0010\n",
            "Epoch 3/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.3250 - accuracy: 0.5720 - val_loss: 1.2419 - val_accuracy: 0.5975 - lr: 0.0010\n",
            "Epoch 4/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.2459 - accuracy: 0.6052 - val_loss: 1.1714 - val_accuracy: 0.6328 - lr: 0.0010\n",
            "Epoch 5/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.1948 - accuracy: 0.6263 - val_loss: 1.1579 - val_accuracy: 0.6424 - lr: 0.0010\n",
            "Epoch 6/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.1544 - accuracy: 0.6446 - val_loss: 1.0450 - val_accuracy: 0.6842 - lr: 0.0010\n",
            "Epoch 7/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 1.1206 - accuracy: 0.6609 - val_loss: 0.9926 - val_accuracy: 0.7028 - lr: 0.0010\n",
            "Epoch 8/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.0927 - accuracy: 0.6717 - val_loss: 1.0432 - val_accuracy: 0.6881 - lr: 0.0010\n",
            "Epoch 9/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.0805 - accuracy: 0.6797 - val_loss: 0.9861 - val_accuracy: 0.7109 - lr: 0.0010\n",
            "Epoch 10/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.0653 - accuracy: 0.6845 - val_loss: 1.0010 - val_accuracy: 0.7137 - lr: 0.0010\n",
            "Epoch 11/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.0506 - accuracy: 0.6916 - val_loss: 0.9481 - val_accuracy: 0.7302 - lr: 0.0010\n",
            "Epoch 12/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 1.0365 - accuracy: 0.6966 - val_loss: 0.9319 - val_accuracy: 0.7330 - lr: 0.0010\n",
            "Epoch 13/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 1.0256 - accuracy: 0.7023 - val_loss: 1.0368 - val_accuracy: 0.7000 - lr: 0.0010\n",
            "Epoch 14/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 1.0164 - accuracy: 0.7060 - val_loss: 0.9099 - val_accuracy: 0.7442 - lr: 0.0010\n",
            "Epoch 15/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.0014 - accuracy: 0.7100 - val_loss: 0.9767 - val_accuracy: 0.7191 - lr: 0.0010\n",
            "Epoch 16/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.9857 - accuracy: 0.7178 - val_loss: 0.9088 - val_accuracy: 0.7418 - lr: 9.0484e-04\n",
            "Epoch 17/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.9571 - accuracy: 0.7292 - val_loss: 0.9006 - val_accuracy: 0.7500 - lr: 8.1873e-04\n",
            "Epoch 18/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.9405 - accuracy: 0.7314 - val_loss: 0.8965 - val_accuracy: 0.7488 - lr: 7.4082e-04\n",
            "Epoch 19/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.9252 - accuracy: 0.7327 - val_loss: 0.8465 - val_accuracy: 0.7610 - lr: 6.7032e-04\n",
            "Epoch 20/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.9078 - accuracy: 0.7411 - val_loss: 0.8376 - val_accuracy: 0.7694 - lr: 6.0653e-04\n",
            "Epoch 21/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.8879 - accuracy: 0.7475 - val_loss: 0.8312 - val_accuracy: 0.7667 - lr: 5.4881e-04\n",
            "Epoch 22/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.8675 - accuracy: 0.7541 - val_loss: 0.8293 - val_accuracy: 0.7654 - lr: 4.9659e-04\n",
            "Epoch 23/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.8578 - accuracy: 0.7553 - val_loss: 0.8018 - val_accuracy: 0.7742 - lr: 4.4933e-04\n",
            "Epoch 24/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.8433 - accuracy: 0.7613 - val_loss: 0.7990 - val_accuracy: 0.7754 - lr: 4.0657e-04\n",
            "Epoch 25/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.8317 - accuracy: 0.7659 - val_loss: 0.8110 - val_accuracy: 0.7726 - lr: 3.6788e-04\n",
            "Epoch 26/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.8160 - accuracy: 0.7713 - val_loss: 0.7924 - val_accuracy: 0.7737 - lr: 3.3287e-04\n",
            "Epoch 27/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.8056 - accuracy: 0.7735 - val_loss: 0.7859 - val_accuracy: 0.7757 - lr: 3.0119e-04\n",
            "Epoch 28/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.7970 - accuracy: 0.7774 - val_loss: 0.7884 - val_accuracy: 0.7740 - lr: 2.7253e-04\n",
            "Epoch 29/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.7898 - accuracy: 0.7782 - val_loss: 0.8257 - val_accuracy: 0.7617 - lr: 2.4660e-04\n",
            "Epoch 30/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.7765 - accuracy: 0.7810 - val_loss: 0.7467 - val_accuracy: 0.7901 - lr: 2.2313e-04\n",
            "Score for fold 2: loss of 0.7466574907302856; accuracy of 79.00999784469604%\n",
            "Model: \"sequential_52\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_156 (Conv2D)         (None, 32, 32, 64)        1792      \n",
            "                                                                 \n",
            " max_pooling2d_52 (MaxPoolin  multiple                 0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_157 (Conv2D)         (None, 16, 16, 128)       73856     \n",
            "                                                                 \n",
            " dropout_52 (Dropout)        (None, 8, 8, 128)         0         \n",
            "                                                                 \n",
            " conv2d_158 (Conv2D)         (None, 8, 8, 256)         295168    \n",
            "                                                                 \n",
            " flatten_52 (Flatten)        (None, 4096)              0         \n",
            "                                                                 \n",
            " dense_52 (Dense)            (None, 10)                40970     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 411,786\n",
            "Trainable params: 411,786\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.7292 - accuracy: 0.3932 - val_loss: 1.4564 - val_accuracy: 0.5103 - lr: 0.0010\n",
            "Epoch 2/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.4374 - accuracy: 0.5181 - val_loss: 1.3168 - val_accuracy: 0.5682 - lr: 0.0010\n",
            "Epoch 3/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.3193 - accuracy: 0.5753 - val_loss: 1.2387 - val_accuracy: 0.6141 - lr: 0.0010\n",
            "Epoch 4/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.2366 - accuracy: 0.6114 - val_loss: 1.1495 - val_accuracy: 0.6511 - lr: 0.0010\n",
            "Epoch 5/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 1.1902 - accuracy: 0.6309 - val_loss: 1.0826 - val_accuracy: 0.6746 - lr: 0.0010\n",
            "Epoch 6/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.1567 - accuracy: 0.6476 - val_loss: 1.0255 - val_accuracy: 0.7000 - lr: 0.0010\n",
            "Epoch 7/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 1.1195 - accuracy: 0.6671 - val_loss: 1.0018 - val_accuracy: 0.7154 - lr: 0.0010\n",
            "Epoch 8/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.0947 - accuracy: 0.6733 - val_loss: 0.9868 - val_accuracy: 0.7226 - lr: 0.0010\n",
            "Epoch 9/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.0750 - accuracy: 0.6823 - val_loss: 0.9551 - val_accuracy: 0.7339 - lr: 0.0010\n",
            "Epoch 10/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 1.0681 - accuracy: 0.6863 - val_loss: 1.0276 - val_accuracy: 0.7108 - lr: 0.0010\n",
            "Epoch 11/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 1.0482 - accuracy: 0.6981 - val_loss: 1.0062 - val_accuracy: 0.7150 - lr: 0.0010\n",
            "Epoch 12/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 1.0332 - accuracy: 0.6996 - val_loss: 1.0135 - val_accuracy: 0.7149 - lr: 0.0010\n",
            "Epoch 13/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 1.0203 - accuracy: 0.7056 - val_loss: 0.9738 - val_accuracy: 0.7333 - lr: 0.0010\n",
            "Epoch 14/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.0142 - accuracy: 0.7100 - val_loss: 0.9101 - val_accuracy: 0.7480 - lr: 0.0010\n",
            "Epoch 15/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.0026 - accuracy: 0.7146 - val_loss: 0.9214 - val_accuracy: 0.7514 - lr: 0.0010\n",
            "Epoch 16/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.9768 - accuracy: 0.7244 - val_loss: 0.9694 - val_accuracy: 0.7338 - lr: 9.0484e-04\n",
            "Epoch 17/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.9580 - accuracy: 0.7276 - val_loss: 0.9353 - val_accuracy: 0.7415 - lr: 8.1873e-04\n",
            "Epoch 18/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.9334 - accuracy: 0.7369 - val_loss: 0.8851 - val_accuracy: 0.7584 - lr: 7.4082e-04\n",
            "Epoch 19/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.9155 - accuracy: 0.7413 - val_loss: 0.8340 - val_accuracy: 0.7719 - lr: 6.7032e-04\n",
            "Epoch 20/30\n",
            "1239/1250 [============================>.] - ETA: 0s - loss: 0.8929 - accuracy: 0.7484"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_22076/281089879.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'Training for fold {fold_no} ...'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m     history = model.train(\n\u001b[0m\u001b[0;32m     21\u001b[0m               \u001b[0mtrain_gen\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m                 \u001b[0mtrainX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_22076/4196491080.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, train_gen, train_x, train_y, test_x, test_y, lr_rate, lr_rate_type)\u001b[0m\n\u001b[0;32m    160\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_gen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtensorboard_callback\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcallback\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    161\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 162\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_gen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtensorboard_callback\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcallback\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m#  self.model.fit(train_x, train_y, validation_data=(test_x, test_y) , epochs=25, verbose=1)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    163\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_gen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtensorboard_callback\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1382\u001b[0m                 _r=1):\n\u001b[0;32m   1383\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1384\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1385\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1386\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    913\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    914\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 915\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    945\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    946\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 947\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    948\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2954\u001b[0m       (graph_function,\n\u001b[0;32m   2955\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2956\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   2957\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   2958\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1851\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1852\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1853\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1854\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1855\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    498\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 499\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    500\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    501\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Random initialiser and 1e-5 weight decay"
      ],
      "metadata": {
        "id": "x55QI_fAt4Mg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "acc_per_fold = []\n",
        "loss_per_fold = []\n",
        "\n",
        "with tf.device('/device:GPU:0'):\n",
        "  fold_no = 1\n",
        "  for train, valid in kfold.split(trainX, trainy):\n",
        "\n",
        "    model = CNN_Model(batch_size=32, depth=3, hidden_neurons_1=64, hidden_neurons_2=128, hidden_neurons_3=256, kernel_size_1=3, kernel_size_2=3, kernel_size_3=3, \n",
        "               dropout_rate=0.2, batch_norm=False, pooling='max', layer_activation_1='relu', layer_activation_2='relu', layer_activation_3='relu',\n",
        "               kernel_initialiser=tf.keras.initializers.RandomUniform(), bias_initialiser='zeros', optimiser='Adam', weight_decay=tf.keras.regularizers.l2(1e-5), learning_rate=1e-3, output_activation='softmax', SGD_momentum=0) \n",
        "  \n",
        "    train_gen = tf.keras.preprocessing.image.ImageDataGenerator(width_shift_range=0.1,\n",
        "      height_shift_range=0.1,\n",
        "      horizontal_flip=True)\n",
        "    train_gen.fit(trainX[train]) #Data augmentation\n",
        "  \n",
        "    print('------------------------------------------------------------------------')\n",
        "    print(f'Training for fold {fold_no} ...')\n",
        "\n",
        "    history = model.train(\n",
        "              train_gen,\n",
        "                trainX[train],\n",
        "                  trainy[train],\n",
        "                trainX[valid],\n",
        "                  trainy[valid],\n",
        "                  lr_rate_type='step')\n",
        "\n",
        "    scores = model.evaluate(trainX[valid], trainy[valid])\n",
        "\n",
        "    print(f'Score for fold {fold_no}: loss of {scores[0]}; accuracy of {scores[1]*100}%')\n",
        "    acc_per_fold.append(scores[1] * 100)\n",
        "    loss_per_fold.append(scores[0])\n",
        "\n",
        "    fold_no+=1\n",
        "\n",
        "# == Provide average scores ==\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Score per fold')\n",
        "for i in range(0, len(acc_per_fold)):\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Average scores for all folds:')\n",
        "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
        "print(f'> Loss: {np.mean(loss_per_fold)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "DsOBwChVt_WX",
        "outputId": "91edca71-6334-4154-f637-c61945037531"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_53\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_159 (Conv2D)         (None, 32, 32, 64)        1792      \n",
            "                                                                 \n",
            " max_pooling2d_53 (MaxPoolin  multiple                 0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_160 (Conv2D)         (None, 16, 16, 128)       73856     \n",
            "                                                                 \n",
            " dropout_53 (Dropout)        (None, 8, 8, 128)         0         \n",
            "                                                                 \n",
            " conv2d_161 (Conv2D)         (None, 8, 8, 256)         295168    \n",
            "                                                                 \n",
            " flatten_53 (Flatten)        (None, 4096)              0         \n",
            "                                                                 \n",
            " dense_53 (Dense)            (None, 10)                40970     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 411,786\n",
            "Trainable params: 411,786\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.6142 - accuracy: 0.4151 - val_loss: 1.2362 - val_accuracy: 0.5577 - lr: 0.0010\n",
            "Epoch 2/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 1.2231 - accuracy: 0.5696 - val_loss: 1.0398 - val_accuracy: 0.6383 - lr: 0.0010\n",
            "Epoch 3/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.0618 - accuracy: 0.6299 - val_loss: 0.9295 - val_accuracy: 0.6779 - lr: 0.0010\n",
            "Epoch 4/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.9825 - accuracy: 0.6604 - val_loss: 0.8765 - val_accuracy: 0.7025 - lr: 0.0010\n",
            "Epoch 5/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.9178 - accuracy: 0.6844 - val_loss: 0.8726 - val_accuracy: 0.7044 - lr: 0.0010\n",
            "Epoch 6/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.8762 - accuracy: 0.7009 - val_loss: 0.8306 - val_accuracy: 0.7204 - lr: 0.0010\n",
            "Epoch 7/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.8397 - accuracy: 0.7156 - val_loss: 0.7573 - val_accuracy: 0.7476 - lr: 0.0010\n",
            "Epoch 8/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.8069 - accuracy: 0.7272 - val_loss: 0.7565 - val_accuracy: 0.7441 - lr: 0.0010\n",
            "Epoch 9/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 0.7883 - accuracy: 0.7334 - val_loss: 0.8045 - val_accuracy: 0.7352 - lr: 0.0010\n",
            "Epoch 10/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.7624 - accuracy: 0.7426 - val_loss: 0.7075 - val_accuracy: 0.7639 - lr: 0.0010\n",
            "Epoch 11/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.7476 - accuracy: 0.7484 - val_loss: 0.7018 - val_accuracy: 0.7659 - lr: 0.0010\n",
            "Epoch 12/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.7325 - accuracy: 0.7528 - val_loss: 0.7086 - val_accuracy: 0.7701 - lr: 0.0010\n",
            "Epoch 13/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.7146 - accuracy: 0.7617 - val_loss: 0.7180 - val_accuracy: 0.7695 - lr: 0.0010\n",
            "Epoch 14/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.7008 - accuracy: 0.7689 - val_loss: 0.6914 - val_accuracy: 0.7780 - lr: 0.0010\n",
            "Epoch 15/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.6956 - accuracy: 0.7675 - val_loss: 0.7049 - val_accuracy: 0.7737 - lr: 0.0010\n",
            "Epoch 16/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.6662 - accuracy: 0.7800 - val_loss: 0.6843 - val_accuracy: 0.7783 - lr: 9.0484e-04\n",
            "Epoch 17/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.6422 - accuracy: 0.7892 - val_loss: 0.6682 - val_accuracy: 0.7812 - lr: 8.1873e-04\n",
            "Epoch 18/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.6270 - accuracy: 0.7909 - val_loss: 0.6373 - val_accuracy: 0.7973 - lr: 7.4082e-04\n",
            "Epoch 19/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.6085 - accuracy: 0.7982 - val_loss: 0.6278 - val_accuracy: 0.7939 - lr: 6.7032e-04\n",
            "Epoch 20/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.5864 - accuracy: 0.8088 - val_loss: 0.6183 - val_accuracy: 0.8050 - lr: 6.0653e-04\n",
            "Epoch 21/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.5727 - accuracy: 0.8116 - val_loss: 0.6441 - val_accuracy: 0.7967 - lr: 5.4881e-04\n",
            "Epoch 22/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 0.5589 - accuracy: 0.8176 - val_loss: 0.6109 - val_accuracy: 0.8051 - lr: 4.9659e-04\n",
            "Epoch 23/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 0.5394 - accuracy: 0.8238 - val_loss: 0.5846 - val_accuracy: 0.8153 - lr: 4.4933e-04\n",
            "Epoch 24/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.5282 - accuracy: 0.8271 - val_loss: 0.5771 - val_accuracy: 0.8164 - lr: 4.0657e-04\n",
            "Epoch 25/30\n",
            " 875/1250 [====================>.........] - ETA: 4s - loss: 0.5158 - accuracy: 0.8351"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_22076/208824732.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'Training for fold {fold_no} ...'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m     history = model.train(\n\u001b[0m\u001b[0;32m     21\u001b[0m               \u001b[0mtrain_gen\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m                 \u001b[0mtrainX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_22076/4196491080.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, train_gen, train_x, train_y, test_x, test_y, lr_rate, lr_rate_type)\u001b[0m\n\u001b[0;32m    160\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_gen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtensorboard_callback\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcallback\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    161\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 162\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_gen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtensorboard_callback\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcallback\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m#  self.model.fit(train_x, train_y, validation_data=(test_x, test_y) , epochs=25, verbose=1)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    163\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_gen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtensorboard_callback\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1382\u001b[0m                 _r=1):\n\u001b[0;32m   1383\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1384\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1385\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1386\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    913\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    914\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 915\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    945\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    946\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 947\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    948\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2954\u001b[0m       (graph_function,\n\u001b[0;32m   2955\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2956\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   2957\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   2958\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1851\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1852\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1853\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1854\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1855\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    498\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 499\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    500\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    501\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optimisers"
      ],
      "metadata": {
        "id": "4b-MzpW0ZSCN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### SGD: Learning rate 1e-2 Momentum 0.3"
      ],
      "metadata": {
        "id": "n5-2HmfA6KVP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "acc_per_fold = []\n",
        "loss_per_fold = []\n",
        "\n",
        "with tf.device('/device:GPU:0'):\n",
        "  fold_no = 1\n",
        "  for train, valid in kfold.split(trainX, trainy):\n",
        "\n",
        "    model = CNN_Model(batch_size=32, depth=3, hidden_neurons_1=64, hidden_neurons_2=128, hidden_neurons_3=256, kernel_size_1=3, kernel_size_2=3, kernel_size_3=3, \n",
        "               dropout_rate=0.2, batch_norm=False, pooling='max', layer_activation_1='relu', layer_activation_2='relu', layer_activation_3='relu',\n",
        "               kernel_initialiser=tf.keras.initializers.HeNormal(), bias_initialiser='zeros', optimiser='SGD', weight_decay=None, learning_rate=1e-2, output_activation='softmax', SGD_momentum=0.3) \n",
        "  \n",
        "    train_gen = tf.keras.preprocessing.image.ImageDataGenerator(width_shift_range=0.1,\n",
        "      height_shift_range=0.1,\n",
        "      horizontal_flip=True)\n",
        "    train_gen.fit(trainX[train]) #Data augmentation\n",
        "  \n",
        "    print('------------------------------------------------------------------------')\n",
        "    print(f'Training for fold {fold_no} ...')\n",
        "\n",
        "    history = model.train(\n",
        "              train_gen,\n",
        "                trainX[train],\n",
        "                  trainy[train],\n",
        "                trainX[valid],\n",
        "                  trainy[valid],\n",
        "                  lr_rate_type='step')\n",
        "\n",
        "    scores = model.evaluate(trainX[valid], trainy[valid])\n",
        "\n",
        "    print(f'Score for fold {fold_no}: loss of {scores[0]}; accuracy of {scores[1]*100}%')\n",
        "    acc_per_fold.append(scores[1] * 100)\n",
        "    loss_per_fold.append(scores[0])\n",
        "\n",
        "    fold_no+=1\n",
        "\n",
        "# == Provide average scores ==\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Score per fold')\n",
        "for i in range(0, len(acc_per_fold)):\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Average scores for all folds:')\n",
        "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
        "print(f'> Loss: {np.mean(loss_per_fold)}')"
      ],
      "metadata": {
        "id": "rNctP8i9ZaEe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6520fea6-c371-40c1-cbe6-69797641ee81"
      },
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_24\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_72 (Conv2D)          (None, 32, 32, 64)        1792      \n",
            "                                                                 \n",
            " max_pooling2d_23 (MaxPoolin  multiple                 0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_73 (Conv2D)          (None, 16, 16, 128)       73856     \n",
            "                                                                 \n",
            " dropout_24 (Dropout)        (None, 8, 8, 128)         0         \n",
            "                                                                 \n",
            " conv2d_74 (Conv2D)          (None, 8, 8, 256)         295168    \n",
            "                                                                 \n",
            " flatten_24 (Flatten)        (None, 4096)              0         \n",
            "                                                                 \n",
            " dense_24 (Dense)            (None, 10)                40970     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 411,786\n",
            "Trainable params: 411,786\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.7506 - accuracy: 0.3708 - val_loss: 1.4751 - val_accuracy: 0.4757 - lr: 0.0100\n",
            "Epoch 2/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.4577 - accuracy: 0.4803 - val_loss: 1.3932 - val_accuracy: 0.5034 - lr: 0.0100\n",
            "Epoch 3/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.3287 - accuracy: 0.5281 - val_loss: 1.1742 - val_accuracy: 0.5900 - lr: 0.0100\n",
            "Epoch 4/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.2405 - accuracy: 0.5643 - val_loss: 1.1937 - val_accuracy: 0.5897 - lr: 0.0100\n",
            "Epoch 5/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.1711 - accuracy: 0.5897 - val_loss: 1.0722 - val_accuracy: 0.6296 - lr: 0.0100\n",
            "Epoch 6/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.1173 - accuracy: 0.6068 - val_loss: 1.0685 - val_accuracy: 0.6348 - lr: 0.0100\n",
            "Epoch 7/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.0689 - accuracy: 0.6297 - val_loss: 0.9865 - val_accuracy: 0.6627 - lr: 0.0100\n",
            "Epoch 8/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.0230 - accuracy: 0.6435 - val_loss: 0.9388 - val_accuracy: 0.6812 - lr: 0.0100\n",
            "Epoch 9/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.9831 - accuracy: 0.6597 - val_loss: 0.9528 - val_accuracy: 0.6760 - lr: 0.0100\n",
            "Epoch 10/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.9547 - accuracy: 0.6695 - val_loss: 0.8988 - val_accuracy: 0.6944 - lr: 0.0100\n",
            "Epoch 11/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.9032 - accuracy: 0.6882 - val_loss: 0.8701 - val_accuracy: 0.7023 - lr: 0.0050\n",
            "Epoch 12/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.8846 - accuracy: 0.6962 - val_loss: 0.8401 - val_accuracy: 0.7149 - lr: 0.0050\n",
            "Epoch 13/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.8707 - accuracy: 0.6989 - val_loss: 0.8213 - val_accuracy: 0.7226 - lr: 0.0050\n",
            "Epoch 14/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.8547 - accuracy: 0.7069 - val_loss: 0.8384 - val_accuracy: 0.7167 - lr: 0.0050\n",
            "Epoch 15/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.8469 - accuracy: 0.7084 - val_loss: 0.8191 - val_accuracy: 0.7207 - lr: 0.0050\n",
            "Epoch 16/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 0.8378 - accuracy: 0.7114 - val_loss: 0.8067 - val_accuracy: 0.7209 - lr: 0.0050\n",
            "Epoch 17/30\n",
            " 339/1250 [=======>......................] - ETA: 12s - loss: 0.8260 - accuracy: 0.7220"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17312/1693749560.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'Training for fold {fold_no} ...'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m     history = model.train(\n\u001b[0m\u001b[0;32m     21\u001b[0m               \u001b[0mtrain_gen\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m                 \u001b[0mtrainX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17312/3965214673.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, train_gen, train_x, train_y, test_x, test_y, lr_rate, lr_rate_type)\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_gen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtensorboard_callback\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcallback\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 165\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_gen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtensorboard_callback\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcallback\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m#  self.model.fit(train_x, train_y, validation_data=(test_x, test_y) , epochs=25, verbose=1)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    166\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_gen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtensorboard_callback\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1382\u001b[0m                 _r=1):\n\u001b[0;32m   1383\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1384\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1385\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1386\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    913\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    914\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 915\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    945\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    946\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 947\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    948\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2954\u001b[0m       (graph_function,\n\u001b[0;32m   2955\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2956\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   2957\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   2958\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1851\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1852\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1853\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1854\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1855\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    498\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 499\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    500\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    501\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### SGD: Learning rate 1e-2 Momentum 0.6"
      ],
      "metadata": {
        "id": "9cnCfMkv6deH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "acc_per_fold = []\n",
        "loss_per_fold = []\n",
        "\n",
        "with tf.device('/device:GPU:0'):\n",
        "  fold_no = 1\n",
        "  for train, valid in kfold.split(trainX, trainy):\n",
        "\n",
        "    model = CNN_Model(batch_size=32, depth=3, hidden_neurons_1=64, hidden_neurons_2=128, hidden_neurons_3=256, kernel_size_1=3, kernel_size_2=3, kernel_size_3=3, \n",
        "               dropout_rate=0.2, batch_norm=False, pooling='max', layer_activation_1='relu', layer_activation_2='relu', layer_activation_3='relu',\n",
        "               kernel_initialiser=tf.keras.initializers.HeNormal(), bias_initialiser='zeros', optimiser='SGD', weight_decay=None, learning_rate=1e-2, output_activation='softmax', SGD_momentum=0.6) \n",
        "  \n",
        "    train_gen = tf.keras.preprocessing.image.ImageDataGenerator(width_shift_range=0.1,\n",
        "      height_shift_range=0.1,\n",
        "      horizontal_flip=True)\n",
        "    train_gen.fit(trainX[train]) #Data augmentation\n",
        "  \n",
        "    print('------------------------------------------------------------------------')\n",
        "    print(f'Training for fold {fold_no} ...')\n",
        "\n",
        "    history = model.train(\n",
        "              train_gen,\n",
        "                trainX[train],\n",
        "                  trainy[train],\n",
        "                trainX[valid],\n",
        "                  trainy[valid],\n",
        "                  lr_rate_type='step')\n",
        "\n",
        "    scores = model.evaluate(trainX[valid], trainy[valid])\n",
        "\n",
        "    print(f'Score for fold {fold_no}: loss of {scores[0]}; accuracy of {scores[1]*100}%')\n",
        "    acc_per_fold.append(scores[1] * 100)\n",
        "    loss_per_fold.append(scores[0])\n",
        "\n",
        "    fold_no+=1\n",
        "\n",
        "# == Provide average scores ==\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Score per fold')\n",
        "for i in range(0, len(acc_per_fold)):\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Average scores for all folds:')\n",
        "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
        "print(f'> Loss: {np.mean(loss_per_fold)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "JVhARilf6ebJ",
        "outputId": "b5429d97-8699-47f2-82d1-958d70e09a87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_58\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_174 (Conv2D)         (None, 32, 32, 64)        1792      \n",
            "                                                                 \n",
            " max_pooling2d_58 (MaxPoolin  multiple                 0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_175 (Conv2D)         (None, 16, 16, 128)       73856     \n",
            "                                                                 \n",
            " dropout_58 (Dropout)        (None, 8, 8, 128)         0         \n",
            "                                                                 \n",
            " conv2d_176 (Conv2D)         (None, 8, 8, 256)         295168    \n",
            "                                                                 \n",
            " flatten_58 (Flatten)        (None, 4096)              0         \n",
            "                                                                 \n",
            " dense_58 (Dense)            (None, 10)                40970     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 411,786\n",
            "Trainable params: 411,786\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 1.7235 - accuracy: 0.3807 - val_loss: 1.4179 - val_accuracy: 0.5044 - lr: 0.0100\n",
            "Epoch 2/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.3779 - accuracy: 0.5120 - val_loss: 1.2352 - val_accuracy: 0.5656 - lr: 0.0100\n",
            "Epoch 3/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.2408 - accuracy: 0.5623 - val_loss: 1.1067 - val_accuracy: 0.6143 - lr: 0.0100\n",
            "Epoch 4/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.1392 - accuracy: 0.6022 - val_loss: 1.0172 - val_accuracy: 0.6480 - lr: 0.0100\n",
            "Epoch 5/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.0739 - accuracy: 0.6241 - val_loss: 1.0116 - val_accuracy: 0.6567 - lr: 0.0100\n",
            "Epoch 6/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.0202 - accuracy: 0.6438 - val_loss: 0.9568 - val_accuracy: 0.6744 - lr: 0.0100\n",
            "Epoch 7/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.9718 - accuracy: 0.6608 - val_loss: 0.8380 - val_accuracy: 0.7135 - lr: 0.0100\n",
            "Epoch 8/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.9381 - accuracy: 0.6717 - val_loss: 0.8678 - val_accuracy: 0.6921 - lr: 0.0100\n",
            "Epoch 9/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.9046 - accuracy: 0.6853 - val_loss: 0.8099 - val_accuracy: 0.7242 - lr: 0.0100\n",
            "Epoch 10/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.8708 - accuracy: 0.6998 - val_loss: 0.8143 - val_accuracy: 0.7197 - lr: 0.0100\n",
            "Epoch 11/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.8499 - accuracy: 0.7061 - val_loss: 0.7684 - val_accuracy: 0.7371 - lr: 0.0100\n",
            "Epoch 12/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.8251 - accuracy: 0.7114 - val_loss: 0.7406 - val_accuracy: 0.7496 - lr: 0.0100\n",
            "Epoch 13/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.8024 - accuracy: 0.7215 - val_loss: 0.7600 - val_accuracy: 0.7385 - lr: 0.0100\n",
            "Epoch 14/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.7907 - accuracy: 0.7267 - val_loss: 0.7260 - val_accuracy: 0.7527 - lr: 0.0100\n",
            "Epoch 15/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.7688 - accuracy: 0.7347 - val_loss: 0.7256 - val_accuracy: 0.7535 - lr: 0.0100\n",
            "Epoch 16/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.7405 - accuracy: 0.7416 - val_loss: 0.7260 - val_accuracy: 0.7533 - lr: 0.0090\n",
            "Epoch 17/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.7195 - accuracy: 0.7509 - val_loss: 0.6725 - val_accuracy: 0.7727 - lr: 0.0082\n",
            "Epoch 18/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.7041 - accuracy: 0.7547 - val_loss: 0.6683 - val_accuracy: 0.7723 - lr: 0.0074\n",
            "Epoch 19/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.6848 - accuracy: 0.7644 - val_loss: 0.6850 - val_accuracy: 0.7638 - lr: 0.0067\n",
            "Epoch 20/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.6687 - accuracy: 0.7692 - val_loss: 0.6745 - val_accuracy: 0.7647 - lr: 0.0061\n",
            "Epoch 21/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.6555 - accuracy: 0.7744 - val_loss: 0.6718 - val_accuracy: 0.7679 - lr: 0.0055\n",
            "Epoch 22/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.6383 - accuracy: 0.7786 - val_loss: 0.6355 - val_accuracy: 0.7802 - lr: 0.0050\n",
            "Epoch 23/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.6263 - accuracy: 0.7821 - val_loss: 0.6197 - val_accuracy: 0.7904 - lr: 0.0045\n",
            "Epoch 24/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.6175 - accuracy: 0.7876 - val_loss: 0.6179 - val_accuracy: 0.7925 - lr: 0.0041\n",
            "Epoch 25/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 0.6111 - accuracy: 0.7880 - val_loss: 0.6270 - val_accuracy: 0.7856 - lr: 0.0037\n",
            "Epoch 26/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.5988 - accuracy: 0.7927 - val_loss: 0.5914 - val_accuracy: 0.7991 - lr: 0.0033\n",
            "Epoch 27/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.5930 - accuracy: 0.7939 - val_loss: 0.6225 - val_accuracy: 0.7878 - lr: 0.0030\n",
            "Epoch 28/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.5841 - accuracy: 0.7972 - val_loss: 0.6032 - val_accuracy: 0.7951 - lr: 0.0027\n",
            "Epoch 29/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.5741 - accuracy: 0.8027 - val_loss: 0.6096 - val_accuracy: 0.7926 - lr: 0.0025\n",
            "Epoch 30/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.5784 - accuracy: 0.7984 - val_loss: 0.6391 - val_accuracy: 0.7853 - lr: 0.0022\n",
            "Score for fold 1: loss of 0.6390799880027771; accuracy of 78.53000164031982%\n",
            "Model: \"sequential_59\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_177 (Conv2D)         (None, 32, 32, 64)        1792      \n",
            "                                                                 \n",
            " max_pooling2d_59 (MaxPoolin  multiple                 0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_178 (Conv2D)         (None, 16, 16, 128)       73856     \n",
            "                                                                 \n",
            " dropout_59 (Dropout)        (None, 8, 8, 128)         0         \n",
            "                                                                 \n",
            " conv2d_179 (Conv2D)         (None, 8, 8, 256)         295168    \n",
            "                                                                 \n",
            " flatten_59 (Flatten)        (None, 4096)              0         \n",
            "                                                                 \n",
            " dense_59 (Dense)            (None, 10)                40970     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 411,786\n",
            "Trainable params: 411,786\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.7771 - accuracy: 0.3632 - val_loss: 1.4415 - val_accuracy: 0.4834 - lr: 0.0100\n",
            "Epoch 2/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.4263 - accuracy: 0.4933 - val_loss: 1.2968 - val_accuracy: 0.5454 - lr: 0.0100\n",
            "Epoch 3/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 1.2715 - accuracy: 0.5543 - val_loss: 1.1321 - val_accuracy: 0.5961 - lr: 0.0100\n",
            "Epoch 4/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.1710 - accuracy: 0.5881 - val_loss: 1.0833 - val_accuracy: 0.6185 - lr: 0.0100\n",
            "Epoch 5/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.0952 - accuracy: 0.6163 - val_loss: 0.9808 - val_accuracy: 0.6556 - lr: 0.0100\n",
            "Epoch 6/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.0371 - accuracy: 0.6386 - val_loss: 0.9642 - val_accuracy: 0.6716 - lr: 0.0100\n",
            "Epoch 7/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.9884 - accuracy: 0.6535 - val_loss: 0.8866 - val_accuracy: 0.6953 - lr: 0.0100\n",
            "Epoch 8/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.9486 - accuracy: 0.6691 - val_loss: 0.8361 - val_accuracy: 0.7197 - lr: 0.0100\n",
            "Epoch 9/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.9101 - accuracy: 0.6846 - val_loss: 0.8230 - val_accuracy: 0.7169 - lr: 0.0100\n",
            "Epoch 10/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.8841 - accuracy: 0.6926 - val_loss: 0.8014 - val_accuracy: 0.7273 - lr: 0.0100\n",
            "Epoch 11/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.8567 - accuracy: 0.6995 - val_loss: 0.7847 - val_accuracy: 0.7383 - lr: 0.0100\n",
            "Epoch 12/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.8246 - accuracy: 0.7143 - val_loss: 0.7842 - val_accuracy: 0.7318 - lr: 0.0100\n",
            "Epoch 13/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.8086 - accuracy: 0.7179 - val_loss: 0.8444 - val_accuracy: 0.7077 - lr: 0.0100\n",
            "Epoch 14/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.7873 - accuracy: 0.7276 - val_loss: 0.7834 - val_accuracy: 0.7358 - lr: 0.0100\n",
            "Epoch 15/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.7710 - accuracy: 0.7325 - val_loss: 0.7076 - val_accuracy: 0.7629 - lr: 0.0100\n",
            "Epoch 16/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.7457 - accuracy: 0.7422 - val_loss: 0.7037 - val_accuracy: 0.7595 - lr: 0.0090\n",
            "Epoch 17/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.7258 - accuracy: 0.7487 - val_loss: 0.6900 - val_accuracy: 0.7702 - lr: 0.0082\n",
            "Epoch 18/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.7073 - accuracy: 0.7545 - val_loss: 0.6817 - val_accuracy: 0.7689 - lr: 0.0074\n",
            "Epoch 19/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.6850 - accuracy: 0.7642 - val_loss: 0.6580 - val_accuracy: 0.7747 - lr: 0.0067\n",
            "Epoch 20/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.6689 - accuracy: 0.7666 - val_loss: 0.6626 - val_accuracy: 0.7770 - lr: 0.0061\n",
            "Epoch 21/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.6591 - accuracy: 0.7720 - val_loss: 0.6432 - val_accuracy: 0.7818 - lr: 0.0055\n",
            "Epoch 22/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.6411 - accuracy: 0.7784 - val_loss: 0.6107 - val_accuracy: 0.7927 - lr: 0.0050\n",
            "Epoch 23/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.6343 - accuracy: 0.7802 - val_loss: 0.6559 - val_accuracy: 0.7788 - lr: 0.0045\n",
            "Epoch 24/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.6226 - accuracy: 0.7844 - val_loss: 0.6269 - val_accuracy: 0.7869 - lr: 0.0041\n",
            "Epoch 25/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.6129 - accuracy: 0.7868 - val_loss: 0.6167 - val_accuracy: 0.7910 - lr: 0.0037\n",
            "Epoch 26/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.6017 - accuracy: 0.7897 - val_loss: 0.6365 - val_accuracy: 0.7813 - lr: 0.0033\n",
            "Epoch 27/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.5934 - accuracy: 0.7954 - val_loss: 0.5951 - val_accuracy: 0.7994 - lr: 0.0030\n",
            "Epoch 28/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.5917 - accuracy: 0.7930 - val_loss: 0.6145 - val_accuracy: 0.7911 - lr: 0.0027\n",
            "Epoch 29/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.5784 - accuracy: 0.8008 - val_loss: 0.5942 - val_accuracy: 0.7987 - lr: 0.0025\n",
            "Epoch 30/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.5734 - accuracy: 0.8016 - val_loss: 0.6065 - val_accuracy: 0.7921 - lr: 0.0022\n",
            "Score for fold 2: loss of 0.6065478920936584; accuracy of 79.21000123023987%\n",
            "Model: \"sequential_60\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_180 (Conv2D)         (None, 32, 32, 64)        1792      \n",
            "                                                                 \n",
            " max_pooling2d_60 (MaxPoolin  multiple                 0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_181 (Conv2D)         (None, 16, 16, 128)       73856     \n",
            "                                                                 \n",
            " dropout_60 (Dropout)        (None, 8, 8, 128)         0         \n",
            "                                                                 \n",
            " conv2d_182 (Conv2D)         (None, 8, 8, 256)         295168    \n",
            "                                                                 \n",
            " flatten_60 (Flatten)        (None, 4096)              0         \n",
            "                                                                 \n",
            " dense_60 (Dense)            (None, 10)                40970     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 411,786\n",
            "Trainable params: 411,786\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.6857 - accuracy: 0.3954 - val_loss: 1.5356 - val_accuracy: 0.4682 - lr: 0.0100\n",
            "Epoch 2/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 1.3630 - accuracy: 0.5176 - val_loss: 1.2668 - val_accuracy: 0.5494 - lr: 0.0100\n",
            "Epoch 3/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 1.2249 - accuracy: 0.5691 - val_loss: 1.1125 - val_accuracy: 0.6039 - lr: 0.0100\n",
            "Epoch 4/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.1212 - accuracy: 0.6072 - val_loss: 1.0040 - val_accuracy: 0.6520 - lr: 0.0100\n",
            "Epoch 5/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.0540 - accuracy: 0.6334 - val_loss: 1.0454 - val_accuracy: 0.6336 - lr: 0.0100\n",
            "Epoch 6/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.9993 - accuracy: 0.6523 - val_loss: 0.9075 - val_accuracy: 0.6873 - lr: 0.0100\n",
            "Epoch 7/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.9475 - accuracy: 0.6708 - val_loss: 0.9014 - val_accuracy: 0.6885 - lr: 0.0100\n",
            "Epoch 8/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.9113 - accuracy: 0.6832 - val_loss: 0.8487 - val_accuracy: 0.7052 - lr: 0.0100\n",
            "Epoch 9/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.8824 - accuracy: 0.6919 - val_loss: 0.7879 - val_accuracy: 0.7294 - lr: 0.0100\n",
            "Epoch 10/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.8544 - accuracy: 0.7040 - val_loss: 0.8219 - val_accuracy: 0.7185 - lr: 0.0100\n",
            "Epoch 11/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.8265 - accuracy: 0.7136 - val_loss: 0.7458 - val_accuracy: 0.7471 - lr: 0.0100\n",
            "Epoch 12/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.8007 - accuracy: 0.7239 - val_loss: 0.7845 - val_accuracy: 0.7348 - lr: 0.0100\n",
            "Epoch 13/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.7815 - accuracy: 0.7293 - val_loss: 0.7629 - val_accuracy: 0.7402 - lr: 0.0100\n",
            "Epoch 14/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 0.7616 - accuracy: 0.7369 - val_loss: 0.7421 - val_accuracy: 0.7488 - lr: 0.0100\n",
            "Epoch 15/30\n",
            "1068/1250 [========================>.....] - ETA: 2s - loss: 0.7418 - accuracy: 0.7437"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_22076/2235855649.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'Training for fold {fold_no} ...'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m     history = model.train(\n\u001b[0m\u001b[0;32m     21\u001b[0m               \u001b[0mtrain_gen\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m                 \u001b[0mtrainX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_22076/1534972907.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, train_gen, train_x, train_y, test_x, test_y, lr_rate, lr_rate_type)\u001b[0m\n\u001b[0;32m    161\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_gen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtensorboard_callback\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcallback\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 163\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_gen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtensorboard_callback\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcallback\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m#  self.model.fit(train_x, train_y, validation_data=(test_x, test_y) , epochs=25, verbose=1)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    164\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_gen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtensorboard_callback\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1382\u001b[0m                 _r=1):\n\u001b[0;32m   1383\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1384\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1385\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1386\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    913\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    914\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 915\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    945\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    946\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 947\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    948\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2954\u001b[0m       (graph_function,\n\u001b[0;32m   2955\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2956\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   2957\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   2958\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1851\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1852\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1853\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1854\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1855\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    498\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 499\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    500\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    501\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### SGD: Learning rate 1e-2 Momentum 0.9"
      ],
      "metadata": {
        "id": "7kdDARCr6hTy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "acc_per_fold = []\n",
        "loss_per_fold = []\n",
        "\n",
        "with tf.device('/device:GPU:0'):\n",
        "  fold_no = 1\n",
        "  for train, valid in kfold.split(trainX, trainy):\n",
        "\n",
        "    model = CNN_Model(batch_size=32, depth=3, hidden_neurons_1=64, hidden_neurons_2=128, hidden_neurons_3=256, kernel_size_1=3, kernel_size_2=3, kernel_size_3=3, \n",
        "               dropout_rate=0.2, batch_norm=False, pooling='max', layer_activation_1='relu', layer_activation_2='relu', layer_activation_3='relu',\n",
        "               kernel_initialiser=tf.keras.initializers.HeNormal(), bias_initialiser='zeros', optimiser='SGD', weight_decay=None, learning_rate=1e-2, output_activation='softmax', SGD_momentum=0.9)\n",
        "  \n",
        "    train_gen = tf.keras.preprocessing.image.ImageDataGenerator(width_shift_range=0.1,\n",
        "      height_shift_range=0.1,\n",
        "      horizontal_flip=True)\n",
        "    train_gen.fit(trainX[train]) #Data augmentation\n",
        "  \n",
        "    print('------------------------------------------------------------------------')\n",
        "    print(f'Training for fold {fold_no} ...')\n",
        "\n",
        "    history = model.train(\n",
        "              train_gen,\n",
        "                trainX[train],\n",
        "                  trainy[train],\n",
        "                trainX[valid],\n",
        "                  trainy[valid],\n",
        "                  lr_rate_type='step')\n",
        "\n",
        "    scores = model.evaluate(trainX[valid], trainy[valid])\n",
        "\n",
        "    print(f'Score for fold {fold_no}: loss of {scores[0]}; accuracy of {scores[1]*100}%')\n",
        "    acc_per_fold.append(scores[1] * 100)\n",
        "    loss_per_fold.append(scores[0])\n",
        "\n",
        "    fold_no+=1\n",
        "\n",
        "# == Provide average scores ==\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Score per fold')\n",
        "for i in range(0, len(acc_per_fold)):\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Average scores for all folds:')\n",
        "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
        "print(f'> Loss: {np.mean(loss_per_fold)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "fXbReHgF6ikF",
        "outputId": "f39f4f77-460e-4324-e674-cecbab2e3b21"
      },
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_25\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_75 (Conv2D)          (None, 32, 32, 64)        1792      \n",
            "                                                                 \n",
            " max_pooling2d_24 (MaxPoolin  multiple                 0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_76 (Conv2D)          (None, 16, 16, 128)       73856     \n",
            "                                                                 \n",
            " dropout_25 (Dropout)        (None, 8, 8, 128)         0         \n",
            "                                                                 \n",
            " conv2d_77 (Conv2D)          (None, 8, 8, 256)         295168    \n",
            "                                                                 \n",
            " flatten_25 (Flatten)        (None, 4096)              0         \n",
            "                                                                 \n",
            " dense_25 (Dense)            (None, 10)                40970     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 411,786\n",
            "Trainable params: 411,786\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.6735 - accuracy: 0.3980 - val_loss: 1.2471 - val_accuracy: 0.5646 - lr: 0.0100\n",
            "Epoch 2/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.2678 - accuracy: 0.5492 - val_loss: 1.0912 - val_accuracy: 0.6210 - lr: 0.0100\n",
            "Epoch 3/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 1.1195 - accuracy: 0.6066 - val_loss: 1.1587 - val_accuracy: 0.5936 - lr: 0.0100\n",
            "Epoch 4/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.0274 - accuracy: 0.6401 - val_loss: 0.9392 - val_accuracy: 0.6753 - lr: 0.0100\n",
            "Epoch 5/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.9732 - accuracy: 0.6627 - val_loss: 0.9232 - val_accuracy: 0.6832 - lr: 0.0100\n",
            "Epoch 6/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.9255 - accuracy: 0.6768 - val_loss: 0.8557 - val_accuracy: 0.7010 - lr: 0.0100\n",
            "Epoch 7/30\n",
            "1250/1250 [==============================] - 19s 16ms/step - loss: 0.8812 - accuracy: 0.6937 - val_loss: 0.7990 - val_accuracy: 0.7245 - lr: 0.0100\n",
            "Epoch 8/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.8543 - accuracy: 0.7048 - val_loss: 0.7630 - val_accuracy: 0.7378 - lr: 0.0100\n",
            "Epoch 9/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 0.8202 - accuracy: 0.7176 - val_loss: 0.7746 - val_accuracy: 0.7363 - lr: 0.0100\n",
            "Epoch 10/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.8050 - accuracy: 0.7205 - val_loss: 0.7596 - val_accuracy: 0.7383 - lr: 0.0100\n",
            "Epoch 11/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.7050 - accuracy: 0.7552 - val_loss: 0.6292 - val_accuracy: 0.7832 - lr: 0.0050\n",
            "Epoch 12/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.6752 - accuracy: 0.7674 - val_loss: 0.6700 - val_accuracy: 0.7681 - lr: 0.0050\n",
            "Epoch 13/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.6593 - accuracy: 0.7722 - val_loss: 0.6394 - val_accuracy: 0.7789 - lr: 0.0050\n",
            "Epoch 14/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.6444 - accuracy: 0.7745 - val_loss: 0.6305 - val_accuracy: 0.7846 - lr: 0.0050\n",
            "Epoch 15/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.6362 - accuracy: 0.7784 - val_loss: 0.6422 - val_accuracy: 0.7847 - lr: 0.0050\n",
            "Epoch 16/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.6198 - accuracy: 0.7865 - val_loss: 0.6332 - val_accuracy: 0.7829 - lr: 0.0050\n",
            "Epoch 17/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.6132 - accuracy: 0.7864 - val_loss: 0.5962 - val_accuracy: 0.7969 - lr: 0.0050\n",
            "Epoch 18/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.6073 - accuracy: 0.7892 - val_loss: 0.5815 - val_accuracy: 0.8007 - lr: 0.0050\n",
            "Epoch 19/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.5942 - accuracy: 0.7945 - val_loss: 0.5923 - val_accuracy: 0.8011 - lr: 0.0050\n",
            "Epoch 20/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.5861 - accuracy: 0.7955 - val_loss: 0.5953 - val_accuracy: 0.7943 - lr: 0.0050\n",
            "Epoch 21/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.5184 - accuracy: 0.8194 - val_loss: 0.5539 - val_accuracy: 0.8103 - lr: 0.0012\n",
            "Epoch 22/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.4986 - accuracy: 0.8281 - val_loss: 0.5475 - val_accuracy: 0.8127 - lr: 0.0012\n",
            "Epoch 23/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.4868 - accuracy: 0.8338 - val_loss: 0.5452 - val_accuracy: 0.8146 - lr: 0.0012\n",
            "Epoch 24/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.4885 - accuracy: 0.8319 - val_loss: 0.5388 - val_accuracy: 0.8196 - lr: 0.0012\n",
            "Epoch 25/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.4822 - accuracy: 0.8328 - val_loss: 0.5464 - val_accuracy: 0.8150 - lr: 0.0012\n",
            "Epoch 26/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.4758 - accuracy: 0.8345 - val_loss: 0.5242 - val_accuracy: 0.8236 - lr: 0.0012\n",
            "Epoch 27/30\n",
            " 304/1250 [======>.......................] - ETA: 12s - loss: 0.4864 - accuracy: 0.8313"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17312/1284178001.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'Training for fold {fold_no} ...'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m     history = model.train(\n\u001b[0m\u001b[0;32m     21\u001b[0m               \u001b[0mtrain_gen\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m                 \u001b[0mtrainX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17312/3965214673.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, train_gen, train_x, train_y, test_x, test_y, lr_rate, lr_rate_type)\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_gen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtensorboard_callback\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcallback\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 165\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_gen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtensorboard_callback\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcallback\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m#  self.model.fit(train_x, train_y, validation_data=(test_x, test_y) , epochs=25, verbose=1)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    166\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_gen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtensorboard_callback\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1382\u001b[0m                 _r=1):\n\u001b[0;32m   1383\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1384\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1385\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1386\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    913\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    914\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 915\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    945\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    946\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 947\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    948\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2954\u001b[0m       (graph_function,\n\u001b[0;32m   2955\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2956\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   2957\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   2958\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1851\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1852\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1853\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1854\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1855\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    498\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 499\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    500\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    501\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### SGD: Learning rate 1e-3 Momentum 0.3"
      ],
      "metadata": {
        "id": "_AZBvlQM6jHI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "acc_per_fold = []\n",
        "loss_per_fold = []\n",
        "\n",
        "with tf.device('/device:GPU:0'):\n",
        "  fold_no = 1\n",
        "  for train, valid in kfold.split(trainX, trainy):\n",
        "\n",
        "    model = CNN_Model(batch_size=32, depth=3, hidden_neurons_1=64, hidden_neurons_2=128, hidden_neurons_3=256, kernel_size_1=3, kernel_size_2=3, kernel_size_3=3, \n",
        "               dropout_rate=0.2, batch_norm=False, pooling='max', layer_activation_1='relu', layer_activation_2='relu', layer_activation_3='relu',\n",
        "               kernel_initialiser=tf.keras.initializers.HeNormal(), bias_initialiser='zeros', optimiser='SGD', weight_decay=None, learning_rate=1e-3, output_activation='softmax', SGD_momentum=0.3)\n",
        "  \n",
        "    train_gen = tf.keras.preprocessing.image.ImageDataGenerator(width_shift_range=0.1,\n",
        "      height_shift_range=0.1,\n",
        "      horizontal_flip=True)\n",
        "    train_gen.fit(trainX[train]) #Data augmentation\n",
        "  \n",
        "    print('------------------------------------------------------------------------')\n",
        "    print(f'Training for fold {fold_no} ...')\n",
        "\n",
        "    history = model.train(\n",
        "              train_gen,\n",
        "                trainX[train],\n",
        "                  trainy[train],\n",
        "                trainX[valid],\n",
        "                  trainy[valid],\n",
        "                  lr_rate_type='step')\n",
        "\n",
        "    scores = model.evaluate(trainX[valid], trainy[valid])\n",
        "\n",
        "    print(f'Score for fold {fold_no}: loss of {scores[0]}; accuracy of {scores[1]*100}%')\n",
        "    acc_per_fold.append(scores[1] * 100)\n",
        "    loss_per_fold.append(scores[0])\n",
        "\n",
        "    fold_no+=1\n",
        "\n",
        "# == Provide average scores ==\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Score per fold')\n",
        "for i in range(0, len(acc_per_fold)):\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Average scores for all folds:')\n",
        "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
        "print(f'> Loss: {np.mean(loss_per_fold)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "9zYzVAgi6kak",
        "outputId": "b303e73b-7035-41ab-aa98-0087289cb03c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_63\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_189 (Conv2D)         (None, 32, 32, 64)        1792      \n",
            "                                                                 \n",
            " max_pooling2d_63 (MaxPoolin  multiple                 0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_190 (Conv2D)         (None, 16, 16, 128)       73856     \n",
            "                                                                 \n",
            " dropout_63 (Dropout)        (None, 8, 8, 128)         0         \n",
            "                                                                 \n",
            " conv2d_191 (Conv2D)         (None, 8, 8, 256)         295168    \n",
            "                                                                 \n",
            " flatten_63 (Flatten)        (None, 4096)              0         \n",
            "                                                                 \n",
            " dense_63 (Dense)            (None, 10)                40970     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 411,786\n",
            "Trainable params: 411,786\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 2.0375 - accuracy: 0.2596 - val_loss: 1.8166 - val_accuracy: 0.3599 - lr: 0.0010\n",
            "Epoch 2/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.8036 - accuracy: 0.3631 - val_loss: 1.6884 - val_accuracy: 0.3971 - lr: 0.0010\n",
            "Epoch 3/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.6891 - accuracy: 0.3997 - val_loss: 1.6509 - val_accuracy: 0.4194 - lr: 0.0010\n",
            "Epoch 4/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.6137 - accuracy: 0.4272 - val_loss: 1.5219 - val_accuracy: 0.4656 - lr: 0.0010\n",
            "Epoch 5/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.5550 - accuracy: 0.4460 - val_loss: 1.4715 - val_accuracy: 0.4824 - lr: 0.0010\n",
            "Epoch 6/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.5092 - accuracy: 0.4636 - val_loss: 1.4504 - val_accuracy: 0.4983 - lr: 0.0010\n",
            "Epoch 7/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.4804 - accuracy: 0.4755 - val_loss: 1.4235 - val_accuracy: 0.5026 - lr: 0.0010\n",
            "Epoch 8/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.4515 - accuracy: 0.4828 - val_loss: 1.3765 - val_accuracy: 0.5204 - lr: 0.0010\n",
            "Epoch 9/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.4219 - accuracy: 0.4958 - val_loss: 1.3896 - val_accuracy: 0.5105 - lr: 0.0010\n",
            "Epoch 10/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.3989 - accuracy: 0.5063 - val_loss: 1.3850 - val_accuracy: 0.5182 - lr: 0.0010\n",
            "Epoch 11/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.3755 - accuracy: 0.5163 - val_loss: 1.3932 - val_accuracy: 0.5199 - lr: 0.0010\n",
            "Epoch 12/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.3547 - accuracy: 0.5250 - val_loss: 1.3759 - val_accuracy: 0.5195 - lr: 0.0010\n",
            "Epoch 13/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.3427 - accuracy: 0.5271 - val_loss: 1.2763 - val_accuracy: 0.5568 - lr: 0.0010\n",
            "Epoch 14/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.3234 - accuracy: 0.5378 - val_loss: 1.2813 - val_accuracy: 0.5610 - lr: 0.0010\n",
            "Epoch 15/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.2982 - accuracy: 0.5470 - val_loss: 1.2898 - val_accuracy: 0.5534 - lr: 0.0010\n",
            "Epoch 16/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.2888 - accuracy: 0.5447 - val_loss: 1.3319 - val_accuracy: 0.5401 - lr: 9.0484e-04\n",
            "Epoch 17/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.2710 - accuracy: 0.5566 - val_loss: 1.2718 - val_accuracy: 0.5627 - lr: 8.1873e-04\n",
            "Epoch 18/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.2590 - accuracy: 0.5593 - val_loss: 1.2886 - val_accuracy: 0.5535 - lr: 7.4082e-04\n",
            "Epoch 19/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.2516 - accuracy: 0.5614 - val_loss: 1.2629 - val_accuracy: 0.5694 - lr: 6.7032e-04\n",
            "Epoch 20/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.2388 - accuracy: 0.5656 - val_loss: 1.2182 - val_accuracy: 0.5787 - lr: 6.0653e-04\n",
            "Epoch 21/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.2269 - accuracy: 0.5716 - val_loss: 1.2617 - val_accuracy: 0.5646 - lr: 5.4881e-04\n",
            "Epoch 22/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.2200 - accuracy: 0.5726 - val_loss: 1.2612 - val_accuracy: 0.5698 - lr: 4.9659e-04\n",
            "Epoch 23/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.2117 - accuracy: 0.5781 - val_loss: 1.2065 - val_accuracy: 0.5878 - lr: 4.4933e-04\n",
            "Epoch 24/30\n",
            "1250/1250 [==============================] - 21s 17ms/step - loss: 1.2041 - accuracy: 0.5789 - val_loss: 1.2392 - val_accuracy: 0.5746 - lr: 4.0657e-04\n",
            "Epoch 25/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 1.1987 - accuracy: 0.5822 - val_loss: 1.2024 - val_accuracy: 0.5903 - lr: 3.6788e-04\n",
            "Epoch 26/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.1964 - accuracy: 0.5807 - val_loss: 1.2066 - val_accuracy: 0.5856 - lr: 3.3287e-04\n",
            "Epoch 27/30\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 1.1894 - accuracy: 0.5820 - val_loss: 1.2219 - val_accuracy: 0.5827 - lr: 3.0119e-04\n",
            "Epoch 28/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.1857 - accuracy: 0.5865 - val_loss: 1.2045 - val_accuracy: 0.5859 - lr: 2.7253e-04\n",
            "Epoch 29/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 1.1844 - accuracy: 0.5885 - val_loss: 1.2096 - val_accuracy: 0.5869 - lr: 2.4660e-04\n",
            "Epoch 30/30\n",
            "1250/1250 [==============================] - 20s 16ms/step - loss: 1.1822 - accuracy: 0.5892 - val_loss: 1.2194 - val_accuracy: 0.5835 - lr: 2.2313e-04\n",
            "Score for fold 1: loss of 1.2194243669509888; accuracy of 58.35000276565552%\n",
            "Model: \"sequential_64\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_192 (Conv2D)         (None, 32, 32, 64)        1792      \n",
            "                                                                 \n",
            " max_pooling2d_64 (MaxPoolin  multiple                 0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_193 (Conv2D)         (None, 16, 16, 128)       73856     \n",
            "                                                                 \n",
            " dropout_64 (Dropout)        (None, 8, 8, 128)         0         \n",
            "                                                                 \n",
            " conv2d_194 (Conv2D)         (None, 8, 8, 256)         295168    \n",
            "                                                                 \n",
            " flatten_64 (Flatten)        (None, 4096)              0         \n",
            "                                                                 \n",
            " dense_64 (Dense)            (None, 10)                40970     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 411,786\n",
            "Trainable params: 411,786\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 2.0470 - accuracy: 0.2607 - val_loss: 1.7922 - val_accuracy: 0.3833 - lr: 0.0010\n",
            "Epoch 2/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.7896 - accuracy: 0.3607 - val_loss: 1.6650 - val_accuracy: 0.4177 - lr: 0.0010\n",
            "Epoch 3/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.6839 - accuracy: 0.3973 - val_loss: 1.5717 - val_accuracy: 0.4369 - lr: 0.0010\n",
            "Epoch 4/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.6133 - accuracy: 0.4241 - val_loss: 1.5162 - val_accuracy: 0.4663 - lr: 0.0010\n",
            "Epoch 5/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.5651 - accuracy: 0.4417 - val_loss: 1.4655 - val_accuracy: 0.4927 - lr: 0.0010\n",
            "Epoch 6/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.5225 - accuracy: 0.4559 - val_loss: 1.4436 - val_accuracy: 0.4848 - lr: 0.0010\n",
            "Epoch 7/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.4930 - accuracy: 0.4692 - val_loss: 1.3982 - val_accuracy: 0.5045 - lr: 0.0010\n",
            "Epoch 8/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.4654 - accuracy: 0.4798 - val_loss: 1.4352 - val_accuracy: 0.4868 - lr: 0.0010\n",
            "Epoch 9/30\n",
            "1161/1250 [==========================>...] - ETA: 1s - loss: 1.4448 - accuracy: 0.4880"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_22076/3954162994.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'Training for fold {fold_no} ...'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m     history = model.train(\n\u001b[0m\u001b[0;32m     21\u001b[0m               \u001b[0mtrain_gen\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m                 \u001b[0mtrainX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_22076/1534972907.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, train_gen, train_x, train_y, test_x, test_y, lr_rate, lr_rate_type)\u001b[0m\n\u001b[0;32m    161\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_gen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtensorboard_callback\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcallback\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 163\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_gen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtensorboard_callback\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcallback\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m#  self.model.fit(train_x, train_y, validation_data=(test_x, test_y) , epochs=25, verbose=1)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    164\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_gen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtensorboard_callback\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1382\u001b[0m                 _r=1):\n\u001b[0;32m   1383\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1384\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1385\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1386\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    913\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    914\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 915\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    945\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    946\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 947\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    948\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2954\u001b[0m       (graph_function,\n\u001b[0;32m   2955\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2956\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   2957\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   2958\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1851\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1852\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1853\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1854\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1855\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    498\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 499\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    500\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    501\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### SGD: Learning rate 1e-3 Momentum 0.6"
      ],
      "metadata": {
        "id": "Ixjwc_sY6k7D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "acc_per_fold = []\n",
        "loss_per_fold = []\n",
        "\n",
        "with tf.device('/device:GPU:0'):\n",
        "  fold_no = 1\n",
        "  for train, valid in kfold.split(trainX, trainy):\n",
        "\n",
        "    model = CNN_Model(batch_size=32, depth=3, hidden_neurons_1=64, hidden_neurons_2=128, hidden_neurons_3=256, kernel_size_1=3, kernel_size_2=3, kernel_size_3=3, \n",
        "               dropout_rate=0.2, batch_norm=False, pooling='max', layer_activation_1='relu', layer_activation_2='relu', layer_activation_3='relu',\n",
        "               kernel_initialiser=tf.keras.initializers.HeNormal(), bias_initialiser='zeros', optimiser='SGD', weight_decay=None, learning_rate=1e-3, output_activation='softmax', SGD_momentum=0.6) \n",
        "  \n",
        "    train_gen = tf.keras.preprocessing.image.ImageDataGenerator(width_shift_range=0.1,\n",
        "      height_shift_range=0.1,\n",
        "      horizontal_flip=True)\n",
        "    train_gen.fit(trainX[train]) #Data augmentation\n",
        "  \n",
        "    print('------------------------------------------------------------------------')\n",
        "    print(f'Training for fold {fold_no} ...')\n",
        "\n",
        "    history = model.train(\n",
        "              train_gen,\n",
        "                trainX[train],\n",
        "                  trainy[train],\n",
        "                trainX[valid],\n",
        "                  trainy[valid],\n",
        "                  lr_rate_type='step')\n",
        "\n",
        "    scores = model.evaluate(trainX[valid], trainy[valid])\n",
        "\n",
        "    print(f'Score for fold {fold_no}: loss of {scores[0]}; accuracy of {scores[1]*100}%')\n",
        "    acc_per_fold.append(scores[1] * 100)\n",
        "    loss_per_fold.append(scores[0])\n",
        "\n",
        "    fold_no+=1\n",
        "\n",
        "# == Provide average scores ==\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Score per fold')\n",
        "for i in range(0, len(acc_per_fold)):\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Average scores for all folds:')\n",
        "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
        "print(f'> Loss: {np.mean(loss_per_fold)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "lOjsGBV26m-h",
        "outputId": "e60dc06f-d9d2-4466-99b5-8220b223ce48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_65\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_195 (Conv2D)         (None, 32, 32, 64)        1792      \n",
            "                                                                 \n",
            " max_pooling2d_65 (MaxPoolin  multiple                 0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_196 (Conv2D)         (None, 16, 16, 128)       73856     \n",
            "                                                                 \n",
            " dropout_65 (Dropout)        (None, 8, 8, 128)         0         \n",
            "                                                                 \n",
            " conv2d_197 (Conv2D)         (None, 8, 8, 256)         295168    \n",
            "                                                                 \n",
            " flatten_65 (Flatten)        (None, 4096)              0         \n",
            "                                                                 \n",
            " dense_65 (Dense)            (None, 10)                40970     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 411,786\n",
            "Trainable params: 411,786\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/30\n",
            "1250/1250 [==============================] - 20s 15ms/step - loss: 1.9781 - accuracy: 0.2849 - val_loss: 1.7486 - val_accuracy: 0.3793 - lr: 0.0010\n",
            "Epoch 2/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 1.7142 - accuracy: 0.3905 - val_loss: 1.6149 - val_accuracy: 0.4308 - lr: 0.0010\n",
            "Epoch 3/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.6036 - accuracy: 0.4268 - val_loss: 1.5050 - val_accuracy: 0.4643 - lr: 0.0010\n",
            "Epoch 4/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.5333 - accuracy: 0.4498 - val_loss: 1.4825 - val_accuracy: 0.4608 - lr: 0.0010\n",
            "Epoch 5/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.4815 - accuracy: 0.4721 - val_loss: 1.4976 - val_accuracy: 0.4664 - lr: 0.0010\n",
            "Epoch 6/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.4425 - accuracy: 0.4888 - val_loss: 1.3844 - val_accuracy: 0.5037 - lr: 0.0010\n",
            "Epoch 7/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 1.4111 - accuracy: 0.4969 - val_loss: 1.3860 - val_accuracy: 0.5055 - lr: 0.0010\n",
            "Epoch 8/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 1.3830 - accuracy: 0.5104 - val_loss: 1.3480 - val_accuracy: 0.5223 - lr: 0.0010\n",
            "Epoch 9/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.3587 - accuracy: 0.5202 - val_loss: 1.3286 - val_accuracy: 0.5334 - lr: 0.0010\n",
            "Epoch 10/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.3311 - accuracy: 0.5310 - val_loss: 1.2850 - val_accuracy: 0.5481 - lr: 0.0010\n",
            "Epoch 11/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.3091 - accuracy: 0.5361 - val_loss: 1.3225 - val_accuracy: 0.5362 - lr: 0.0010\n",
            "Epoch 12/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.2889 - accuracy: 0.5453 - val_loss: 1.2878 - val_accuracy: 0.5445 - lr: 0.0010\n",
            "Epoch 13/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 1.2731 - accuracy: 0.5527 - val_loss: 1.3018 - val_accuracy: 0.5462 - lr: 0.0010\n",
            "Epoch 14/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 1.2509 - accuracy: 0.5614 - val_loss: 1.2095 - val_accuracy: 0.5723 - lr: 0.0010\n",
            "Epoch 15/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.2366 - accuracy: 0.5662 - val_loss: 1.2656 - val_accuracy: 0.5607 - lr: 0.0010\n",
            "Epoch 16/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.2179 - accuracy: 0.5728 - val_loss: 1.2277 - val_accuracy: 0.5750 - lr: 9.0484e-04\n",
            "Epoch 17/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 1.1985 - accuracy: 0.5813 - val_loss: 1.1134 - val_accuracy: 0.6117 - lr: 8.1873e-04\n",
            "Epoch 18/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 1.1866 - accuracy: 0.5852 - val_loss: 1.1955 - val_accuracy: 0.5855 - lr: 7.4082e-04\n",
            "Epoch 19/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 1.1747 - accuracy: 0.5882 - val_loss: 1.1626 - val_accuracy: 0.6002 - lr: 6.7032e-04\n",
            "Epoch 20/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 1.1629 - accuracy: 0.5954 - val_loss: 1.1508 - val_accuracy: 0.5972 - lr: 6.0653e-04\n",
            "Epoch 21/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.1515 - accuracy: 0.5976 - val_loss: 1.1684 - val_accuracy: 0.5917 - lr: 5.4881e-04\n",
            "Epoch 22/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.1460 - accuracy: 0.6005 - val_loss: 1.1976 - val_accuracy: 0.5867 - lr: 4.9659e-04\n",
            "Epoch 23/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.1358 - accuracy: 0.6039 - val_loss: 1.1033 - val_accuracy: 0.6191 - lr: 4.4933e-04\n",
            "Epoch 24/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.1311 - accuracy: 0.6047 - val_loss: 1.1736 - val_accuracy: 0.5919 - lr: 4.0657e-04\n",
            "Epoch 25/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 1.1244 - accuracy: 0.6096 - val_loss: 1.1070 - val_accuracy: 0.6137 - lr: 3.6788e-04\n",
            "Epoch 26/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 1.1208 - accuracy: 0.6104 - val_loss: 1.1452 - val_accuracy: 0.6018 - lr: 3.3287e-04\n",
            "Epoch 27/30\n",
            "1178/1250 [===========================>..] - ETA: 0s - loss: 1.1145 - accuracy: 0.6110"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_22076/4196085861.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'Training for fold {fold_no} ...'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m     history = model.train(\n\u001b[0m\u001b[0;32m     21\u001b[0m               \u001b[0mtrain_gen\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m                 \u001b[0mtrainX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_22076/1534972907.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, train_gen, train_x, train_y, test_x, test_y, lr_rate, lr_rate_type)\u001b[0m\n\u001b[0;32m    161\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_gen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtensorboard_callback\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcallback\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 163\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_gen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtensorboard_callback\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcallback\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m#  self.model.fit(train_x, train_y, validation_data=(test_x, test_y) , epochs=25, verbose=1)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    164\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_gen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtensorboard_callback\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1382\u001b[0m                 _r=1):\n\u001b[0;32m   1383\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1384\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1385\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1386\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    913\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    914\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 915\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    945\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    946\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 947\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    948\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2954\u001b[0m       (graph_function,\n\u001b[0;32m   2955\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2956\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   2957\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   2958\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1851\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1852\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1853\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1854\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1855\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    498\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 499\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    500\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    501\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### SGD: Learning rate 1e-3 Momentum 0.9"
      ],
      "metadata": {
        "id": "zm0BZ2y66nl4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "acc_per_fold = []\n",
        "loss_per_fold = []\n",
        "\n",
        "with tf.device('/device:GPU:0'):\n",
        "  fold_no = 1\n",
        "  for train, valid in kfold.split(trainX, trainy):\n",
        "\n",
        "    model = CNN_Model(batch_size=32, depth=3, hidden_neurons_1=64, hidden_neurons_2=128, hidden_neurons_3=256, kernel_size_1=3, kernel_size_2=3, kernel_size_3=3, \n",
        "               dropout_rate=0.2, batch_norm=False, pooling='max', layer_activation_1='relu', layer_activation_2='relu', layer_activation_3='relu',\n",
        "               kernel_initialiser=tf.keras.initializers.HeNormal(), bias_initialiser='zeros', optimiser='SGD', weight_decay=None, learning_rate=1e-3, output_activation='softmax', SGD_momentum=0.9) \n",
        "  \n",
        "    train_gen = tf.keras.preprocessing.image.ImageDataGenerator(width_shift_range=0.1,\n",
        "      height_shift_range=0.1,\n",
        "      horizontal_flip=True)\n",
        "    train_gen.fit(trainX[train]) #Data augmentation\n",
        "  \n",
        "    print('------------------------------------------------------------------------')\n",
        "    print(f'Training for fold {fold_no} ...')\n",
        "\n",
        "    history = model.train(\n",
        "              train_gen,\n",
        "                trainX[train],\n",
        "                  trainy[train],\n",
        "                trainX[valid],\n",
        "                  trainy[valid],\n",
        "                  lr_rate_type='step')\n",
        "\n",
        "    scores = model.evaluate(trainX[valid], trainy[valid])\n",
        "\n",
        "    print(f'Score for fold {fold_no}: loss of {scores[0]}; accuracy of {scores[1]*100}%')\n",
        "    acc_per_fold.append(scores[1] * 100)\n",
        "    loss_per_fold.append(scores[0])\n",
        "\n",
        "    fold_no+=1\n",
        "\n",
        "# == Provide average scores ==\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Score per fold')\n",
        "for i in range(0, len(acc_per_fold)):\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Average scores for all folds:')\n",
        "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
        "print(f'> Loss: {np.mean(loss_per_fold)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "lKrjNh_t6pe9",
        "outputId": "097b17f0-aaa1-45fd-9756-b4ed80f5e6e7"
      },
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_26\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_78 (Conv2D)          (None, 32, 32, 64)        1792      \n",
            "                                                                 \n",
            " max_pooling2d_25 (MaxPoolin  multiple                 0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_79 (Conv2D)          (None, 16, 16, 128)       73856     \n",
            "                                                                 \n",
            " dropout_26 (Dropout)        (None, 8, 8, 128)         0         \n",
            "                                                                 \n",
            " conv2d_80 (Conv2D)          (None, 8, 8, 256)         295168    \n",
            "                                                                 \n",
            " flatten_26 (Flatten)        (None, 4096)              0         \n",
            "                                                                 \n",
            " dense_26 (Dense)            (None, 10)                40970     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 411,786\n",
            "Trainable params: 411,786\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 1.7818 - accuracy: 0.3632 - val_loss: 1.5795 - val_accuracy: 0.4379 - lr: 0.0010\n",
            "Epoch 2/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.4956 - accuracy: 0.4653 - val_loss: 1.3626 - val_accuracy: 0.5174 - lr: 0.0010\n",
            "Epoch 3/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.3795 - accuracy: 0.5098 - val_loss: 1.2659 - val_accuracy: 0.5483 - lr: 0.0010\n",
            "Epoch 4/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.2960 - accuracy: 0.5454 - val_loss: 1.2385 - val_accuracy: 0.5489 - lr: 0.0010\n",
            "Epoch 5/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.2334 - accuracy: 0.5630 - val_loss: 1.1800 - val_accuracy: 0.5840 - lr: 0.0010\n",
            "Epoch 6/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.1799 - accuracy: 0.5857 - val_loss: 1.2053 - val_accuracy: 0.5747 - lr: 0.0010\n",
            "Epoch 7/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 1.1306 - accuracy: 0.6036 - val_loss: 1.0241 - val_accuracy: 0.6440 - lr: 0.0010\n",
            "Epoch 8/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.0959 - accuracy: 0.6182 - val_loss: 1.0600 - val_accuracy: 0.6310 - lr: 0.0010\n",
            "Epoch 9/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 1.0577 - accuracy: 0.6319 - val_loss: 1.0888 - val_accuracy: 0.6192 - lr: 0.0010\n",
            "Epoch 10/30\n",
            "1250/1250 [==============================] - 20s 16ms/step - loss: 1.0299 - accuracy: 0.6406 - val_loss: 0.9981 - val_accuracy: 0.6557 - lr: 0.0010\n",
            "Epoch 11/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.9812 - accuracy: 0.6615 - val_loss: 0.9745 - val_accuracy: 0.6622 - lr: 5.0000e-04\n",
            "Epoch 12/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.9666 - accuracy: 0.6661 - val_loss: 0.9135 - val_accuracy: 0.6886 - lr: 5.0000e-04\n",
            "Epoch 13/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.9534 - accuracy: 0.6703 - val_loss: 0.9398 - val_accuracy: 0.6776 - lr: 5.0000e-04\n",
            "Epoch 14/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 0.9381 - accuracy: 0.6741 - val_loss: 0.9385 - val_accuracy: 0.6771 - lr: 5.0000e-04\n",
            "Epoch 15/30\n",
            "1250/1250 [==============================] - 20s 16ms/step - loss: 0.9246 - accuracy: 0.6812 - val_loss: 0.9385 - val_accuracy: 0.6809 - lr: 5.0000e-04\n",
            "Epoch 16/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 0.9156 - accuracy: 0.6841 - val_loss: 0.9197 - val_accuracy: 0.6882 - lr: 5.0000e-04\n",
            "Epoch 17/30\n",
            " 683/1250 [===============>..............] - ETA: 7s - loss: 0.9080 - accuracy: 0.6843"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17312/2731039421.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'Training for fold {fold_no} ...'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m     history = model.train(\n\u001b[0m\u001b[0;32m     21\u001b[0m               \u001b[0mtrain_gen\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m                 \u001b[0mtrainX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17312/3965214673.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, train_gen, train_x, train_y, test_x, test_y, lr_rate, lr_rate_type)\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_gen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtensorboard_callback\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcallback\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 165\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_gen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtensorboard_callback\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcallback\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m#  self.model.fit(train_x, train_y, validation_data=(test_x, test_y) , epochs=25, verbose=1)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    166\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_gen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtensorboard_callback\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1374\u001b[0m         \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1375\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcatch_stop_iteration\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1376\u001b[1;33m           \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1377\u001b[0m             with tf.profiler.experimental.Trace(\n\u001b[0;32m   1378\u001b[0m                 \u001b[1;34m'train'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36msteps\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1244\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_insufficient_data\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Set by `catch_stop_iteration`.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1245\u001b[0m         \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1246\u001b[1;33m       \u001b[0moriginal_spe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_steps_per_execution\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1247\u001b[0m       can_run_full_execution = (\n\u001b[0;32m   1248\u001b[0m           \u001b[0moriginal_spe\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    672\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    673\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 674\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    675\u001b[0m     raise NotImplementedError(\n\u001b[0;32m    676\u001b[0m         \"numpy() is only available when eager execution is enabled.\")\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1221\u001b[0m     \"\"\"\n\u001b[0;32m   1222\u001b[0m     \u001b[1;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1223\u001b[1;33m     \u001b[0mmaybe_arr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1224\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1225\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1187\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1188\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1189\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1190\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1191\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### SGD: Learning rate 1e-4 Momentum 0.3"
      ],
      "metadata": {
        "id": "L2f-PVtW6qzb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "acc_per_fold = []\n",
        "loss_per_fold = []\n",
        "\n",
        "with tf.device('/device:GPU:0'):\n",
        "  fold_no = 1\n",
        "  for train, valid in kfold.split(trainX, trainy):\n",
        "\n",
        "    model = CNN_Model(batch_size=32, depth=3, hidden_neurons_1=64, hidden_neurons_2=128, hidden_neurons_3=256, kernel_size_1=3, kernel_size_2=3, kernel_size_3=3, \n",
        "               dropout_rate=0.2, batch_norm=False, pooling='max', layer_activation_1='relu', layer_activation_2='relu', layer_activation_3='relu',\n",
        "               kernel_initialiser=tf.keras.initializers.HeNormal(), bias_initialiser='zeros', optimiser='SGD', weight_decay=None, learning_rate=1e-4, output_activation='softmax', SGD_momentum=0.3) \n",
        "  \n",
        "    train_gen = tf.keras.preprocessing.image.ImageDataGenerator(width_shift_range=0.1,\n",
        "      height_shift_range=0.1,\n",
        "      horizontal_flip=True)\n",
        "    train_gen.fit(trainX[train]) #Data augmentation\n",
        "  \n",
        "    print('------------------------------------------------------------------------')\n",
        "    print(f'Training for fold {fold_no} ...')\n",
        "\n",
        "    history = model.train(\n",
        "              train_gen,\n",
        "                trainX[train],\n",
        "                  trainy[train],\n",
        "                trainX[valid],\n",
        "                  trainy[valid],\n",
        "                  lr_rate_type='step')\n",
        "\n",
        "    scores = model.evaluate(trainX[valid], trainy[valid])\n",
        "\n",
        "    print(f'Score for fold {fold_no}: loss of {scores[0]}; accuracy of {scores[1]*100}%')\n",
        "    acc_per_fold.append(scores[1] * 100)\n",
        "    loss_per_fold.append(scores[0])\n",
        "\n",
        "    fold_no+=1\n",
        "\n",
        "# == Provide average scores ==\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Score per fold')\n",
        "for i in range(0, len(acc_per_fold)):\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Average scores for all folds:')\n",
        "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
        "print(f'> Loss: {np.mean(loss_per_fold)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ZxvFdkd06sGw",
        "outputId": "982cace2-a023-4ace-9b79-baf6bb223bf9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_67\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_201 (Conv2D)         (None, 32, 32, 64)        1792      \n",
            "                                                                 \n",
            " max_pooling2d_67 (MaxPoolin  multiple                 0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_202 (Conv2D)         (None, 16, 16, 128)       73856     \n",
            "                                                                 \n",
            " dropout_67 (Dropout)        (None, 8, 8, 128)         0         \n",
            "                                                                 \n",
            " conv2d_203 (Conv2D)         (None, 8, 8, 256)         295168    \n",
            "                                                                 \n",
            " flatten_67 (Flatten)        (None, 4096)              0         \n",
            "                                                                 \n",
            " dense_67 (Dense)            (None, 10)                40970     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 411,786\n",
            "Trainable params: 411,786\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 2.3034 - accuracy: 0.1415 - val_loss: 2.1747 - val_accuracy: 0.2098 - lr: 1.0000e-04\n",
            "Epoch 2/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 2.1672 - accuracy: 0.2042 - val_loss: 2.0919 - val_accuracy: 0.2445 - lr: 1.0000e-04\n",
            "Epoch 3/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 2.0923 - accuracy: 0.2339 - val_loss: 2.0279 - val_accuracy: 0.2839 - lr: 1.0000e-04\n",
            "Epoch 4/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 2.0452 - accuracy: 0.2567 - val_loss: 1.9846 - val_accuracy: 0.3054 - lr: 1.0000e-04\n",
            "Epoch 5/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 2.0051 - accuracy: 0.2732 - val_loss: 1.9501 - val_accuracy: 0.3155 - lr: 1.0000e-04\n",
            "Epoch 6/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.9715 - accuracy: 0.2890 - val_loss: 1.9203 - val_accuracy: 0.3307 - lr: 1.0000e-04\n",
            "Epoch 7/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.9465 - accuracy: 0.2995 - val_loss: 1.8944 - val_accuracy: 0.3384 - lr: 1.0000e-04\n",
            "Epoch 8/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.9181 - accuracy: 0.3131 - val_loss: 1.8723 - val_accuracy: 0.3439 - lr: 1.0000e-04\n",
            "Epoch 9/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.8964 - accuracy: 0.3241 - val_loss: 1.8491 - val_accuracy: 0.3551 - lr: 1.0000e-04\n",
            "Epoch 10/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.8799 - accuracy: 0.3295 - val_loss: 1.8297 - val_accuracy: 0.3655 - lr: 1.0000e-04\n",
            "Epoch 11/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.8600 - accuracy: 0.3395 - val_loss: 1.8071 - val_accuracy: 0.3773 - lr: 1.0000e-04\n",
            "Epoch 12/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.8427 - accuracy: 0.3472 - val_loss: 1.7929 - val_accuracy: 0.3825 - lr: 1.0000e-04\n",
            "Epoch 13/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.8283 - accuracy: 0.3523 - val_loss: 1.7750 - val_accuracy: 0.3854 - lr: 1.0000e-04\n",
            "Epoch 14/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.8122 - accuracy: 0.3568 - val_loss: 1.7640 - val_accuracy: 0.3893 - lr: 1.0000e-04\n",
            "Epoch 15/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.8001 - accuracy: 0.3570 - val_loss: 1.7468 - val_accuracy: 0.3962 - lr: 1.0000e-04\n",
            "Epoch 16/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.7868 - accuracy: 0.3659 - val_loss: 1.7378 - val_accuracy: 0.3966 - lr: 9.0484e-05\n",
            "Epoch 17/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.7748 - accuracy: 0.3702 - val_loss: 1.7252 - val_accuracy: 0.3979 - lr: 8.1873e-05\n",
            "Epoch 18/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.7636 - accuracy: 0.3760 - val_loss: 1.7168 - val_accuracy: 0.4039 - lr: 7.4082e-05\n",
            "Epoch 19/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.7580 - accuracy: 0.3776 - val_loss: 1.7098 - val_accuracy: 0.4014 - lr: 6.7032e-05\n",
            "Epoch 20/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.7495 - accuracy: 0.3779 - val_loss: 1.7030 - val_accuracy: 0.4030 - lr: 6.0653e-05\n",
            "Epoch 21/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.7446 - accuracy: 0.3800 - val_loss: 1.7039 - val_accuracy: 0.4075 - lr: 5.4881e-05\n",
            "Epoch 22/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.7370 - accuracy: 0.3835 - val_loss: 1.6942 - val_accuracy: 0.4060 - lr: 4.9659e-05\n",
            "Epoch 23/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.7340 - accuracy: 0.3854 - val_loss: 1.6890 - val_accuracy: 0.4066 - lr: 4.4933e-05\n",
            "Epoch 24/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 1.7282 - accuracy: 0.3867 - val_loss: 1.6846 - val_accuracy: 0.4111 - lr: 4.0657e-05\n",
            "Epoch 25/30\n",
            " 374/1250 [=======>......................] - ETA: 13s - loss: 1.7280 - accuracy: 0.3895"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_22076/3957168892.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'Training for fold {fold_no} ...'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m     history = model.train(\n\u001b[0m\u001b[0;32m     21\u001b[0m               \u001b[0mtrain_gen\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m                 \u001b[0mtrainX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_22076/1534972907.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, train_gen, train_x, train_y, test_x, test_y, lr_rate, lr_rate_type)\u001b[0m\n\u001b[0;32m    161\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_gen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtensorboard_callback\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcallback\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 163\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_gen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtensorboard_callback\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcallback\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m#  self.model.fit(train_x, train_y, validation_data=(test_x, test_y) , epochs=25, verbose=1)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    164\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_gen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtensorboard_callback\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1382\u001b[0m                 _r=1):\n\u001b[0;32m   1383\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1384\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1385\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1386\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    913\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    914\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 915\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    945\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    946\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 947\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    948\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2954\u001b[0m       (graph_function,\n\u001b[0;32m   2955\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2956\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   2957\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   2958\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1851\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1852\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1853\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1854\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1855\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    498\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 499\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    500\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    501\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### SGD: Learning rate 1e-4 Momentum 0.6"
      ],
      "metadata": {
        "id": "lhWE9BFj6seB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "acc_per_fold = []\n",
        "loss_per_fold = []\n",
        "\n",
        "with tf.device('/device:GPU:0'):\n",
        "  fold_no = 1\n",
        "  for train, valid in kfold.split(trainX, trainy):\n",
        "\n",
        "    model = CNN_Model(batch_size=32, depth=3, hidden_neurons_1=64, hidden_neurons_2=128, hidden_neurons_3=256, kernel_size_1=3, kernel_size_2=3, kernel_size_3=3, \n",
        "               dropout_rate=0.2, batch_norm=False, pooling='max', layer_activation_1='relu', layer_activation_2='relu', layer_activation_3='relu',\n",
        "               kernel_initialiser=tf.keras.initializers.HeNormal(), bias_initialiser='zeros', optimiser='SGD', weight_decay=None, learning_rate=1e-4, output_activation='softmax', SGD_momentum=0.6) \n",
        "  \n",
        "    train_gen = tf.keras.preprocessing.image.ImageDataGenerator(width_shift_range=0.1,\n",
        "      height_shift_range=0.1,\n",
        "      horizontal_flip=True)\n",
        "    train_gen.fit(trainX[train]) #Data augmentation\n",
        "  \n",
        "    print('------------------------------------------------------------------------')\n",
        "    print(f'Training for fold {fold_no} ...')\n",
        "\n",
        "    history = model.train(\n",
        "              train_gen,\n",
        "                trainX[train],\n",
        "                  trainy[train],\n",
        "                trainX[valid],\n",
        "                  trainy[valid],\n",
        "                  lr_rate_type='step')\n",
        "\n",
        "    scores = model.evaluate(trainX[valid], trainy[valid])\n",
        "\n",
        "    print(f'Score for fold {fold_no}: loss of {scores[0]}; accuracy of {scores[1]*100}%')\n",
        "    acc_per_fold.append(scores[1] * 100)\n",
        "    loss_per_fold.append(scores[0])\n",
        "\n",
        "    fold_no+=1\n",
        "\n",
        "# == Provide average scores ==\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Score per fold')\n",
        "for i in range(0, len(acc_per_fold)):\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Average scores for all folds:')\n",
        "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
        "print(f'> Loss: {np.mean(loss_per_fold)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-33an5UP6uFi",
        "outputId": "4a892158-b2e5-488a-9bbb-e51c913983ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_68\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_204 (Conv2D)         (None, 32, 32, 64)        1792      \n",
            "                                                                 \n",
            " max_pooling2d_68 (MaxPoolin  multiple                 0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_205 (Conv2D)         (None, 16, 16, 128)       73856     \n",
            "                                                                 \n",
            " dropout_68 (Dropout)        (None, 8, 8, 128)         0         \n",
            "                                                                 \n",
            " conv2d_206 (Conv2D)         (None, 8, 8, 256)         295168    \n",
            "                                                                 \n",
            " flatten_68 (Flatten)        (None, 4096)              0         \n",
            "                                                                 \n",
            " dense_68 (Dense)            (None, 10)                40970     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 411,786\n",
            "Trainable params: 411,786\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 2.2335 - accuracy: 0.1772 - val_loss: 2.0673 - val_accuracy: 0.2660 - lr: 1.0000e-04\n",
            "Epoch 2/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 2.0645 - accuracy: 0.2488 - val_loss: 1.9589 - val_accuracy: 0.3142 - lr: 1.0000e-04\n",
            "Epoch 3/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 1.9850 - accuracy: 0.2824 - val_loss: 1.8905 - val_accuracy: 0.3448 - lr: 1.0000e-04\n",
            "Epoch 4/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 1.9265 - accuracy: 0.3099 - val_loss: 1.8420 - val_accuracy: 0.3688 - lr: 1.0000e-04\n",
            "Epoch 5/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.8847 - accuracy: 0.3283 - val_loss: 1.8094 - val_accuracy: 0.3710 - lr: 1.0000e-04\n",
            "Epoch 6/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 1.8474 - accuracy: 0.3451 - val_loss: 1.7660 - val_accuracy: 0.3850 - lr: 1.0000e-04\n",
            "Epoch 7/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.8167 - accuracy: 0.3573 - val_loss: 1.7346 - val_accuracy: 0.3994 - lr: 1.0000e-04\n",
            "Epoch 8/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 1.7896 - accuracy: 0.3636 - val_loss: 1.7072 - val_accuracy: 0.4077 - lr: 1.0000e-04\n",
            "Epoch 9/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 1.7627 - accuracy: 0.3768 - val_loss: 1.6830 - val_accuracy: 0.4133 - lr: 1.0000e-04\n",
            "Epoch 10/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.7400 - accuracy: 0.3871 - val_loss: 1.6592 - val_accuracy: 0.4223 - lr: 1.0000e-04\n",
            "Epoch 11/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.7183 - accuracy: 0.3929 - val_loss: 1.6421 - val_accuracy: 0.4276 - lr: 1.0000e-04\n",
            "Epoch 12/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.6995 - accuracy: 0.4008 - val_loss: 1.6292 - val_accuracy: 0.4302 - lr: 1.0000e-04\n",
            "Epoch 13/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.6772 - accuracy: 0.4059 - val_loss: 1.6077 - val_accuracy: 0.4385 - lr: 1.0000e-04\n",
            "Epoch 14/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 1.6644 - accuracy: 0.4118 - val_loss: 1.5839 - val_accuracy: 0.4466 - lr: 1.0000e-04\n",
            "Epoch 15/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.6444 - accuracy: 0.4184 - val_loss: 1.5740 - val_accuracy: 0.4492 - lr: 1.0000e-04\n",
            "Epoch 16/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.6334 - accuracy: 0.4211 - val_loss: 1.5655 - val_accuracy: 0.4476 - lr: 9.0484e-05\n",
            "Epoch 17/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.6206 - accuracy: 0.4277 - val_loss: 1.5507 - val_accuracy: 0.4549 - lr: 8.1873e-05\n",
            "Epoch 18/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.6097 - accuracy: 0.4331 - val_loss: 1.5628 - val_accuracy: 0.4476 - lr: 7.4082e-05\n",
            "Epoch 19/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.5999 - accuracy: 0.4353 - val_loss: 1.5351 - val_accuracy: 0.4589 - lr: 6.7032e-05\n",
            "Epoch 20/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.5912 - accuracy: 0.4367 - val_loss: 1.5248 - val_accuracy: 0.4628 - lr: 6.0653e-05\n",
            "Epoch 21/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.5816 - accuracy: 0.4381 - val_loss: 1.5279 - val_accuracy: 0.4605 - lr: 5.4881e-05\n",
            "Epoch 22/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.5752 - accuracy: 0.4424 - val_loss: 1.5204 - val_accuracy: 0.4631 - lr: 4.9659e-05\n",
            "Epoch 23/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.5698 - accuracy: 0.4430 - val_loss: 1.5181 - val_accuracy: 0.4624 - lr: 4.4933e-05\n",
            "Epoch 24/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.5644 - accuracy: 0.4439 - val_loss: 1.5099 - val_accuracy: 0.4694 - lr: 4.0657e-05\n",
            "Epoch 25/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.5604 - accuracy: 0.4489 - val_loss: 1.5164 - val_accuracy: 0.4655 - lr: 3.6788e-05\n",
            "Epoch 26/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.5586 - accuracy: 0.4457 - val_loss: 1.5097 - val_accuracy: 0.4664 - lr: 3.3287e-05\n",
            "Epoch 27/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.5527 - accuracy: 0.4497 - val_loss: 1.5130 - val_accuracy: 0.4664 - lr: 3.0119e-05\n",
            "Epoch 28/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 1.5515 - accuracy: 0.4498 - val_loss: 1.5111 - val_accuracy: 0.4667 - lr: 2.7253e-05\n",
            "Epoch 29/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.5492 - accuracy: 0.4513 - val_loss: 1.4921 - val_accuracy: 0.4729 - lr: 2.4660e-05\n",
            "Epoch 30/30\n",
            "  24/1250 [..............................] - ETA: 15s - loss: 1.5240 - accuracy: 0.4622"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_22076/754624681.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'Training for fold {fold_no} ...'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m     history = model.train(\n\u001b[0m\u001b[0;32m     21\u001b[0m               \u001b[0mtrain_gen\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m                 \u001b[0mtrainX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_22076/1534972907.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, train_gen, train_x, train_y, test_x, test_y, lr_rate, lr_rate_type)\u001b[0m\n\u001b[0;32m    161\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_gen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtensorboard_callback\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcallback\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 163\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_gen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtensorboard_callback\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcallback\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m#  self.model.fit(train_x, train_y, validation_data=(test_x, test_y) , epochs=25, verbose=1)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    164\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_gen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtensorboard_callback\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1382\u001b[0m                 _r=1):\n\u001b[0;32m   1383\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1384\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1385\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1386\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    913\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    914\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 915\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    945\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    946\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 947\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    948\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2954\u001b[0m       (graph_function,\n\u001b[0;32m   2955\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2956\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   2957\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   2958\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1851\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1852\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1853\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1854\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1855\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    498\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 499\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    500\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    501\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### SGD: Learning rate 1e-4 Momentum 0.9"
      ],
      "metadata": {
        "id": "zGJe-XzJ6ulj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "acc_per_fold = []\n",
        "loss_per_fold = []\n",
        "\n",
        "with tf.device('/device:GPU:0'):\n",
        "  fold_no = 1\n",
        "  for train, valid in kfold.split(trainX, trainy):\n",
        "\n",
        "    model = CNN_Model(batch_size=32, depth=3, hidden_neurons_1=64, hidden_neurons_2=128, hidden_neurons_3=256, kernel_size_1=3, kernel_size_2=3, kernel_size_3=3, \n",
        "               dropout_rate=0.2, batch_norm=False, pooling='max', layer_activation_1='relu', layer_activation_2='relu', layer_activation_3='relu',\n",
        "               kernel_initialiser=tf.keras.initializers.HeNormal(), bias_initialiser='zeros', optimiser='SGD', weight_decay=None, learning_rate=1e-4, output_activation='softmax', SGD_momentum=0.9) \n",
        "  \n",
        "    train_gen = tf.keras.preprocessing.image.ImageDataGenerator(width_shift_range=0.1,\n",
        "      height_shift_range=0.1,\n",
        "      horizontal_flip=True)\n",
        "    train_gen.fit(trainX[train]) #Data augmentation\n",
        "  \n",
        "    print('------------------------------------------------------------------------')\n",
        "    print(f'Training for fold {fold_no} ...')\n",
        "\n",
        "    history = model.train(\n",
        "              train_gen,\n",
        "                trainX[train],\n",
        "                  trainy[train],\n",
        "                trainX[valid],\n",
        "                  trainy[valid],\n",
        "                  lr_rate_type='step') \n",
        "\n",
        "    scores = model.evaluate(trainX[valid], trainy[valid])\n",
        "\n",
        "    print(f'Score for fold {fold_no}: loss of {scores[0]}; accuracy of {scores[1]*100}%')\n",
        "    acc_per_fold.append(scores[1] * 100)\n",
        "    loss_per_fold.append(scores[0])\n",
        "\n",
        "    fold_no+=1\n",
        "\n",
        "# == Provide average scores ==\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Score per fold')\n",
        "for i in range(0, len(acc_per_fold)):\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Average scores for all folds:')\n",
        "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
        "print(f'> Loss: {np.mean(loss_per_fold)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "PKyntjYP6wgI",
        "outputId": "9bebe86f-f3dc-4e4e-e129-be3d4802a3dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_69\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_207 (Conv2D)         (None, 32, 32, 64)        1792      \n",
            "                                                                 \n",
            " max_pooling2d_69 (MaxPoolin  multiple                 0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_208 (Conv2D)         (None, 16, 16, 128)       73856     \n",
            "                                                                 \n",
            " dropout_69 (Dropout)        (None, 8, 8, 128)         0         \n",
            "                                                                 \n",
            " conv2d_209 (Conv2D)         (None, 8, 8, 256)         295168    \n",
            "                                                                 \n",
            " flatten_69 (Flatten)        (None, 4096)              0         \n",
            "                                                                 \n",
            " dense_69 (Dense)            (None, 10)                40970     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 411,786\n",
            "Trainable params: 411,786\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 2.0827 - accuracy: 0.2438 - val_loss: 1.8906 - val_accuracy: 0.3436 - lr: 1.0000e-04\n",
            "Epoch 2/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.8700 - accuracy: 0.3371 - val_loss: 1.7639 - val_accuracy: 0.3873 - lr: 1.0000e-04\n",
            "Epoch 3/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.7646 - accuracy: 0.3769 - val_loss: 1.6578 - val_accuracy: 0.4223 - lr: 1.0000e-04\n",
            "Epoch 4/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.6926 - accuracy: 0.4001 - val_loss: 1.5974 - val_accuracy: 0.4361 - lr: 1.0000e-04\n",
            "Epoch 5/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.6346 - accuracy: 0.4209 - val_loss: 1.5467 - val_accuracy: 0.4537 - lr: 1.0000e-04\n",
            "Epoch 6/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 1.5888 - accuracy: 0.4378 - val_loss: 1.5176 - val_accuracy: 0.4647 - lr: 1.0000e-04\n",
            "Epoch 7/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 1.5492 - accuracy: 0.4536 - val_loss: 1.5761 - val_accuracy: 0.4389 - lr: 1.0000e-04\n",
            "Epoch 8/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.5214 - accuracy: 0.4597 - val_loss: 1.4452 - val_accuracy: 0.4855 - lr: 1.0000e-04\n",
            "Epoch 9/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.4893 - accuracy: 0.4714 - val_loss: 1.4388 - val_accuracy: 0.4855 - lr: 1.0000e-04\n",
            "Epoch 10/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.4650 - accuracy: 0.4779 - val_loss: 1.4034 - val_accuracy: 0.5021 - lr: 1.0000e-04\n",
            "Epoch 11/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.4448 - accuracy: 0.4890 - val_loss: 1.4224 - val_accuracy: 0.4923 - lr: 1.0000e-04\n",
            "Epoch 12/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.4305 - accuracy: 0.4929 - val_loss: 1.3456 - val_accuracy: 0.5251 - lr: 1.0000e-04\n",
            "Epoch 13/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.4118 - accuracy: 0.5031 - val_loss: 1.3454 - val_accuracy: 0.5218 - lr: 1.0000e-04\n",
            "Epoch 14/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 1.3988 - accuracy: 0.5065 - val_loss: 1.3354 - val_accuracy: 0.5282 - lr: 1.0000e-04\n",
            "Epoch 15/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.3867 - accuracy: 0.5084 - val_loss: 1.3006 - val_accuracy: 0.5420 - lr: 1.0000e-04\n",
            "Epoch 16/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.3658 - accuracy: 0.5172 - val_loss: 1.3863 - val_accuracy: 0.5098 - lr: 9.0484e-05\n",
            "Epoch 17/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.3559 - accuracy: 0.5238 - val_loss: 1.3874 - val_accuracy: 0.5079 - lr: 8.1873e-05\n",
            "Epoch 18/30\n",
            "1175/1250 [===========================>..] - ETA: 0s - loss: 1.3450 - accuracy: 0.5271"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_22076/2103438578.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'Training for fold {fold_no} ...'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m     history = model.train(\n\u001b[0m\u001b[0;32m     21\u001b[0m               \u001b[0mtrain_gen\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m                 \u001b[0mtrainX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_22076/1534972907.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, train_gen, train_x, train_y, test_x, test_y, lr_rate, lr_rate_type)\u001b[0m\n\u001b[0;32m    161\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_gen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtensorboard_callback\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcallback\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 163\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_gen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtensorboard_callback\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcallback\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m#  self.model.fit(train_x, train_y, validation_data=(test_x, test_y) , epochs=25, verbose=1)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    164\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_gen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtensorboard_callback\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1382\u001b[0m                 _r=1):\n\u001b[0;32m   1383\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1384\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1385\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1386\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    913\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    914\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 915\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    945\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    946\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 947\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    948\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2954\u001b[0m       (graph_function,\n\u001b[0;32m   2955\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2956\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   2957\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   2958\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1851\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1852\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1853\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1854\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1855\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    498\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 499\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    500\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    501\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### SGD no momentum"
      ],
      "metadata": {
        "id": "39A-JNC7FdNx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "acc_per_fold = []\n",
        "loss_per_fold = []\n",
        "\n",
        "with tf.device('/device:GPU:0'):\n",
        "  fold_no = 1\n",
        "  for train, valid in kfold.split(trainX, trainy):\n",
        "\n",
        "    model = CNN_Model(batch_size=32, depth=3, hidden_neurons_1=64, hidden_neurons_2=128, hidden_neurons_3=256, kernel_size_1=3, kernel_size_2=3, kernel_size_3=3, \n",
        "               dropout_rate=0.2, batch_norm=False, pooling='max', layer_activation_1='relu', layer_activation_2='relu', layer_activation_3='relu',\n",
        "               kernel_initialiser='glorot_uniform', bias_initialiser='zeros', optimiser='SGD', weight_decay=None, learning_rate=1e-3, output_activation='softmax', SGD_momentum=0) \n",
        "  \n",
        "    train_gen = tf.keras.preprocessing.image.ImageDataGenerator(width_shift_range=0.1,\n",
        "      height_shift_range=0.1,\n",
        "      horizontal_flip=True)\n",
        "    train_gen.fit(trainX[train]) #Data augmentation\n",
        "  \n",
        "    print('------------------------------------------------------------------------')\n",
        "    print(f'Training for fold {fold_no} ...')\n",
        "\n",
        "    history = model.train(\n",
        "              train_gen,\n",
        "                trainX[train],\n",
        "                  trainy[train],\n",
        "                trainX[valid],\n",
        "                  trainy[valid],\n",
        "                  lr_rate_type='step')\n",
        "\n",
        "    scores = model.evaluate(trainX[valid], trainy[valid])\n",
        "\n",
        "    print(f'Score for fold {fold_no}: loss of {scores[0]}; accuracy of {scores[1]*100}%')\n",
        "    acc_per_fold.append(scores[1] * 100)\n",
        "    loss_per_fold.append(scores[0])\n",
        "\n",
        "    fold_no+=1\n",
        "\n",
        "# == Provide average scores ==\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Score per fold')\n",
        "for i in range(0, len(acc_per_fold)):\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Average scores for all folds:')\n",
        "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
        "print(f'> Loss: {np.mean(loss_per_fold)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "6VvCwJ72Fjhb",
        "outputId": "9d1e28af-206b-4d18-aa1b-f564d6ffee2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_55\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_165 (Conv2D)         (None, 32, 32, 64)        1792      \n",
            "                                                                 \n",
            " max_pooling2d_55 (MaxPoolin  multiple                 0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_166 (Conv2D)         (None, 16, 16, 128)       73856     \n",
            "                                                                 \n",
            " dropout_55 (Dropout)        (None, 8, 8, 128)         0         \n",
            "                                                                 \n",
            " conv2d_167 (Conv2D)         (None, 8, 8, 256)         295168    \n",
            "                                                                 \n",
            " flatten_55 (Flatten)        (None, 4096)              0         \n",
            "                                                                 \n",
            " dense_55 (Dense)            (None, 10)                40970     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 411,786\n",
            "Trainable params: 411,786\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 2.2966 - accuracy: 0.1120 - val_loss: 2.2896 - val_accuracy: 0.1110 - lr: 0.0010\n",
            "Epoch 2/30\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 2.2792 - accuracy: 0.1479 - val_loss: 2.2661 - val_accuracy: 0.1766 - lr: 0.0010\n",
            "Epoch 3/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 2.2412 - accuracy: 0.1961 - val_loss: 2.2086 - val_accuracy: 0.2266 - lr: 0.0010\n",
            "Epoch 4/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 2.1518 - accuracy: 0.2412 - val_loss: 2.0861 - val_accuracy: 0.2711 - lr: 0.0010\n",
            "Epoch 5/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 2.0439 - accuracy: 0.2704 - val_loss: 2.0052 - val_accuracy: 0.2860 - lr: 0.0010\n",
            "Epoch 6/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 1.9935 - accuracy: 0.2873 - val_loss: 1.9710 - val_accuracy: 0.2917 - lr: 0.0010\n",
            "Epoch 7/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 1.9645 - accuracy: 0.3036 - val_loss: 1.9405 - val_accuracy: 0.3112 - lr: 0.0010\n",
            "Epoch 8/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 1.9378 - accuracy: 0.3144 - val_loss: 1.9122 - val_accuracy: 0.3324 - lr: 0.0010\n",
            "Epoch 9/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.9070 - accuracy: 0.3303 - val_loss: 1.8675 - val_accuracy: 0.3481 - lr: 0.0010\n",
            "Epoch 10/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.8740 - accuracy: 0.3431 - val_loss: 1.8214 - val_accuracy: 0.3636 - lr: 0.0010\n",
            "Epoch 11/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.8342 - accuracy: 0.3557 - val_loss: 1.8015 - val_accuracy: 0.3699 - lr: 0.0010\n",
            "Epoch 12/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 1.7939 - accuracy: 0.3708 - val_loss: 1.7435 - val_accuracy: 0.3874 - lr: 0.0010\n",
            "Epoch 13/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.7582 - accuracy: 0.3805 - val_loss: 1.7024 - val_accuracy: 0.3998 - lr: 0.0010\n",
            "Epoch 14/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 1.7264 - accuracy: 0.3885 - val_loss: 1.6626 - val_accuracy: 0.4062 - lr: 0.0010\n",
            "Epoch 15/30\n",
            " 280/1250 [=====>........................] - ETA: 14s - loss: 1.7123 - accuracy: 0.3881"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_22076/426610261.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'Training for fold {fold_no} ...'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m     history = model.train(\n\u001b[0m\u001b[0;32m     21\u001b[0m               \u001b[0mtrain_gen\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m                 \u001b[0mtrainX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_22076/1534972907.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, train_gen, train_x, train_y, test_x, test_y, lr_rate, lr_rate_type)\u001b[0m\n\u001b[0;32m    161\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_gen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtensorboard_callback\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcallback\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 163\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_gen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtensorboard_callback\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcallback\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m#  self.model.fit(train_x, train_y, validation_data=(test_x, test_y) , epochs=25, verbose=1)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    164\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_gen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtensorboard_callback\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1382\u001b[0m                 _r=1):\n\u001b[0;32m   1383\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1384\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1385\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1386\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    913\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    914\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 915\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    945\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    946\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 947\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    948\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2954\u001b[0m       (graph_function,\n\u001b[0;32m   2955\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2956\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   2957\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   2958\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1851\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1852\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1853\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1854\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1855\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    498\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 499\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    500\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    501\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### RMSProp"
      ],
      "metadata": {
        "id": "_Je8Y0OR6N9R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "acc_per_fold = []\n",
        "loss_per_fold = []\n",
        "\n",
        "with tf.device('/device:GPU:0'):\n",
        "  fold_no = 1\n",
        "  for train, valid in kfold.split(trainX, trainy):\n",
        "\n",
        "    model = CNN_Model(batch_size=32, depth=3, hidden_neurons_1=64, hidden_neurons_2=128, hidden_neurons_3=256, kernel_size_1=3, kernel_size_2=3, kernel_size_3=3, \n",
        "               dropout_rate=0.2, batch_norm=False, pooling='max', layer_activation_1='relu', layer_activation_2='relu', layer_activation_3='relu',\n",
        "               kernel_initialiser='glorot_uniform', bias_initialiser='zeros', optimiser='RMSprop', weight_decay=None, learning_rate=1e-3, output_activation='softmax', SGD_momentum=0)\n",
        "  \n",
        "    train_gen = tf.keras.preprocessing.image.ImageDataGenerator(width_shift_range=0.1,\n",
        "      height_shift_range=0.1,\n",
        "      horizontal_flip=True)\n",
        "    train_gen.fit(trainX[train])  #Data augmentation\n",
        "  \n",
        "    print('------------------------------------------------------------------------')\n",
        "    print(f'Training for fold {fold_no} ...')\n",
        "\n",
        "    history = model.train(\n",
        "              train_gen,\n",
        "                trainX[train],\n",
        "                  trainy[train],\n",
        "                trainX[valid],\n",
        "                  trainy[valid],\n",
        "                  lr_rate_type='step')\n",
        "\n",
        "    scores = model.evaluate(trainX[valid], trainy[valid])\n",
        "\n",
        "    print(f'Score for fold {fold_no}: loss of {scores[0]}; accuracy of {scores[1]*100}%')\n",
        "    acc_per_fold.append(scores[1] * 100)\n",
        "    loss_per_fold.append(scores[0])\n",
        "\n",
        "    fold_no+=1\n",
        "\n",
        "# == Provide average scores ==\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Score per fold')\n",
        "for i in range(0, len(acc_per_fold)):\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Average scores for all folds:')\n",
        "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
        "print(f'> Loss: {np.mean(loss_per_fold)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "28cH06P2IZdC",
        "outputId": "0977550e-d62f-438e-d7af-3da3af3a5d84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_39\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_122 (Conv2D)         (None, 32, 32, 32)        896       \n",
            "                                                                 \n",
            " max_pooling2d_34 (MaxPoolin  multiple                 0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_123 (Conv2D)         (None, 16, 16, 64)        18496     \n",
            "                                                                 \n",
            " dropout_39 (Dropout)        (None, 8, 8, 64)          0         \n",
            "                                                                 \n",
            " conv2d_124 (Conv2D)         (None, 8, 8, 128)         73856     \n",
            "                                                                 \n",
            " flatten_39 (Flatten)        (None, 2048)              0         \n",
            "                                                                 \n",
            " dense_39 (Dense)            (None, 10)                20490     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 113,738\n",
            "Trainable params: 113,738\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/25\n",
            "1250/1250 [==============================] - 21s 16ms/step - loss: 1.6267 - accuracy: 0.4114 - val_loss: 1.3337 - val_accuracy: 0.5234 - lr: 0.0010\n",
            "Epoch 2/25\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.2760 - accuracy: 0.5509 - val_loss: 1.0387 - val_accuracy: 0.6359 - lr: 0.0010\n",
            "Epoch 3/25\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 1.1187 - accuracy: 0.6100 - val_loss: 0.9824 - val_accuracy: 0.6606 - lr: 0.0010\n",
            "Epoch 4/25\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.0220 - accuracy: 0.6456 - val_loss: 0.9889 - val_accuracy: 0.6670 - lr: 0.0010\n",
            "Epoch 5/25\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.9590 - accuracy: 0.6663 - val_loss: 0.9295 - val_accuracy: 0.6867 - lr: 0.0010\n",
            "Epoch 6/25\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.9152 - accuracy: 0.6831 - val_loss: 0.8687 - val_accuracy: 0.7025 - lr: 0.0010\n",
            "Epoch 7/25\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 0.8778 - accuracy: 0.6972 - val_loss: 0.7875 - val_accuracy: 0.7326 - lr: 0.0010\n",
            "Epoch 8/25\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.8549 - accuracy: 0.7070 - val_loss: 0.7551 - val_accuracy: 0.7381 - lr: 0.0010\n",
            "Epoch 9/25\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.8241 - accuracy: 0.7167 - val_loss: 0.7451 - val_accuracy: 0.7504 - lr: 0.0010\n",
            "Epoch 10/25\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 0.8080 - accuracy: 0.7225 - val_loss: 0.8005 - val_accuracy: 0.7269 - lr: 0.0010\n",
            "Epoch 11/25\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 0.7800 - accuracy: 0.7320 - val_loss: 0.7275 - val_accuracy: 0.7542 - lr: 9.0484e-04\n",
            "Epoch 12/25\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.7545 - accuracy: 0.7410 - val_loss: 0.7018 - val_accuracy: 0.7651 - lr: 8.1873e-04\n",
            "Epoch 13/25\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 0.7300 - accuracy: 0.7502 - val_loss: 0.7249 - val_accuracy: 0.7629 - lr: 7.4082e-04\n",
            "Epoch 14/25\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.7043 - accuracy: 0.7585 - val_loss: 0.7010 - val_accuracy: 0.7643 - lr: 6.7032e-04\n",
            "Epoch 15/25\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.6872 - accuracy: 0.7635 - val_loss: 0.6832 - val_accuracy: 0.7723 - lr: 6.0653e-04\n",
            "Epoch 16/25\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.6705 - accuracy: 0.7700 - val_loss: 0.7168 - val_accuracy: 0.7653 - lr: 5.4881e-04\n",
            "Epoch 17/25\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.6515 - accuracy: 0.7765 - val_loss: 0.7015 - val_accuracy: 0.7642 - lr: 4.9659e-04\n",
            "Epoch 18/25\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.6428 - accuracy: 0.7811 - val_loss: 0.6309 - val_accuracy: 0.7902 - lr: 4.4933e-04\n",
            "Epoch 19/25\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.6251 - accuracy: 0.7855 - val_loss: 0.6757 - val_accuracy: 0.7716 - lr: 4.0657e-04\n",
            "Epoch 20/25\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.6123 - accuracy: 0.7897 - val_loss: 0.6617 - val_accuracy: 0.7807 - lr: 3.6788e-04\n",
            "Epoch 21/25\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.6025 - accuracy: 0.7947 - val_loss: 0.6225 - val_accuracy: 0.7870 - lr: 3.3287e-04\n",
            "Epoch 22/25\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.5914 - accuracy: 0.7966 - val_loss: 0.5972 - val_accuracy: 0.8003 - lr: 3.0119e-04\n",
            "Epoch 23/25\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.5853 - accuracy: 0.7999 - val_loss: 0.6360 - val_accuracy: 0.7866 - lr: 2.7253e-04\n",
            "Epoch 24/25\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.5740 - accuracy: 0.8042 - val_loss: 0.5965 - val_accuracy: 0.8041 - lr: 2.4660e-04\n",
            "Epoch 25/25\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.5710 - accuracy: 0.8053 - val_loss: 0.6061 - val_accuracy: 0.7952 - lr: 2.2313e-04\n",
            "Score for fold 1: loss of 0.6060677170753479; accuracy of 79.5199990272522%\n",
            "Model: \"sequential_40\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_125 (Conv2D)         (None, 32, 32, 32)        896       \n",
            "                                                                 \n",
            " max_pooling2d_35 (MaxPoolin  multiple                 0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_126 (Conv2D)         (None, 16, 16, 64)        18496     \n",
            "                                                                 \n",
            " dropout_40 (Dropout)        (None, 8, 8, 64)          0         \n",
            "                                                                 \n",
            " conv2d_127 (Conv2D)         (None, 8, 8, 128)         73856     \n",
            "                                                                 \n",
            " flatten_40 (Flatten)        (None, 2048)              0         \n",
            "                                                                 \n",
            " dense_40 (Dense)            (None, 10)                20490     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 113,738\n",
            "Trainable params: 113,738\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/25\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.6365 - accuracy: 0.4069 - val_loss: 1.3543 - val_accuracy: 0.5203 - lr: 0.0010\n",
            "Epoch 2/25\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.2735 - accuracy: 0.5501 - val_loss: 1.0877 - val_accuracy: 0.6135 - lr: 0.0010\n",
            "Epoch 3/25\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 1.1193 - accuracy: 0.6095 - val_loss: 0.9709 - val_accuracy: 0.6694 - lr: 0.0010\n",
            "Epoch 4/25\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.0282 - accuracy: 0.6472 - val_loss: 0.9399 - val_accuracy: 0.6825 - lr: 0.0010\n",
            "Epoch 5/25\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.9643 - accuracy: 0.6677 - val_loss: 0.8678 - val_accuracy: 0.7048 - lr: 0.0010\n",
            "Epoch 6/25\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.9227 - accuracy: 0.6809 - val_loss: 0.8305 - val_accuracy: 0.7208 - lr: 0.0010\n",
            "Epoch 7/25\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.8935 - accuracy: 0.6905 - val_loss: 0.8318 - val_accuracy: 0.7231 - lr: 0.0010\n",
            "Epoch 8/25\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.8648 - accuracy: 0.7013 - val_loss: 0.7860 - val_accuracy: 0.7298 - lr: 0.0010\n",
            "Epoch 9/25\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.8421 - accuracy: 0.7114 - val_loss: 0.8559 - val_accuracy: 0.7109 - lr: 0.0010\n",
            "Epoch 10/25\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.8218 - accuracy: 0.7162 - val_loss: 0.8003 - val_accuracy: 0.7245 - lr: 0.0010\n",
            "Epoch 11/25\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.7929 - accuracy: 0.7286 - val_loss: 0.7600 - val_accuracy: 0.7483 - lr: 9.0484e-04\n",
            "Epoch 12/25\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.7701 - accuracy: 0.7361 - val_loss: 0.6917 - val_accuracy: 0.7704 - lr: 8.1873e-04\n",
            "Epoch 13/25\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.7401 - accuracy: 0.7480 - val_loss: 0.6725 - val_accuracy: 0.7757 - lr: 7.4082e-04\n",
            "Epoch 14/25\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.7215 - accuracy: 0.7502 - val_loss: 0.7336 - val_accuracy: 0.7592 - lr: 6.7032e-04\n",
            "Epoch 15/25\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.6999 - accuracy: 0.7591 - val_loss: 0.6864 - val_accuracy: 0.7705 - lr: 6.0653e-04\n",
            "Epoch 16/25\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.6864 - accuracy: 0.7661 - val_loss: 0.6401 - val_accuracy: 0.7883 - lr: 5.4881e-04\n",
            "Epoch 17/25\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.6737 - accuracy: 0.7681 - val_loss: 0.7219 - val_accuracy: 0.7584 - lr: 4.9659e-04\n",
            "Epoch 18/25\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.6493 - accuracy: 0.7753 - val_loss: 0.6658 - val_accuracy: 0.7762 - lr: 4.4933e-04\n",
            "Epoch 19/25\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 0.6385 - accuracy: 0.7805 - val_loss: 0.6409 - val_accuracy: 0.7827 - lr: 4.0657e-04\n",
            "Epoch 20/25\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.6250 - accuracy: 0.7844 - val_loss: 0.6463 - val_accuracy: 0.7841 - lr: 3.6788e-04\n",
            "Epoch 21/25\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.6148 - accuracy: 0.7881 - val_loss: 0.6320 - val_accuracy: 0.7924 - lr: 3.3287e-04\n",
            "Epoch 22/25\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.6068 - accuracy: 0.7917 - val_loss: 0.6111 - val_accuracy: 0.7915 - lr: 3.0119e-04\n",
            "Epoch 23/25\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.5995 - accuracy: 0.7943 - val_loss: 0.6137 - val_accuracy: 0.7950 - lr: 2.7253e-04\n",
            "Epoch 24/25\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.5854 - accuracy: 0.7991 - val_loss: 0.5835 - val_accuracy: 0.8039 - lr: 2.4660e-04\n",
            "Epoch 25/25\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.5848 - accuracy: 0.7971 - val_loss: 0.6041 - val_accuracy: 0.7970 - lr: 2.2313e-04\n",
            "Score for fold 2: loss of 0.6040621399879456; accuracy of 79.6999990940094%\n",
            "Model: \"sequential_41\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_128 (Conv2D)         (None, 32, 32, 32)        896       \n",
            "                                                                 \n",
            " max_pooling2d_36 (MaxPoolin  multiple                 0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_129 (Conv2D)         (None, 16, 16, 64)        18496     \n",
            "                                                                 \n",
            " dropout_41 (Dropout)        (None, 8, 8, 64)          0         \n",
            "                                                                 \n",
            " conv2d_130 (Conv2D)         (None, 8, 8, 128)         73856     \n",
            "                                                                 \n",
            " flatten_41 (Flatten)        (None, 2048)              0         \n",
            "                                                                 \n",
            " dense_41 (Dense)            (None, 10)                20490     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 113,738\n",
            "Trainable params: 113,738\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/25\n",
            " 999/1250 [======================>.......] - ETA: 3s - loss: 1.6729 - accuracy: 0.3924"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_20584/1085865555.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'Training for fold {fold_no} ...'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m     history = model.train(\n\u001b[0m\u001b[0;32m     21\u001b[0m               \u001b[0mtrain_gen\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m                 \u001b[0mtrainX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_20584/855518553.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, train_gen, train_x, train_y, test_x, test_y, lr_rate)\u001b[0m\n\u001b[0;32m    141\u001b[0m     \u001b[0msteps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlr_rate\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 143\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_gen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m25\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtensorboard_callback\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcallback\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m#  self.model.fit(train_x, train_y, validation_data=(test_x, test_y) , epochs=25, verbose=1)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    144\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_gen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m25\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtensorboard_callback\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1382\u001b[0m                 _r=1):\n\u001b[0;32m   1383\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1384\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1385\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1386\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    913\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    914\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 915\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    945\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    946\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 947\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    948\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2954\u001b[0m       (graph_function,\n\u001b[0;32m   2955\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2956\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   2957\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   2958\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1851\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1852\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1853\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1854\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1855\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    498\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 499\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    500\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    501\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comparing data augmentaion, cutout, mixup and cutmix"
      ],
      "metadata": {
        "id": "xmPASDyeZgom"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Data augmentation"
      ],
      "metadata": {
        "id": "RxuMMAG6YYXp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "acc_per_fold = []\n",
        "loss_per_fold = []\n",
        "\n",
        "with tf.device('/device:GPU:0'):\n",
        "  fold_no = 1\n",
        "  for train, valid in kfold.split(trainX, trainy):\n",
        "\n",
        "    model = CNN_Model(batch_size=32, depth=3, hidden_neurons_1=64, hidden_neurons_2=128, hidden_neurons_3=256, kernel_size_1=3, kernel_size_2=3, kernel_size_3=3, \n",
        "               dropout_rate=0.2, batch_norm=False, pooling='max', layer_activation_1='relu', layer_activation_2='relu', layer_activation_3='relu',\n",
        "               kernel_initialiser='he_uniform', bias_initialiser='zeros', optimiser='SGD', weight_decay=None, learning_rate=0.01, output_activation='softmax', SGD_momentum=0.9) \n",
        "  \n",
        "    train_gen = tf.keras.preprocessing.image.ImageDataGenerator(width_shift_range=0.1,\n",
        "      height_shift_range=0.1,\n",
        "      horizontal_flip=True)\n",
        "    train_gen.fit(trainX[train])\n",
        "  \n",
        "    print('------------------------------------------------------------------------')\n",
        "    print(f'Training for fold {fold_no} ...')\n",
        "\n",
        "    history = model.train(\n",
        "              train_gen,\n",
        "                trainX[train],\n",
        "                  trainy[train],\n",
        "                trainX[valid],\n",
        "                  trainy[valid],\n",
        "                  lr_rate_type='step')\n",
        "\n",
        "    scores = model.evaluate(trainX[valid], trainy[valid])\n",
        "\n",
        "    print(f'Score for fold {fold_no}: loss of {scores[0]}; accuracy of {scores[1]*100}%')\n",
        "    acc_per_fold.append(scores[1] * 100)\n",
        "    loss_per_fold.append(scores[0])\n",
        "\n",
        "    fold_no+=1\n",
        "\n",
        "# == Provide average scores ==\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Score per fold')\n",
        "for i in range(0, len(acc_per_fold)):\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Average scores for all folds:')\n",
        "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
        "print(f'> Loss: {np.mean(loss_per_fold)}')"
      ],
      "metadata": {
        "id": "7Eoehu_6ZmeO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "28a327f7-d9eb-40dc-e30b-e6b515537c68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_71\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_213 (Conv2D)         (None, 32, 32, 64)        1792      \n",
            "                                                                 \n",
            " max_pooling2d_71 (MaxPoolin  multiple                 0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_214 (Conv2D)         (None, 16, 16, 128)       73856     \n",
            "                                                                 \n",
            " dropout_71 (Dropout)        (None, 8, 8, 128)         0         \n",
            "                                                                 \n",
            " conv2d_215 (Conv2D)         (None, 8, 8, 256)         295168    \n",
            "                                                                 \n",
            " flatten_71 (Flatten)        (None, 4096)              0         \n",
            "                                                                 \n",
            " dense_71 (Dense)            (None, 10)                40970     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 411,786\n",
            "Trainable params: 411,786\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/30\n",
            "1250/1250 [==============================] - 19s 14ms/step - loss: 1.5864 - accuracy: 0.4285 - val_loss: 1.3140 - val_accuracy: 0.5354 - lr: 0.0100\n",
            "Epoch 2/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.2307 - accuracy: 0.5634 - val_loss: 1.0791 - val_accuracy: 0.6240 - lr: 0.0100\n",
            "Epoch 3/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.0968 - accuracy: 0.6181 - val_loss: 0.9717 - val_accuracy: 0.6665 - lr: 0.0100\n",
            "Epoch 4/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.0150 - accuracy: 0.6449 - val_loss: 0.8874 - val_accuracy: 0.6947 - lr: 0.0100\n",
            "Epoch 5/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.9454 - accuracy: 0.6721 - val_loss: 0.8541 - val_accuracy: 0.7091 - lr: 0.0100\n",
            "Epoch 6/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.9036 - accuracy: 0.6874 - val_loss: 0.8886 - val_accuracy: 0.6937 - lr: 0.0100\n",
            "Epoch 7/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.8681 - accuracy: 0.6985 - val_loss: 0.8011 - val_accuracy: 0.7221 - lr: 0.0100\n",
            "Epoch 8/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.8365 - accuracy: 0.7090 - val_loss: 0.7803 - val_accuracy: 0.7307 - lr: 0.0100\n",
            "Epoch 9/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.8110 - accuracy: 0.7197 - val_loss: 0.7724 - val_accuracy: 0.7356 - lr: 0.0100\n",
            "Epoch 10/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.7898 - accuracy: 0.7281 - val_loss: 0.7705 - val_accuracy: 0.7365 - lr: 0.0100\n",
            "Epoch 11/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.6741 - accuracy: 0.7669 - val_loss: 0.6617 - val_accuracy: 0.7738 - lr: 0.0050\n",
            "Epoch 12/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.6596 - accuracy: 0.7724 - val_loss: 0.6682 - val_accuracy: 0.7750 - lr: 0.0050\n",
            "Epoch 13/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.6383 - accuracy: 0.7796 - val_loss: 0.6379 - val_accuracy: 0.7843 - lr: 0.0050\n",
            "Epoch 14/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.6276 - accuracy: 0.7845 - val_loss: 0.6406 - val_accuracy: 0.7825 - lr: 0.0050\n",
            "Epoch 15/30\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: 0.6165 - accuracy: 0.7886 - val_loss: 0.6222 - val_accuracy: 0.7883 - lr: 0.0050\n",
            "Epoch 16/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.6008 - accuracy: 0.7934 - val_loss: 0.6041 - val_accuracy: 0.7955 - lr: 0.0050\n",
            "Epoch 17/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.5934 - accuracy: 0.7927 - val_loss: 0.6238 - val_accuracy: 0.7864 - lr: 0.0050\n",
            "Epoch 18/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.5858 - accuracy: 0.7972 - val_loss: 0.6238 - val_accuracy: 0.7903 - lr: 0.0050\n",
            "Epoch 19/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.5783 - accuracy: 0.8025 - val_loss: 0.6101 - val_accuracy: 0.7945 - lr: 0.0050\n",
            "Epoch 20/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.5702 - accuracy: 0.8018 - val_loss: 0.6038 - val_accuracy: 0.7937 - lr: 0.0050\n",
            "Epoch 21/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.5035 - accuracy: 0.8259 - val_loss: 0.5787 - val_accuracy: 0.8036 - lr: 0.0012\n",
            "Epoch 22/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.4869 - accuracy: 0.8296 - val_loss: 0.5568 - val_accuracy: 0.8123 - lr: 0.0012\n",
            "Epoch 23/30\n",
            "1250/1250 [==============================] - 18s 15ms/step - loss: 0.4708 - accuracy: 0.8361 - val_loss: 0.5496 - val_accuracy: 0.8144 - lr: 0.0012\n",
            "Epoch 24/30\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.4667 - accuracy: 0.8376 - val_loss: 0.5459 - val_accuracy: 0.8165 - lr: 0.0012\n",
            "Epoch 25/30\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.4680 - accuracy: 0.8364 - val_loss: 0.5388 - val_accuracy: 0.8170 - lr: 0.0012\n",
            "Epoch 26/30\n",
            " 423/1250 [=========>....................] - ETA: 10s - loss: 0.4502 - accuracy: 0.8437"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_22076/4193288170.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'Training for fold {fold_no} ...'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m     history = model.train(\n\u001b[0m\u001b[0;32m     21\u001b[0m               \u001b[0mtrain_gen\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m                 \u001b[0mtrainX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_22076/1534972907.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, train_gen, train_x, train_y, test_x, test_y, lr_rate, lr_rate_type)\u001b[0m\n\u001b[0;32m    161\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_gen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtensorboard_callback\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcallback\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 163\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_gen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtensorboard_callback\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcallback\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m#  self.model.fit(train_x, train_y, validation_data=(test_x, test_y) , epochs=25, verbose=1)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    164\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_gen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtensorboard_callback\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1382\u001b[0m                 _r=1):\n\u001b[0;32m   1383\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1384\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1385\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1386\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    913\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    914\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 915\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    945\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    946\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 947\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    948\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2954\u001b[0m       (graph_function,\n\u001b[0;32m   2955\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2956\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   2957\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   2958\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1851\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1852\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1853\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1854\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1855\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    498\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 499\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    500\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    501\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Cutout"
      ],
      "metadata": {
        "id": "runblX19Ycxu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "acc_per_fold = []\n",
        "loss_per_fold = []\n",
        "\n",
        "with tf.device('/device:GPU:0'):\n",
        "  fold_no = 1\n",
        "  for train, valid in kfold.split(trainX, trainy):\n",
        "\n",
        "    model = CNN_Model(batch_size=32, depth=3, hidden_neurons_1=64, hidden_neurons_2=128, hidden_neurons_3=256, kernel_size_1=3, kernel_size_2=3, kernel_size_3=3, \n",
        "               dropout_rate=0.2, batch_norm=False, pooling='max', layer_activation_1='relu', layer_activation_2='relu', layer_activation_3='relu',\n",
        "               kernel_initialiser='he_uniform', bias_initialiser='zeros', optimiser='SGD', weight_decay=None, learning_rate=1e-2, output_activation='softmax',use_cutout=True, SGD_momentum=0.9) \n",
        "\n",
        "  \n",
        "    print('------------------------------------------------------------------------')\n",
        "    print(f'Training for fold {fold_no} ...')\n",
        "\n",
        "    history = model.train(\n",
        "              batch_generator(\n",
        "                trainX[train],\n",
        "                trainy[train],\n",
        "                m=8,\n",
        "                batch_size=32,\n",
        "                epochs=30, \n",
        "                augment=apply_mask\n",
        "              ),\n",
        "                trainX[train],\n",
        "                  trainy[train],\n",
        "                trainX[valid],\n",
        "                  trainy[valid],\n",
        "                  lr_rate_type='step')\n",
        "\n",
        "    scores = model.evaluate(trainX[valid], trainy[valid])\n",
        "\n",
        "    print(f'Score for fold {fold_no}: loss of {scores[0]}; accuracy of {scores[1]*100}%')\n",
        "    acc_per_fold.append(scores[1] * 100)\n",
        "    loss_per_fold.append(scores[0])\n",
        "\n",
        "    fold_no+=1\n",
        "\n",
        "# == Provide average scores ==\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Score per fold')\n",
        "for i in range(0, len(acc_per_fold)):\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Average scores for all folds:')\n",
        "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
        "print(f'> Loss: {np.mean(loss_per_fold)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "OWpl4ZXv5gWG",
        "outputId": "88995c8e-b013-45ae-814b-7509a05f6882"
      },
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_28\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_84 (Conv2D)          (None, 32, 32, 64)        1792      \n",
            "                                                                 \n",
            " max_pooling2d_27 (MaxPoolin  multiple                 0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_85 (Conv2D)          (None, 16, 16, 128)       73856     \n",
            "                                                                 \n",
            " dropout_28 (Dropout)        (None, 8, 8, 128)         0         \n",
            "                                                                 \n",
            " conv2d_86 (Conv2D)          (None, 8, 8, 256)         295168    \n",
            "                                                                 \n",
            " flatten_28 (Flatten)        (None, 4096)              0         \n",
            "                                                                 \n",
            " dense_28 (Dense)            (None, 10)                40970     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 411,786\n",
            "Trainable params: 411,786\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/30\n",
            "1250/1250 [==============================] - 29s 22ms/step - loss: 1.9684 - accuracy: 0.2899 - val_loss: 1.7244 - val_accuracy: 0.3848 - lr: 0.0100\n",
            "Epoch 2/30\n",
            "1250/1250 [==============================] - 28s 23ms/step - loss: 1.7330 - accuracy: 0.3754 - val_loss: 1.5765 - val_accuracy: 0.4492 - lr: 0.0100\n",
            "Epoch 3/30\n",
            "1250/1250 [==============================] - 29s 23ms/step - loss: 1.6166 - accuracy: 0.4177 - val_loss: 1.3981 - val_accuracy: 0.4981 - lr: 0.0100\n",
            "Epoch 4/30\n",
            "1250/1250 [==============================] - 28s 22ms/step - loss: 1.5433 - accuracy: 0.4442 - val_loss: 1.2484 - val_accuracy: 0.5463 - lr: 0.0100\n",
            "Epoch 5/30\n",
            "1250/1250 [==============================] - 29s 23ms/step - loss: 1.4865 - accuracy: 0.4672 - val_loss: 1.2232 - val_accuracy: 0.5743 - lr: 0.0100\n",
            "Epoch 6/30\n",
            "1250/1250 [==============================] - 28s 22ms/step - loss: 1.4411 - accuracy: 0.4814 - val_loss: 1.1999 - val_accuracy: 0.5721 - lr: 0.0100\n",
            "Epoch 7/30\n",
            "1250/1250 [==============================] - 29s 23ms/step - loss: 1.4036 - accuracy: 0.4929 - val_loss: 1.1188 - val_accuracy: 0.6066 - lr: 0.0100\n",
            "Epoch 8/30\n",
            "1250/1250 [==============================] - 29s 23ms/step - loss: 1.3789 - accuracy: 0.5073 - val_loss: 1.0639 - val_accuracy: 0.6236 - lr: 0.0100\n",
            "Epoch 9/30\n",
            "1250/1250 [==============================] - 28s 22ms/step - loss: 1.3508 - accuracy: 0.5151 - val_loss: 1.1209 - val_accuracy: 0.6038 - lr: 0.0100\n",
            "Epoch 10/30\n",
            "1250/1250 [==============================] - 28s 23ms/step - loss: 1.3267 - accuracy: 0.5261 - val_loss: 1.0109 - val_accuracy: 0.6464 - lr: 0.0100\n",
            "Epoch 11/30\n",
            "1250/1250 [==============================] - 30s 24ms/step - loss: 1.2483 - accuracy: 0.5549 - val_loss: 1.0122 - val_accuracy: 0.6487 - lr: 0.0050\n",
            "Epoch 12/30\n",
            "1250/1250 [==============================] - 30s 24ms/step - loss: 1.2295 - accuracy: 0.5614 - val_loss: 0.9159 - val_accuracy: 0.6823 - lr: 0.0050\n",
            "Epoch 13/30\n",
            "1250/1250 [==============================] - 29s 23ms/step - loss: 1.2107 - accuracy: 0.5697 - val_loss: 0.8887 - val_accuracy: 0.6940 - lr: 0.0050\n",
            "Epoch 14/30\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 1.2023 - accuracy: 0.5718 - val_loss: 0.9150 - val_accuracy: 0.6802 - lr: 0.0050\n",
            "Epoch 15/30\n",
            "1250/1250 [==============================] - 29s 23ms/step - loss: 1.1892 - accuracy: 0.5759 - val_loss: 0.8718 - val_accuracy: 0.6966 - lr: 0.0050\n",
            "Epoch 16/30\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 1.1733 - accuracy: 0.5832 - val_loss: 0.8924 - val_accuracy: 0.6931 - lr: 0.0050\n",
            "Epoch 17/30\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 1.1687 - accuracy: 0.5835 - val_loss: 0.8948 - val_accuracy: 0.6901 - lr: 0.0050\n",
            "Epoch 18/30\n",
            "1250/1250 [==============================] - 30s 24ms/step - loss: 1.1556 - accuracy: 0.5864 - val_loss: 0.9554 - val_accuracy: 0.6816 - lr: 0.0050\n",
            "Epoch 19/30\n",
            "1250/1250 [==============================] - 28s 23ms/step - loss: 1.1452 - accuracy: 0.5924 - val_loss: 0.8485 - val_accuracy: 0.7107 - lr: 0.0050\n",
            "Epoch 20/30\n",
            "1250/1250 [==============================] - 28s 22ms/step - loss: 1.1376 - accuracy: 0.5940 - val_loss: 0.8843 - val_accuracy: 0.7017 - lr: 0.0050\n",
            "Epoch 21/30\n",
            "1250/1250 [==============================] - 28s 22ms/step - loss: 1.0745 - accuracy: 0.6197 - val_loss: 0.8009 - val_accuracy: 0.7270 - lr: 0.0012\n",
            "Epoch 22/30\n",
            "1250/1250 [==============================] - 29s 23ms/step - loss: 1.0553 - accuracy: 0.6262 - val_loss: 0.7898 - val_accuracy: 0.7320 - lr: 0.0012\n",
            "Epoch 23/30\n",
            "1250/1250 [==============================] - 28s 23ms/step - loss: 1.0524 - accuracy: 0.6254 - val_loss: 0.7832 - val_accuracy: 0.7344 - lr: 0.0012\n",
            "Epoch 24/30\n",
            " 829/1250 [==================>...........] - ETA: 9s - loss: 1.0462 - accuracy: 0.6275"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17312/3502573288.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'Training for fold {fold_no} ...'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     history = model.train(\n\u001b[0m\u001b[0;32m     17\u001b[0m               batch_generator(\n\u001b[0;32m     18\u001b[0m                 \u001b[0mtrainX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17312/3965214673.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, train_gen, train_x, train_y, test_x, test_y, lr_rate, lr_rate_type)\u001b[0m\n\u001b[0;32m    161\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlr_rate\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcutout\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 163\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_gen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtensorboard_callback\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcallback\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    164\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_gen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtensorboard_callback\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcallback\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m#  self.model.fit(train_x, train_y, validation_data=(test_x, test_y) , epochs=25, verbose=1)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1382\u001b[0m                 _r=1):\n\u001b[0;32m   1383\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1384\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1385\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1386\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    913\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    914\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 915\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    945\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    946\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 947\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    948\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2954\u001b[0m       (graph_function,\n\u001b[0;32m   2955\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2956\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   2957\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   2958\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1851\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1852\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1853\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1854\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1855\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    498\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 499\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    500\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    501\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### CutMix"
      ],
      "metadata": {
        "id": "RYLVBj_8YjSG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "acc_per_fold = []\n",
        "loss_per_fold = []\n",
        "\n",
        "with tf.device('/device:GPU:0'):\n",
        "  fold_no = 1\n",
        "  for train, valid in kfold.split(trainX, trainy):\n",
        "\n",
        "    model = CNN_Model(batch_size=32, depth=3, hidden_neurons_1=64, hidden_neurons_2=128, hidden_neurons_3=256, kernel_size_1=3, kernel_size_2=3, kernel_size_3=3, \n",
        "               dropout_rate=0.2, batch_norm=False, pooling='max', layer_activation_1='relu', layer_activation_2='relu', layer_activation_3='relu',\n",
        "               kernel_initialiser='he_uniform', bias_initialiser='zeros', optimiser='SGD', weight_decay=None, learning_rate=1e-2, output_activation='softmax', SGD_momentum=0.9) \n",
        "  \n",
        "    print('------------------------------------------------------------------------')\n",
        "    print(f'Training for fold {fold_no} ...')\n",
        "\n",
        "    train_ds_one = (\n",
        "    tf.data.Dataset.from_tensor_slices((trainX[train], trainy[train]))\n",
        "    .shuffle(1024)\n",
        "    .map(preprocess_image, num_parallel_calls=AUTO)\n",
        "    )\n",
        "    train_ds_two = (\n",
        "      tf.data.Dataset.from_tensor_slices((trainX[train], trainy[train]))\n",
        "      .shuffle(1024)\n",
        "      .map(preprocess_image, num_parallel_calls=AUTO)\n",
        "    )\n",
        "\n",
        "    train_ds = tf.data.Dataset.zip((train_ds_one, train_ds_two))\n",
        "\n",
        "    train_ds_cmu = (\n",
        "      train_ds.shuffle(1024)\n",
        "      .map(cutmix, num_parallel_calls=AUTO)\n",
        "      .batch(BATCH_SIZE)\n",
        "      .prefetch(AUTO)\n",
        "      )\n",
        "    \n",
        "    val_ds = tf.data.Dataset.from_tensor_slices((trainX[valid], trainy[valid]))\n",
        "\n",
        "    val_ds = (\n",
        "    val_ds.map(preprocess_image, num_parallel_calls=AUTO)\n",
        "    .batch(32)\n",
        "    .prefetch(AUTO)\n",
        "    )\n",
        "\n",
        "    history = model.train_mix(\n",
        "              train_ds_cmu,\n",
        "                val_ds,\n",
        "                lr_rate_type='step')\n",
        "\n",
        "    scores = model.evaluate_mix(val_ds)\n",
        "\n",
        "    print(f'Score for fold {fold_no}: loss of {scores[0]}; accuracy of {scores[1]*100}%')\n",
        "    acc_per_fold.append(scores[1] * 100)\n",
        "    loss_per_fold.append(scores[0])\n",
        "\n",
        "    fold_no+=1\n",
        "\n",
        "# == Provide average scores ==\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Score per fold')\n",
        "for i in range(0, len(acc_per_fold)):\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Average scores for all folds:')\n",
        "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
        "print(f'> Loss: {np.mean(loss_per_fold)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0iSE_SCRfbeY",
        "outputId": "520be258-d255-4225-b898-bf3cd400717b"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 32, 32, 64)        1792      \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  multiple                 0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 16, 16, 128)       73856     \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 8, 8, 128)         0         \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 8, 8, 256)         295168    \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 4096)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 10)                40970     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 411,786\n",
            "Trainable params: 411,786\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/30\n",
            "1250/1250 [==============================] - 17s 7ms/step - loss: 1.9176 - accuracy: 0.3420 - val_loss: 1.3786 - val_accuracy: 0.5455 - lr: 0.0100\n",
            "Epoch 2/30\n",
            "1250/1250 [==============================] - 9s 7ms/step - loss: 1.6637 - accuracy: 0.4651 - val_loss: 1.1644 - val_accuracy: 0.6326 - lr: 0.0100\n",
            "Epoch 3/30\n",
            "1250/1250 [==============================] - 9s 7ms/step - loss: 1.5697 - accuracy: 0.5128 - val_loss: 1.0308 - val_accuracy: 0.6696 - lr: 0.0100\n",
            "Epoch 4/30\n",
            "1250/1250 [==============================] - 9s 7ms/step - loss: 1.5099 - accuracy: 0.5432 - val_loss: 0.9729 - val_accuracy: 0.6983 - lr: 0.0100\n",
            "Epoch 5/30\n",
            "1250/1250 [==============================] - 9s 7ms/step - loss: 1.4663 - accuracy: 0.5659 - val_loss: 0.9662 - val_accuracy: 0.6850 - lr: 0.0100\n",
            "Epoch 6/30\n",
            "1250/1250 [==============================] - 9s 7ms/step - loss: 1.4285 - accuracy: 0.5798 - val_loss: 0.9187 - val_accuracy: 0.7151 - lr: 0.0100\n",
            "Epoch 7/30\n",
            "1250/1250 [==============================] - 9s 7ms/step - loss: 1.4027 - accuracy: 0.5960 - val_loss: 0.8933 - val_accuracy: 0.7174 - lr: 0.0100\n",
            "Epoch 8/30\n",
            "1250/1250 [==============================] - 9s 7ms/step - loss: 1.3807 - accuracy: 0.6058 - val_loss: 0.8656 - val_accuracy: 0.7258 - lr: 0.0100\n",
            "Epoch 9/30\n",
            "1250/1250 [==============================] - 9s 7ms/step - loss: 1.3550 - accuracy: 0.6178 - val_loss: 0.8425 - val_accuracy: 0.7352 - lr: 0.0100\n",
            "Epoch 10/30\n",
            "1250/1250 [==============================] - 9s 7ms/step - loss: 1.3382 - accuracy: 0.6259 - val_loss: 0.7760 - val_accuracy: 0.7413 - lr: 0.0100\n",
            "Epoch 11/30\n",
            "1250/1250 [==============================] - 9s 7ms/step - loss: 1.2761 - accuracy: 0.6579 - val_loss: 0.7514 - val_accuracy: 0.7715 - lr: 0.0050\n",
            "Epoch 12/30\n",
            "1250/1250 [==============================] - 10s 8ms/step - loss: 1.2591 - accuracy: 0.6617 - val_loss: 0.7267 - val_accuracy: 0.7766 - lr: 0.0050\n",
            "Epoch 13/30\n",
            "1250/1250 [==============================] - 9s 7ms/step - loss: 1.2462 - accuracy: 0.6726 - val_loss: 0.7171 - val_accuracy: 0.7758 - lr: 0.0050\n",
            "Epoch 14/30\n",
            "1250/1250 [==============================] - 9s 7ms/step - loss: 1.2313 - accuracy: 0.6780 - val_loss: 0.7328 - val_accuracy: 0.7758 - lr: 0.0050\n",
            "Epoch 15/30\n",
            "1250/1250 [==============================] - 9s 7ms/step - loss: 1.2236 - accuracy: 0.6828 - val_loss: 0.7157 - val_accuracy: 0.7722 - lr: 0.0050\n",
            "Epoch 16/30\n",
            "1250/1250 [==============================] - 9s 7ms/step - loss: 1.2164 - accuracy: 0.6847 - val_loss: 0.7345 - val_accuracy: 0.7758 - lr: 0.0050\n",
            "Epoch 17/30\n",
            "1250/1250 [==============================] - 9s 7ms/step - loss: 1.2089 - accuracy: 0.6899 - val_loss: 0.7230 - val_accuracy: 0.7739 - lr: 0.0050\n",
            "Epoch 18/30\n",
            "1250/1250 [==============================] - 9s 7ms/step - loss: 1.1995 - accuracy: 0.6920 - val_loss: 0.6874 - val_accuracy: 0.7861 - lr: 0.0050\n",
            "Epoch 19/30\n",
            "1250/1250 [==============================] - 9s 7ms/step - loss: 1.1915 - accuracy: 0.6985 - val_loss: 0.7109 - val_accuracy: 0.7814 - lr: 0.0050\n",
            "Epoch 20/30\n",
            "1250/1250 [==============================] - 9s 7ms/step - loss: 1.1810 - accuracy: 0.7026 - val_loss: 0.6880 - val_accuracy: 0.7803 - lr: 0.0050\n",
            "Epoch 21/30\n",
            "1250/1250 [==============================] - 9s 7ms/step - loss: 1.1429 - accuracy: 0.7187 - val_loss: 0.6636 - val_accuracy: 0.7950 - lr: 0.0012\n",
            "Epoch 22/30\n",
            "1250/1250 [==============================] - 9s 7ms/step - loss: 1.1325 - accuracy: 0.7236 - val_loss: 0.6583 - val_accuracy: 0.7968 - lr: 0.0012\n",
            "Epoch 23/30\n",
            "1250/1250 [==============================] - 9s 7ms/step - loss: 1.1298 - accuracy: 0.7265 - val_loss: 0.6543 - val_accuracy: 0.8007 - lr: 0.0012\n",
            "Epoch 24/30\n",
            "1250/1250 [==============================] - 9s 7ms/step - loss: 1.1223 - accuracy: 0.7297 - val_loss: 0.6560 - val_accuracy: 0.7978 - lr: 0.0012\n",
            "Epoch 25/30\n",
            "1250/1250 [==============================] - 9s 7ms/step - loss: 1.1170 - accuracy: 0.7329 - val_loss: 0.6560 - val_accuracy: 0.7977 - lr: 0.0012\n",
            "Epoch 26/30\n",
            "1250/1250 [==============================] - 9s 7ms/step - loss: 1.1212 - accuracy: 0.7301 - val_loss: 0.6537 - val_accuracy: 0.8022 - lr: 0.0012\n",
            "Epoch 27/30\n",
            "1250/1250 [==============================] - 10s 8ms/step - loss: 1.1184 - accuracy: 0.7340 - val_loss: 0.6512 - val_accuracy: 0.7991 - lr: 0.0012\n",
            "Epoch 28/30\n",
            "1250/1250 [==============================] - 10s 8ms/step - loss: 1.1184 - accuracy: 0.7315 - val_loss: 0.6451 - val_accuracy: 0.7996 - lr: 0.0012\n",
            "Epoch 29/30\n",
            "1250/1250 [==============================] - 9s 7ms/step - loss: 1.1164 - accuracy: 0.7337 - val_loss: 0.6436 - val_accuracy: 0.8024 - lr: 0.0012\n",
            "Epoch 30/30\n",
            "1250/1250 [==============================] - 9s 8ms/step - loss: 1.1051 - accuracy: 0.7392 - val_loss: 0.6482 - val_accuracy: 0.8034 - lr: 0.0012\n",
            "313/313 [==============================] - 1s 4ms/step - loss: 0.6482 - accuracy: 0.8034\n",
            "Score for fold 1: loss of 0.6482334733009338; accuracy of 80.33999800682068%\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_3 (Conv2D)           (None, 32, 32, 64)        1792      \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  multiple                 0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_4 (Conv2D)           (None, 16, 16, 128)       73856     \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 8, 8, 128)         0         \n",
            "                                                                 \n",
            " conv2d_5 (Conv2D)           (None, 8, 8, 256)         295168    \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 4096)              0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 10)                40970     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 411,786\n",
            "Trainable params: 411,786\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/30\n",
            "1250/1250 [==============================] - 10s 8ms/step - loss: 1.8868 - accuracy: 0.3556 - val_loss: 1.3138 - val_accuracy: 0.5645 - lr: 0.0100\n",
            "Epoch 2/30\n",
            "1250/1250 [==============================] - 10s 8ms/step - loss: 1.6561 - accuracy: 0.4706 - val_loss: 1.1555 - val_accuracy: 0.6253 - lr: 0.0100\n",
            "Epoch 3/30\n",
            "1144/1250 [==========================>...] - ETA: 0s - loss: 1.5629 - accuracy: 0.5182"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_19816/1529249451.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     41\u001b[0m     )\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m     history = model.train_mix(\n\u001b[0m\u001b[0;32m     44\u001b[0m               \u001b[0mtrain_ds_cmu\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m                 \u001b[0mval_ds\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_19816/3965214673.py\u001b[0m in \u001b[0;36mtrain_mix\u001b[1;34m(self, train_ds, val_ds, lr_rate, lr_rate_type)\u001b[0m\n\u001b[0;32m    180\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'categorical_crossentropy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlr_rate\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 182\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_ds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_ds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtensorboard_callback\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    183\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_ds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_ds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtensorboard_callback\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1382\u001b[0m                 _r=1):\n\u001b[0;32m   1383\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1384\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1385\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1386\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    913\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    914\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 915\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    945\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    946\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 947\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    948\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2954\u001b[0m       (graph_function,\n\u001b[0;32m   2955\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2956\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   2957\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   2958\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1851\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1852\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1853\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1854\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1855\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    498\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 499\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    500\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    501\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training model on full dataset and testing"
      ],
      "metadata": {
        "id": "LfnH8R8qZnym"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Best achieving model 77.3% accuracy for 3 hidden layers without batch normalisation and using data augmentation\n",
        "acc_per_fold = []\n",
        "loss_per_fold = []\n",
        "\n",
        "with tf.device('/device:GPU:0'):\n",
        "\n",
        "    model = CNN_Model(batch_size=32, depth=3, hidden_neurons_1=32, hidden_neurons_2=64, hidden_neurons_3=128, kernel_size_1=3, kernel_size_2=3, kernel_size_3=3, \n",
        "               dropout_rate=0.4, batch_norm=False, pooling='max', layer_activation_1='relu', layer_activation_2='relu', layer_activation_3='relu',\n",
        "               kernel_initialiser='he_uniform', bias_initialiser='zeros', optimiser='SGD', weight_decay=None, learning_rate=1e-2, output_activation='softmax', SGD_momentum=0.9)\n",
        "  \n",
        "    train_gen = tf.keras.preprocessing.image.ImageDataGenerator(width_shift_range=0.1,\n",
        "      height_shift_range=0.1,\n",
        "      horizontal_flip=True) #Data Augmentation\n",
        "    train_gen.fit(trainX)\n",
        "  \n",
        "    print('------------------------------------------------------------------------')\n",
        "    print('Training with full data')\n",
        "\n",
        "    history = model.train(\n",
        "              train_gen,\n",
        "                trainX,\n",
        "                  trainy,\n",
        "                testX,\n",
        "                  testy,\n",
        "                  lr_rate_type='step')\n",
        "\n",
        "    scores = model.evaluate(testX, testy)\n",
        "    acc_per_fold.append(scores[1] * 100)\n",
        "    loss_per_fold.append(scores[0])\n",
        "\n",
        "# == Provide average scores ==\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Score per fold')\n",
        "for i in range(0, len(acc_per_fold)):\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Average scores for all folds:')\n",
        "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
        "print(f'> Loss: {np.mean(loss_per_fold)}')"
      ],
      "metadata": {
        "id": "2RrzuN_MZni-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1da735f-b9c3-4d2c-8c33-a7580ed25fe3"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_12 (Conv2D)          (None, 32, 32, 32)        896       \n",
            "                                                                 \n",
            " max_pooling2d_4 (MaxPooling  multiple                 0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_13 (Conv2D)          (None, 16, 16, 64)        18496     \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 8, 8, 64)          0         \n",
            "                                                                 \n",
            " conv2d_14 (Conv2D)          (None, 8, 8, 128)         73856     \n",
            "                                                                 \n",
            " flatten_4 (Flatten)         (None, 2048)              0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 10)                20490     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 113,738\n",
            "Trainable params: 113,738\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training with full data\n",
            "Epoch 1/30\n",
            "1562/1562 [==============================] - 22s 14ms/step - loss: 1.7365 - accuracy: 0.3666 - val_loss: 1.4403 - val_accuracy: 0.4952 - lr: 0.0100\n",
            "Epoch 2/30\n",
            "1562/1562 [==============================] - 21s 14ms/step - loss: 1.3973 - accuracy: 0.4976 - val_loss: 1.2434 - val_accuracy: 0.5657 - lr: 0.0100\n",
            "Epoch 3/30\n",
            "1562/1562 [==============================] - 23s 15ms/step - loss: 1.2677 - accuracy: 0.5491 - val_loss: 1.1268 - val_accuracy: 0.6242 - lr: 0.0100\n",
            "Epoch 4/30\n",
            "1562/1562 [==============================] - 21s 14ms/step - loss: 1.1985 - accuracy: 0.5740 - val_loss: 1.0943 - val_accuracy: 0.6296 - lr: 0.0100\n",
            "Epoch 5/30\n",
            "1562/1562 [==============================] - 21s 13ms/step - loss: 1.1476 - accuracy: 0.5963 - val_loss: 1.0611 - val_accuracy: 0.6387 - lr: 0.0100\n",
            "Epoch 6/30\n",
            "1562/1562 [==============================] - 23s 15ms/step - loss: 1.1077 - accuracy: 0.6096 - val_loss: 0.9525 - val_accuracy: 0.6810 - lr: 0.0100\n",
            "Epoch 7/30\n",
            "1562/1562 [==============================] - 24s 16ms/step - loss: 1.0793 - accuracy: 0.6200 - val_loss: 0.9883 - val_accuracy: 0.6599 - lr: 0.0100\n",
            "Epoch 8/30\n",
            "1562/1562 [==============================] - 25s 16ms/step - loss: 1.0682 - accuracy: 0.6258 - val_loss: 0.9494 - val_accuracy: 0.6723 - lr: 0.0100\n",
            "Epoch 9/30\n",
            "1562/1562 [==============================] - 23s 15ms/step - loss: 1.0429 - accuracy: 0.6333 - val_loss: 0.9187 - val_accuracy: 0.6827 - lr: 0.0100\n",
            "Epoch 10/30\n",
            "1562/1562 [==============================] - 20s 13ms/step - loss: 1.0185 - accuracy: 0.6441 - val_loss: 0.9389 - val_accuracy: 0.6802 - lr: 0.0100\n",
            "Epoch 11/30\n",
            "1562/1562 [==============================] - 22s 14ms/step - loss: 0.9256 - accuracy: 0.6766 - val_loss: 0.8808 - val_accuracy: 0.6949 - lr: 0.0050\n",
            "Epoch 12/30\n",
            "1562/1562 [==============================] - 21s 14ms/step - loss: 0.8940 - accuracy: 0.6871 - val_loss: 0.8330 - val_accuracy: 0.7128 - lr: 0.0050\n",
            "Epoch 13/30\n",
            "1562/1562 [==============================] - 22s 14ms/step - loss: 0.8865 - accuracy: 0.6914 - val_loss: 0.7775 - val_accuracy: 0.7361 - lr: 0.0050\n",
            "Epoch 14/30\n",
            "1562/1562 [==============================] - 22s 14ms/step - loss: 0.8707 - accuracy: 0.6947 - val_loss: 0.7957 - val_accuracy: 0.7322 - lr: 0.0050\n",
            "Epoch 15/30\n",
            "1562/1562 [==============================] - 21s 13ms/step - loss: 0.8622 - accuracy: 0.6985 - val_loss: 0.7808 - val_accuracy: 0.7304 - lr: 0.0050\n",
            "Epoch 16/30\n",
            "1562/1562 [==============================] - 20s 13ms/step - loss: 0.8496 - accuracy: 0.7013 - val_loss: 0.8554 - val_accuracy: 0.7073 - lr: 0.0050\n",
            "Epoch 17/30\n",
            "1562/1562 [==============================] - 22s 14ms/step - loss: 0.8457 - accuracy: 0.7070 - val_loss: 0.7728 - val_accuracy: 0.7379 - lr: 0.0050\n",
            "Epoch 18/30\n",
            "1562/1562 [==============================] - 22s 14ms/step - loss: 0.8282 - accuracy: 0.7116 - val_loss: 0.7670 - val_accuracy: 0.7379 - lr: 0.0050\n",
            "Epoch 19/30\n",
            "1562/1562 [==============================] - 23s 15ms/step - loss: 0.8239 - accuracy: 0.7110 - val_loss: 0.7414 - val_accuracy: 0.7542 - lr: 0.0050\n",
            "Epoch 20/30\n",
            "1562/1562 [==============================] - 21s 14ms/step - loss: 0.8211 - accuracy: 0.7141 - val_loss: 0.7817 - val_accuracy: 0.7298 - lr: 0.0050\n",
            "Epoch 21/30\n",
            "1562/1562 [==============================] - 21s 13ms/step - loss: 0.7480 - accuracy: 0.7399 - val_loss: 0.6859 - val_accuracy: 0.7666 - lr: 0.0012\n",
            "Epoch 22/30\n",
            "1562/1562 [==============================] - 21s 13ms/step - loss: 0.7307 - accuracy: 0.7456 - val_loss: 0.7084 - val_accuracy: 0.7574 - lr: 0.0012\n",
            "Epoch 23/30\n",
            "1562/1562 [==============================] - 21s 13ms/step - loss: 0.7315 - accuracy: 0.7445 - val_loss: 0.6781 - val_accuracy: 0.7701 - lr: 0.0012\n",
            "Epoch 24/30\n",
            "1562/1562 [==============================] - 22s 14ms/step - loss: 0.7235 - accuracy: 0.7477 - val_loss: 0.6810 - val_accuracy: 0.7683 - lr: 0.0012\n",
            "Epoch 25/30\n",
            "1562/1562 [==============================] - 22s 14ms/step - loss: 0.7169 - accuracy: 0.7516 - val_loss: 0.6833 - val_accuracy: 0.7722 - lr: 0.0012\n",
            "Epoch 26/30\n",
            "1562/1562 [==============================] - 22s 14ms/step - loss: 0.7178 - accuracy: 0.7504 - val_loss: 0.6809 - val_accuracy: 0.7694 - lr: 0.0012\n",
            "Epoch 27/30\n",
            "1562/1562 [==============================] - 21s 13ms/step - loss: 0.7080 - accuracy: 0.7533 - val_loss: 0.6609 - val_accuracy: 0.7759 - lr: 0.0012\n",
            "Epoch 28/30\n",
            "1562/1562 [==============================] - 21s 13ms/step - loss: 0.7037 - accuracy: 0.7551 - val_loss: 0.6699 - val_accuracy: 0.7737 - lr: 0.0012\n",
            "Epoch 29/30\n",
            "1562/1562 [==============================] - 21s 13ms/step - loss: 0.7038 - accuracy: 0.7539 - val_loss: 0.6456 - val_accuracy: 0.7796 - lr: 0.0012\n",
            "Epoch 30/30\n",
            "1562/1562 [==============================] - 22s 14ms/step - loss: 0.7067 - accuracy: 0.7527 - val_loss: 0.6673 - val_accuracy: 0.7734 - lr: 0.0012\n",
            "------------------------------------------------------------------------\n",
            "Score per fold\n",
            "------------------------------------------------------------------------\n",
            "> Fold 1 - Loss: 0.667330801486969 - Accuracy: 77.34000086784363%\n",
            "------------------------------------------------------------------------\n",
            "Average scores for all folds:\n",
            "> Accuracy: 77.34000086784363 (+- 0.0)\n",
            "> Loss: 0.667330801486969\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Model achieved 76.2% accuracy for 3 hidden layers without batch normalisation and using cut mix\n",
        "acc_per_fold = []\n",
        "loss_per_fold = []\n",
        "\n",
        "with tf.device('/device:GPU:0'):\n",
        "\n",
        "    model = CNN_Model(batch_size=32, depth=3, hidden_neurons_1=32, hidden_neurons_2=64, hidden_neurons_3=128, kernel_size_1=3, kernel_size_2=3, kernel_size_3=3, \n",
        "               dropout_rate=0.4, batch_norm=False, pooling='max', layer_activation_1='relu', layer_activation_2='relu', layer_activation_3='relu',\n",
        "               kernel_initialiser='he_uniform', bias_initialiser='zeros', optimiser='SGD', weight_decay=None, learning_rate=1e-2, output_activation='softmax', SGD_momentum=0.9) \n",
        "  \n",
        "    print('------------------------------------------------------------------------')\n",
        "    print(f'Training for fold full training data ...')\n",
        "\n",
        "    train_ds_one = (\n",
        "    tf.data.Dataset.from_tensor_slices((trainX, trainy))\n",
        "    .shuffle(1024)\n",
        "    .map(preprocess_image, num_parallel_calls=AUTO)\n",
        "    )\n",
        "    train_ds_two = (\n",
        "      tf.data.Dataset.from_tensor_slices((trainX, trainy))\n",
        "      .shuffle(1024)\n",
        "      .map(preprocess_image, num_parallel_calls=AUTO)\n",
        "    )\n",
        "\n",
        "    train_ds = tf.data.Dataset.zip((train_ds_one, train_ds_two))\n",
        "\n",
        "    train_ds_cmu = (\n",
        "      train_ds.shuffle(1024)\n",
        "      .map(cutmix, num_parallel_calls=AUTO)\n",
        "      .batch(64)\n",
        "      .prefetch(AUTO)\n",
        "      )   #Modifying training data for CutMix\n",
        "    \n",
        "    test_ds = tf.data.Dataset.from_tensor_slices((testX, testy))\n",
        "\n",
        "    test_ds = (\n",
        "    test_ds.map(preprocess_image, num_parallel_calls=AUTO)\n",
        "    .batch(64)\n",
        "    .prefetch(AUTO)\n",
        "    )  #Modifying testing data for CutMix\n",
        "\n",
        "    history = model.train_mix(\n",
        "              train_ds_cmu,\n",
        "                test_ds,\n",
        "                lr_rate_type='step')  #CutMix data augmentation\n",
        "\n",
        "    scores = model.evaluate_mix(test_ds)\n",
        "\n",
        "    print(f'Model score: loss of {scores[0]}; accuracy of {scores[1]*100}%')\n",
        "    acc_per_fold.append(scores[1] * 100)\n",
        "    loss_per_fold.append(scores[0])\n",
        "\n",
        "# == Provide average scores ==\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Score per fold')\n",
        "for i in range(0, len(acc_per_fold)):\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Average scores for all folds:')\n",
        "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
        "print(f'> Loss: {np.mean(loss_per_fold)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oa5LdtUmIt8i",
        "outputId": "be9c7060-4dd2-43bf-f3d6-92c71f3960de"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_3 (Conv2D)           (None, 32, 32, 32)        896       \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  multiple                 0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_4 (Conv2D)           (None, 16, 16, 64)        18496     \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 8, 8, 64)          0         \n",
            "                                                                 \n",
            " conv2d_5 (Conv2D)           (None, 8, 8, 128)         73856     \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 2048)              0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 10)                20490     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 113,738\n",
            "Trainable params: 113,738\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold full training data ...\n",
            "Epoch 1/30\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 1.9464 - accuracy: 0.3250 - val_loss: 1.5061 - val_accuracy: 0.4783 - lr: 0.0100\n",
            "Epoch 2/30\n",
            "782/782 [==============================] - 7s 8ms/step - loss: 1.7616 - accuracy: 0.4146 - val_loss: 1.3295 - val_accuracy: 0.5638 - lr: 0.0100\n",
            "Epoch 3/30\n",
            "782/782 [==============================] - 6s 8ms/step - loss: 1.6790 - accuracy: 0.4548 - val_loss: 1.2629 - val_accuracy: 0.5840 - lr: 0.0100\n",
            "Epoch 4/30\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 1.6172 - accuracy: 0.4879 - val_loss: 1.1746 - val_accuracy: 0.6242 - lr: 0.0100\n",
            "Epoch 5/30\n",
            "782/782 [==============================] - 6s 8ms/step - loss: 1.5761 - accuracy: 0.5056 - val_loss: 1.1047 - val_accuracy: 0.6431 - lr: 0.0100\n",
            "Epoch 6/30\n",
            "782/782 [==============================] - 6s 8ms/step - loss: 1.5513 - accuracy: 0.5217 - val_loss: 1.0975 - val_accuracy: 0.6503 - lr: 0.0100\n",
            "Epoch 7/30\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 1.5247 - accuracy: 0.5344 - val_loss: 1.0396 - val_accuracy: 0.6754 - lr: 0.0100\n",
            "Epoch 8/30\n",
            "782/782 [==============================] - 6s 8ms/step - loss: 1.5017 - accuracy: 0.5461 - val_loss: 1.0108 - val_accuracy: 0.6781 - lr: 0.0100\n",
            "Epoch 9/30\n",
            "782/782 [==============================] - 6s 8ms/step - loss: 1.4815 - accuracy: 0.5577 - val_loss: 1.0524 - val_accuracy: 0.6660 - lr: 0.0100\n",
            "Epoch 10/30\n",
            "782/782 [==============================] - 7s 8ms/step - loss: 1.4636 - accuracy: 0.5649 - val_loss: 0.9882 - val_accuracy: 0.7064 - lr: 0.0100\n",
            "Epoch 11/30\n",
            "782/782 [==============================] - 7s 8ms/step - loss: 1.4224 - accuracy: 0.5827 - val_loss: 0.9497 - val_accuracy: 0.7063 - lr: 0.0050\n",
            "Epoch 12/30\n",
            "782/782 [==============================] - 6s 8ms/step - loss: 1.4123 - accuracy: 0.5904 - val_loss: 0.9453 - val_accuracy: 0.7033 - lr: 0.0050\n",
            "Epoch 13/30\n",
            "782/782 [==============================] - 7s 8ms/step - loss: 1.4008 - accuracy: 0.5946 - val_loss: 0.8876 - val_accuracy: 0.7319 - lr: 0.0050\n",
            "Epoch 14/30\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 1.3951 - accuracy: 0.5973 - val_loss: 0.9135 - val_accuracy: 0.7284 - lr: 0.0050\n",
            "Epoch 15/30\n",
            "782/782 [==============================] - 6s 8ms/step - loss: 1.3870 - accuracy: 0.5980 - val_loss: 0.8792 - val_accuracy: 0.7345 - lr: 0.0050\n",
            "Epoch 16/30\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 1.3783 - accuracy: 0.6067 - val_loss: 0.8687 - val_accuracy: 0.7351 - lr: 0.0050\n",
            "Epoch 17/30\n",
            "782/782 [==============================] - 7s 8ms/step - loss: 1.3755 - accuracy: 0.6078 - val_loss: 0.8553 - val_accuracy: 0.7383 - lr: 0.0050\n",
            "Epoch 18/30\n",
            "782/782 [==============================] - 6s 8ms/step - loss: 1.3681 - accuracy: 0.6083 - val_loss: 0.8471 - val_accuracy: 0.7467 - lr: 0.0050\n",
            "Epoch 19/30\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 1.3587 - accuracy: 0.6141 - val_loss: 0.8377 - val_accuracy: 0.7430 - lr: 0.0050\n",
            "Epoch 20/30\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 1.3581 - accuracy: 0.6122 - val_loss: 0.8167 - val_accuracy: 0.7515 - lr: 0.0050\n",
            "Epoch 21/30\n",
            "782/782 [==============================] - 6s 8ms/step - loss: 1.3278 - accuracy: 0.6277 - val_loss: 0.8016 - val_accuracy: 0.7599 - lr: 0.0012\n",
            "Epoch 22/30\n",
            "782/782 [==============================] - 7s 10ms/step - loss: 1.3213 - accuracy: 0.6334 - val_loss: 0.7934 - val_accuracy: 0.7569 - lr: 0.0012\n",
            "Epoch 23/30\n",
            "782/782 [==============================] - 7s 8ms/step - loss: 1.3250 - accuracy: 0.6299 - val_loss: 0.7966 - val_accuracy: 0.7613 - lr: 0.0012\n",
            "Epoch 24/30\n",
            "782/782 [==============================] - 6s 8ms/step - loss: 1.3182 - accuracy: 0.6329 - val_loss: 0.7929 - val_accuracy: 0.7601 - lr: 0.0012\n",
            "Epoch 25/30\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 1.3188 - accuracy: 0.6332 - val_loss: 0.7813 - val_accuracy: 0.7629 - lr: 0.0012\n",
            "Epoch 26/30\n",
            "782/782 [==============================] - 6s 8ms/step - loss: 1.3167 - accuracy: 0.6332 - val_loss: 0.7900 - val_accuracy: 0.7619 - lr: 0.0012\n",
            "Epoch 27/30\n",
            "782/782 [==============================] - 6s 8ms/step - loss: 1.3087 - accuracy: 0.6356 - val_loss: 0.7941 - val_accuracy: 0.7613 - lr: 0.0012\n",
            "Epoch 28/30\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 1.3148 - accuracy: 0.6325 - val_loss: 0.7742 - val_accuracy: 0.7646 - lr: 0.0012\n",
            "Epoch 29/30\n",
            "782/782 [==============================] - 7s 8ms/step - loss: 1.3105 - accuracy: 0.6344 - val_loss: 0.7787 - val_accuracy: 0.7590 - lr: 0.0012\n",
            "Epoch 30/30\n",
            "782/782 [==============================] - 6s 8ms/step - loss: 1.3034 - accuracy: 0.6408 - val_loss: 0.7845 - val_accuracy: 0.7620 - lr: 0.0012\n",
            "157/157 [==============================] - 1s 3ms/step - loss: 0.7845 - accuracy: 0.7620\n",
            "Model score: loss of 0.7844604849815369; accuracy of 76.2000024318695%\n",
            "------------------------------------------------------------------------\n",
            "Score per fold\n",
            "------------------------------------------------------------------------\n",
            "> Fold 1 - Loss: 0.7844604849815369 - Accuracy: 76.2000024318695%\n",
            "------------------------------------------------------------------------\n",
            "Average scores for all folds:\n",
            "> Accuracy: 76.2000024318695 (+- 0.0)\n",
            "> Loss: 0.7844604849815369\n"
          ]
        }
      ]
    }
  ]
}